---
title: "R Notebook"
output: html_notebook
---



```{r - setup, reload data}
# install.packages("penalizedSVM")

library(sparseSVM)
library(SVMMaj)
library(WeightSVM)
library(e1071) # SVM package 
library(kernlab) # SVM package/s - has multiple libs such as LiblineaR, kernlab, e1071, obliqueRF
library(penalizedSVM) # needed for Smoothly clipped absolute deviation penalty (SCAD)



# Notes for kernlab - seems to have e1071 svm impl too aswell as other libraries
# https://topepo.github.io/caret/available-models.html 
# https://stackoverflow.com/questions/18911338/vastly-different-results-for-svm-model-using-e1071-and-caret
# https://www.thekerneltrip.com/statistics/kernlab-vs-e1071/#:~:text=While%20kernlab%20implements%20kernel%2Dbased,bagged%20clustering%2C%20naive%20Bayes%20classifier. 





# library(rdwd) # DWD package
library(sdwd) # SparseDWD
library(kerndwd)
library(DWDLargeR)

# Other
library(caret) # data split - https://rdrr.io/rforge/caret/man/createDataPartition.html
library(mlbench) # https://cran.r-project.org/web/packages/mlbench/mlbench.pdf
library(ggplot2) # lib for making plots
library(Boruta)
library(glmnet)
library(elasticnet)
library(randomForest)


# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
library(kableExtra) # Used to generate report ready tables

getwd()


y_labels_train = read.csv(file='mlsp-2014-mri/Train/train_labels.csv', head=TRUE, sep=",") # this is your y var
# Convert 'Class' into an unordered categorical variable, and assign labels
# to each level.
y_labels_train$Class = factor(y_labels_train$Class) 
# labels=c('Healthy_Control','Schizophrenic_Patient')
# 46 control # 40 patient 

############################
# loading and merging data #
############################

FNC_train = read.csv(file='mlsp-2014-mri/Train/train_FNC.csv',head=TRUE,sep=",")
SBM_train = read.csv(file='mlsp-2014-mri/Train/train_SBM.csv',head=TRUE,sep=",")
train_data = merge(FNC_train, SBM_train, by = "Id")
train_data = merge(train_data, y_labels_train, by="Id")

# These dont have lables so its pointless to use for testing
# SBM_test = read.csv(file='mlsp-2014-mri/Test/test_SBM.csv',head=TRUE,sep=",")
# FNC_test = read.csv(file='mlsp-2014-mri/Test/test_FNC.csv',head=TRUE,sep=",")
# test_data = merge(FNC_test, SBM_test, by = "Id")

#######################################
# data 80% - 20% split for train-test #
#######################################
train_data = train_data[,2:412]

# https://rdrr.io/rforge/caret/man/createDataPartition.html
# get indices for 70% of the data set
tmp_data = createDataPartition(y = train_data$Class, p = 0.8, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]


get_top_features=function(features, size){
  var_imp = varImp(features)
  tmp_df = data.frame(var_imp[1])
  if(missing(size)) {
        feature_names = rownames(tmp_df)[order(tmp_df$Overall, decreasing=TRUE)]

    } else {
        feature_names = rownames(tmp_df)[order(tmp_df$Overall, decreasing=TRUE)][1:size]

    }
  features_subset_df = subset(train_data, select = feature_names)
  return(features_subset_df)

}


```



```{r - Regularized Random Forest - DONE - TODO go higher than 25}

# 0 = 'Healthy Control', 1 = 'Schizophrenic Patient'

#############################
# Regularized Random Forest #
#############################
# making a dummy var, something you'd use in general linear regression

tmp_data = cbind(train_data, rnorm(1:dim(train_data)[1]))
colnames(tmp_data)[413] = 'dummy_var'
cols_list = colnames(tmp_data)
cols = cols_list[cols_list != "Id"]
data_with_dummy_var = subset(tmp_data, select=c(cols))


size=25
RRF = train(Class ~ ., data=train_data, method="RRF")
RRF_features = varImp(RRF)

feature_names = data.frame(RRF_features[1])
feature_names = rownames(feature_names)[order(feature_names$Overall, decreasing=TRUE)][1:size]


plot(RRF_features, top = size, main='Variable Importance')


RRF_features_data =  get_top_features(RRF, size)


```



```{r - RFE - DONE}

# TODO investigate what to do with this if boruta is being used

#######
# RFE # 
#######
# https://topepo.github.io/caret/feature-selection-using-genetic-algorithms.html


# 203 was the highest you can go without including terms that lower model preformance 
size = 203
result_rfe_all_features = rfe(x = train_data[,1:410], 
                   y = train_data$Class, 
                   # sizes = 410, # best all features
                   # 410 found that some features were lowering accuracy
                   sizes = size,
                   rfeControl = control)

#predictors(result_rfe_all_features)


RFE_features_data = get_top_features(result_rfe_all_features, size)


Varimp_data_all = data.frame(feature = row.names(varImp(result_rfe_all_features))[1:size],
                          importance = varImp(result_rfe_all_features)[1:size, 1])

ggplot(data=Varimp_data_all, 
       aes(x = reorder(feature, -importance), y = importance, fill = feature)) +
  geom_bar(stat="identity") + labs(x = "Features", y = "Variable Importance") + 
  # geom_text(aes(label = round(importance, 1)), vjust=1.6, color="white", size=4) + 
  theme_bw() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```



```{r - LASSO - DONE, not sure if worth, needs more thought}
#########
# LASSO #
#########
lasso_cv = cv.glmnet(as.matrix(train_data[,1:410]), as.double(train_data$Class), family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='mse', keep=T)

plot(lasso_cv)
# lasso_cv$lambda.min
cat('Min Lambda: ', lasso_cv$lambda.min, '\n 1Sd Lambda: ', lasso_cv$lambda.1se)

# could use multinomial if you can extract names from this
# coef(lasso_cv, s=lasso_cv$lambda.min)


# family has to be binomial to get coef otherwise multinomial
df_lass_cv_coef = round(as.matrix(coef(lasso_cv, s=lasso_cv$lambda.min)), 4)

# See all contributing variables/not shrunk to 0
df_lass_cv_coef_survivors = df_lass_cv_coef[df_lass_cv_coef[, 1] != 0, ]


feature_names = rownames(df_lass_cv_coef)[order(df_lass_cv_coef, decreasing=TRUE)][1:20]

tmp_df = data.frame(feature_names)

LASSO_features_data = subset(train_data, select = tmp_df$feature_names[2:19])


```



```{r - Boruta - DONE}

##########
# Boruta #
##########

boruta_output = Boruta(Class ~ ., data=train_data, doTrace=0)  

boruta_signif_with_tentative = getSelectedAttributes(boruta_output, withTentative = TRUE)

imps_with_tentative = attStats(boruta_output)
imps2_with_tentative = imps_with_tentative[imps_with_tentative$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2_with_tentative[order(-imps2_with_tentative$meanImp), ]) 
Boruta_features_data_with_tentative = subset(train_data, select = rownames(imps2_with_tentative))

##########################
# no tentative variables #
##########################

boruta_without_tentative = TentativeRoughFix(boruta_output)
boruta_signif_without_tentative = getSelectedAttributes(boruta_output, withTentative = FALSE)
boruta_signif_2 = getSelectedAttributes(boruta_output)

imps_without_tentative = attStats(boruta_without_tentative)
imps2_without_tentative = imps_without_tentative[imps_without_tentative$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2_without_tentative[order(-imps2_without_tentative$meanImp), ]) 
Boruta_features_data_without_tentative = subset(train_data, select = rownames(imps2_without_tentative))


# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  




```



```{r - rpart - DONE}

##################################
# rpart model feature importance #
################################## 

rpart = train(Class ~ ., data=train_data, method="rpart")
plot(rpart)
rpartImp = varImp(rpart)

rpart_features_data = get_top_features(rpart, 5)

rpart_features_data


```


```{r - Stepwise - }
############
# Stepwise #
############

# Step 1: Define base intercept only model
lm_base = lm(Class ~ 1 , data=train_data)  

# Step 2: Full model with all predictors
lm_full = lm(Class ~ . , data=train_data) 

# Step 3: Perform step-wise algorithm. direction='both' implies both forward and backward stepwise
stepwise_m = step(lm_base, scope = list(lower = lm_base, upper = lm_full), direction = "both", trace = 0, steps = 1000)  

# Step 4: Get the shortlisted variable.
shortlisted_vars = names(unlist(stepwise_m[[1]])) 
shortlisted_vars = shortlisted_vars[!shortlisted_vars %in% "(Intercept)"] # remove intercept

# Show
print(shortlistedVars)


```

```{r - QVT - }




```




```{r - Feature selection, echo=TRUE, message=TRUE, warning=TRUE, paged.print=TRUE}

# https://academic.oup.com/comjnl/advance-article-abstract/doi/10.1093/comjnl/bxac033/6565626?redirectedFrom=fulltext




#######
# GLM #
#######
# Full interaction model 
cols_list = colnames(train_data)
cols = cols_list[cols_list != "Id"]
tmp_data = subset(tmp_data, select=c(cols))

# memory error......
# glm1 = glm(Class ~ (.)^2, family=binomial, data = tmp_data)

glm1 = glm(Class ~ ., family=binomial, data = tmp_data)

summary(glm1)

# TODO not working very well





#########################
# IG (Information Gain) #
#########################


# TODO

#
# SVM - SCAD #
# 

# https://cran.r-project.org/web/packages/penalizedSVM/penalizedSVM.pdf
# https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-12-138



################################
# relative importance using LR #
################################

# install.packages('relaimpo')
library(relaimpo)

# Build linear regression model
lm_model = lm(Class ~ ., data=data_with_dummy_var[,1:411])

# calculate relative importance
rel_imp = calc.relimp(lm_model, type = "lmg", rela = F)  

# Sort
cat('Relative Importances: \n')
sort(round(rel_imp$lmg, 3), decreasing=TRUE)

bootsub = boot.relimp(Class ~ ., data=data_with_dummy_var[,1:411],
                       b = 1000, type = 'lmg', rank = TRUE, diff = TRUE)

plot(booteval.relimp(bootsub, level=.95))



# TODO 
# read about DALEX for feature importance 
library(DALEX)

# Variable importance with DALEX
explained_rf = explain(random_forest, data=data=data_with_dummy_var[,1:411], y=data_with_dummy_var$Class)

# Get the variable importances
varimps = variable_dropout(explained_rf, type='raw')

print(varimps)

#######
# QVT #
#######
# TODO implement this or see if you have time to

##########
# QVT(B) #
##########

##########
# QVT(L) #
##########

##########
# QVT(X) #
##########

##########
# QVT(I) #
##########




```



```{r - SVM Baseline - Implemented}
sample_sub = read.csv(file='mlsp-2014-mri/submission_example.csv', head=TRUE,sep=",")

getModelInfo(model = "svmPoly")

# best params {'C': 100, 'cache_size': 200, 'coef0': 0.7, 'gamma': 0.0001, 'kernel': 'poly', 'random_state': 0} rs 104
set.seed(104)
train_control = trainControl(method = "cv", number = 10, search = "grid")

svm_model_poly = svm(Class ~ ., data = train_sample_data, kernel = "polynomial", cost=100, coef0= 0.7, cachesize=200, gamma=0.0001, scale = TRUE) 
# scale = True because ive 86 features
# tune poly look for better degree
pred = predict(svm_model_poly, test_data)

sample_sub$Probability = pred

write.csv(sample_sub,file='new_submission.csv',row.names=FALSE)

svm_model_rbf = svm(Class ~ ., data = train_sample_data, scale = TRUE) # rbf
pred = predict(svm_model_rbf, test_data, type="Class")

svm_model_linear = svm(Class ~ ., data = train_sample_data, kernel = "linear", scale = TRUE)
pred = predict(svm_model_linear, test_data, type="Class")

svm_model_sigmoid = svm(Class ~ ., data = train_sample_data, kernel = "sigmoid", scale = TRUE)
pred = predict(svm_model_sigmoid, test_data, type="Class")

```



```{r - DWD Baseline - Missing}

```




```{r - sparseSVM - Implemented}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(train_sample_data, select = -c(Class))
# convert to matrix
x = as.matrix(tmp_x)
sparse_svm = sparseSVM(x, y)

tmp_test_data = as.matrix(tmp_x)

pred = predict(sparse_svm, tmp_test_data, lambda = c(0.2, 0.1))



sparse_svm_cross_val = cv.sparseSVM(x, y, nfolds = 5, ncores = 2, seed = 1234)

pred = predict(sparse_svm_cross_val, tmp_test_data, lambda = c(0.2, 0.1))


```




```{r - SVMMaj - Implemented}
# depends on running setup in sparseSVM

# TODO look up plotting functions for this alg
svm_maj = svmmaj(x, y, hinge = 'quadratic', lambda = 1)


summary(svm_maj)

# weights.obs = list(positive = 2, negative = 1)
# I want to penalise negatives more, right?
weights.obs = list(positive = 1, negative = 2)
weights.obs
## using radial basis kernel
svm_maj_v2 = svmmaj(x, y, hinge = 'quadratic', lambda = 1, 
                weights.obs = weights.obs, scale = 'interval', kernel = rbfdot, kernel.sigma = 1)

summary(svm_maj_v2)

## I-spline basis
svm_maj_v3 = svmmaj(x, y, weight.obs = weight.obs, spline.knots = 3, spline.degree = 2)
plotWeights(svm_maj_v3, plotdim = c(2, 4))

pred = predict(svm_maj, tmp_test_data)
pred = predict(svm_maj_v2, tmp_test_data)
pred = predict(svm_maj_v3, tmp_test_data)

```




```{r - WeightSVM - Work in prog}
# depends on running setup in sparseSVM

# reset data - clean this up and write why you did it so you dont forget
# TODO read above
# https://www.rdocumentation.org/packages/WeightSVM/versions/1.7-9/topics/wsvm
# https://cran.r-project.org/web/packages/WeightSVM/WeightSVM.pdf

tmp_data = createDataPartition(y = train_data$Class, p = 0.8, list=FALSE)

# seperate test and training sets
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]

# this worked
weight_svm = wsvm(Class ~ ., weight = rep(1,69), data = train_sample_data)

pred = predict(weight_svm, tmp_test_data, lambda = c(0.2, 0.1))


```




```{r - SparseDWD - Work in prog}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(train_sample_data, select = -c(Class))
# convert to matrix
x = as.matrix(tmp_x)
sparse_dwd = sdwd(x, y, lambda2=1)

tmp_test_data = as.matrix(tmp_x)

pred = predict(sparse_dwd, tmp_test_data)


```




```{r - kerndwd}
# https://cran.r-project.org/web/packages/kerndwd/kerndwd.pdf

# fit a linear DWD
kern = vanilladot()
dwd_linear = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_linear, kern, x, test_data)



# fit a DWD using Gaussian kernel
kern = rbfdot(sigma=1)
dwd_gaussian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_gaussian, kern, x, test_data)


# fit a weighted kernel DWD
kern = rbfdot(sigma=1)
weights = c(1, 2)[factor(y)]
dwd_weighted_gaussian = kerndwd(x, y, kern, qval=1, wt = weights, eps=1e-5, maxit=1e5)
pred = predict(dwd_weighted_gaussian, kern, x, test_data)


# fit polynomial kernel
kern = polydot(degree = 1, scale = 1, offset = 1)
dwd_polynomial = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_polynomial, kern, x, test_data)



# TODO https://discuss.analyticsvidhya.com/t/how-to-resolve-error-na-nan-inf-in-foreign-function-call-arg-6-in-knn/7280/3
# fit laplacian kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = laplacedot(sigma = 1)
dwd_laplacian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_laplacian, kern, x, test_data)


# fit bessel kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = besseldot(sigma = 1, order = 1, degree = 1)
nobs = nrow(test_data)
dwd_bessel = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_bessel, kern, x, test_data)


# fit anova rbf kernel where rbf is a gaussian kernel 
kern = anovadot(sigma = 1, degree = 1)
dwd_anova = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_anova, kern, x, test_data)


# fit spline kernel - Error: cannot allocate vector of size 106.8 Gb
kern = splinedot()
dwd_spline = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_spline, kern, x, test_data)


# TODO add models to list and loop over them later - read about optimizing these kernels later

# sigest(x)






```




```{r - DWDLargeR}


# calculate the best penalty parameter
penalty_param = penaltyParameter(x, y,expon=1)
# solve the generalized DWD model
dwd_larger_r = genDWD(x, y, penalty_param, expon=1)

pred = predict(dwd_larger_r, test_data)

# TODO Error in genDWD(x, y, penalty_param, expon = 1) : trying to get slot "ra" from an object of a basic class ("matrix") with no slots
# Im tired do this later

```



```{r - TODO write functions here to gather statistics, put into df and write out, loop over models and append etc}
# TODO read title





```