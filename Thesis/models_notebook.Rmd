---
title: "R Notebook"
output: html_notebook
---



```{r - setup, reload data}

# QVT setup 
library(irr)
library(ROCR)
library(infotheo)


# install.packages("penalizedSVM")

library(sparseSVM)
library(SVMMaj)
library(WeightSVM)
library(e1071) # SVM package 
library(kernlab) # SVM package/s - has multiple libs such as LiblineaR, kernlab, e1071, obliqueRF
library(penalizedSVM) # needed for Smoothly clipped absolute deviation penalty (SCAD)
library(Metrics)


# Notes for kernlab - seems to have e1071 svm impl too aswell as other libraries
# https://topepo.github.io/caret/available-models.html 
# https://stackoverflow.com/questions/18911338/vastly-different-results-for-svm-model-using-e1071-and-caret
# https://www.thekerneltrip.com/statistics/kernlab-vs-e1071/#:~:text=While%20kernlab%20implements%20kernel%2Dbased,bagged%20clustering%2C%20naive%20Bayes%20classifier. 





# library(rdwd) # DWD package
library(sdwd) # SparseDWD
library(kerndwd)
library(DWDLargeR)

# Other
library(caret) # data split - https://rdrr.io/rforge/caret/man/createDataPartition.html
library(mlbench) # https://cran.r-project.org/web/packages/mlbench/mlbench.pdf
library(ggplot2) # lib for making plots
library(Boruta)
library(glmnet)
library(elasticnet)
library(randomForest)


# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html
library(kableExtra) # Used to generate report ready tables

getwd()


y_labels_train = read.csv(file='mlsp-2014-mri/Train/train_labels.csv', head=TRUE, sep=",") # this is your y var
# Convert 'Class' into an unordered categorical variable, and assign labels
# to each level.

y_labels_train$Class = factor(y_labels_train$Class) 

# labels=c('Healthy_Control','Schizophrenic_Patient')
# 46 control # 40 patient 

############################
# loading and merging data #
############################

FNC_train = read.csv(file='mlsp-2014-mri/Train/train_FNC.csv',head=TRUE,sep=",")
SBM_train = read.csv(file='mlsp-2014-mri/Train/train_SBM.csv',head=TRUE,sep=",")
train_data = merge(FNC_train, SBM_train, by = "Id")
train_data = merge(train_data, y_labels_train, by="Id")

# These dont have lables so its pointless to use for testing
SBM_test = read.csv(file='mlsp-2014-mri/Test/test_SBM.csv',head=TRUE,sep=",")
FNC_test = read.csv(file='mlsp-2014-mri/Test/test_FNC.csv',head=TRUE,sep=",")
test_data = merge(FNC_test, SBM_test, by = "Id")

#######################################
# data 80% - 20% split for train-test #
#######################################
train_data = train_data[,2:412]

# https://rdrr.io/rforge/caret/man/createDataPartition.html
# get indices for 70% of the data set
tmp_data = createDataPartition(y = train_data$Class, p = 0.7, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]


set_new_data_after_feature_selection=function(train_data){
  tmp_data = createDataPartition(y = train_data$Class, p = 0.7, list=FALSE)
  train_sample_data = RRF_features_data[tmp_data,]
  test_sample_data = RRF_features_data[-tmp_data,]
}

data_split=function(input_ds){
  tmp_data = createDataPartition(y = input_ds$Class, p = 0.7, list=FALSE)
  return(tmp_data)
}

get_top_features=function(features, size, feature_names){
  
  if(missing(feature_names)){
    var_imp = varImp(features)
    tmp_df = data.frame(var_imp[1])
  
    if(missing(size)) {
          feature_names = rownames(tmp_df)[order(tmp_df$Overall, decreasing=TRUE)]
  
      } else {
          feature_names = rownames(tmp_df)[order(tmp_df$Overall, decreasing=TRUE)][1:size]
  
      }
  }
  
  features_subset_df = subset(train_data, select = feature_names)
  return(features_subset_df)

}

get_data_for_kaggle_score=function(preds){
  sample_sub = read.csv(file='mlsp-2014-mri/submission_example.csv', head=TRUE,sep=",")
  sample_sub$Probability = preds
  write.csv(sample_sub,file='new_submission.csv', row.names=FALSE)
}

get_test_metrics=function(test_sample_data, preds){
  
  #sse = sse(as.numeric(test_sample_data$Class), preds)
  #RMSE = RMSE(as.numeric(test_sample_data$Class), preds)
  #mse = mse(as.numeric(test_sample_data$Class), preds)
  #R2 = R2(preds, as.numeric(test_sample_data$Class))
  auc = Metrics::auc(test_sample_data$Class, preds)
  cf = confusionMatrix(as.factor(preds), as.factor(test_sample_data$Class), mode="everything", positive = "1")
  
  print(cf)
  print(paste0("AUC is: ", round(auc, 2)))
  #print(paste0("SSE is: ", sse))
  #print(paste0("RMSE is: ", RMSE))
  #print(paste0("mse is: ", mse))
  #print(paste0("R2 is: ", round(R2, 2)))
  
}


```


```{r - Regularized Random Forest - DONE - TODO go higher than 25}

# 0 = 'Healthy Control', 1 = 'Schizophrenic Patient'

#############################
# Regularized Random Forest #
#############################
# making a dummy var, something you'd use in general linear regression

#tmp_data = cbind(train_data, rnorm(1:dim(train_data)[1]))
#colnames(tmp_data)[412] = 'dummy_var'
#cols_list = colnames(tmp_data)
#cols = cols_list[cols_list != "Id"]
#data_with_dummy_var = subset(tmp_data, select=c(cols))


size=25
RRF = train(Class ~ ., data=train_data, method="RRF")
RRF_features = varImp(RRF)
feature_names = data.frame(RRF_features[1])
feature_names = rownames(feature_names)[order(feature_names$Overall, decreasing=TRUE)][1:size]
feature_names = append(feature_names, "Class")
feature_names

# plot(RRF_features, top = size, main='Variable Importance')
size= 26
RRF_features_data =  get_top_features(RRF, size, feature_names)

tmp_data = createDataPartition(y = RRF_features_data$Class, p = 0.7, list=FALSE)
train_sample_data = RRF_features_data[tmp_data,]
test_sample_data = RRF_features_data[-tmp_data,]


```


```{r - RFE - DONE}

# TODO investigate what to do with this if boruta is being used

#######
# RFE # 
#######
# https://topepo.github.io/caret/feature-selection-using-genetic-algorithms.html


# 203 was the highest you can go without including terms that lower model preformance 
train_control = rfeControl(functions=rfFuncs, method = "cv", number = 10)
size = 203

result_rfe_all_features = rfe(x = train_data[,1:411], 
                   y = train_data$Class, 
                   # sizes = 410, # best all features
                   # 410 found that some features were lowering accuracy
                   sizes = size,
                   rfeControl = train_control)

#predictors(result_rfe_all_features)

result_rfe_all_features$variables

RFE_features_data = get_top_features(result_rfe_all_features, size)


tmp_data = createDataPartition(y = RFE_features_data$Class, p = 0.7, list=FALSE)
train_sample_data_rfe = RFE_features_data[tmp_data,]
test_sample_data_rfe = RFE_features_data[-tmp_data,]
train_sample_data = train_sample_data_rfe
test_sample_data = test_sample_data_rfe

Varimp_data_all = data.frame(feature = row.names(varImp(result_rfe_all_features))[1:size],
                          importance = varImp(result_rfe_all_features)[1:size, 1])

ggplot(data=Varimp_data_all, 
       aes(x = reorder(feature, -importance), y = importance, fill = feature)) +
  geom_bar(stat="identity") + labs(x = "Features", y = "Variable Importance") + 
  # geom_text(aes(label = round(importance, 1)), vjust=1.6, color="white", size=4) + 
  theme_bw() + theme(legend.position = "none", axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))




```


```{r - LASSO - DONE}
#########
# LASSO #
#########
size = 20
lasso_cv = cv.glmnet(as.matrix(train_data[,1:410]), as.double(train_data$Class), family='binomial', alpha=1, parallel=TRUE, standardize=TRUE, type.measure='mse', keep=T)

plot(lasso_cv)
# lasso_cv$lambda.min
cat('Min Lambda: ', lasso_cv$lambda.min, '\n 1Sd Lambda: ', lasso_cv$lambda.1se)

# could use multinomial if you can extract names from this
# coef(lasso_cv, s=lasso_cv$lambda.min)


# family has to be binomial to get coef otherwise multinomial
df_lass_cv_coef = round(as.matrix(coef(lasso_cv, s=lasso_cv$lambda.min)), 4)

# See all contributing variables/not shrunk to 0
df_lass_cv_coef_survivors = df_lass_cv_coef[df_lass_cv_coef[, 1] != 0, ]


feature_names = rownames(df_lass_cv_coef)[order(df_lass_cv_coef, decreasing=TRUE)][1:size]
feature_names = append(feature_names, "Class")
feature_names = feature_names[feature_names != "(Intercept)"]
feature_names
LASSO_features_data =  get_top_features(lasso_cv, size, feature_names)

tmp_data = createDataPartition(y = LASSO_features_data$Class, p = 0.7, list=FALSE)
train_sample_data = LASSO_features_data[tmp_data,]
test_sample_data = LASSO_features_data[-tmp_data,]

```


```{r - Boruta - DONE}

##########
# Boruta #
##########

boruta_output = Boruta(Class ~ ., data=train_sample_data, doTrace=0)  

#boruta_signif_with_tentative = getSelectedAttributes(boruta_output, withTentative = TRUE)

#imps_with_tentative = attStats(boruta_output)
#imps2_with_tentative = imps_with_tentative[imps_with_tentative$decision != 'Rejected', c('meanImp', 'decision')]
#head(imps2_with_tentative[order(-imps2_with_tentative$meanImp), ]) 
#Boruta_features_data_with_tentative = subset(train_sample_data, select = rownames(imps2_with_tentative))

##########################
# no tentative variables #
##########################

boruta_without_tentative = TentativeRoughFix(boruta_output)
boruta_signif_without_tentative = getSelectedAttributes(boruta_output, withTentative = FALSE)
boruta_signif_2 = getSelectedAttributes(boruta_output)

imps_without_tentative = attStats(boruta_without_tentative)
imps2_without_tentative = imps_without_tentative[imps_without_tentative$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2_without_tentative[order(-imps2_without_tentative$meanImp), ]) 
feature_names = rownames(imps2_without_tentative)
feature_names = append(feature_names, "Class")
feature_names
Boruta_features_data_without_tentative = get_top_features(feature_names = feature_names)


# Plot variable importance
plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")  
Boruta_features_data_without_tentative
tmp_data = createDataPartition(y = Boruta_features_data_without_tentative$Class, p = 0.7, list=FALSE)
train_sample_data = Boruta_features_data_without_tentative[tmp_data,]
test_sample_data = Boruta_features_data_without_tentative[-tmp_data,]


```


```{r - rpart - DONE}

##################################
# rpart model feature importance #
################################## 

rpart = train(Class ~ ., data=train_sample_data, method="rpart")
plot(rpart)
rpartImp = varImp(rpart)

rpart_features_data = get_top_features(rpart, 5)

feature_names = colnames(rpart_features_data)
feature_names = append(feature_names, "Class")

rpart_features_data = subset(train_data, select = feature_names)

tmp_data = createDataPartition(y = rpart_features_data$Class, p = 0.7, list=FALSE)
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]

```


```{r - QVT - }




```


```{r - SVM Baseline - Done}
# best params {'C': 100, 'cache_size': 200, 'coef0': 0.7, 'gamma': 0.0001, 'kernel': 'poly', 'random_state': 0} rs 104



svm_model_poly = svm(Class ~ ., data = train_sample_data, kernel = "polynomial",
                     cost=100, coef0= 0.7, cachesize=200, gamma=0.0001, scale = TRUE) 

preds = predict(svm_model_poly, newdata = test_sample_data)
from_svm = preds
auc = Metrics::auc(tmp_test_sample$Class, preds)
auc
preds = pred_v2
preds = as.numeric(levels(preds))[preds]
preds
from_svm
get_test_metrics(test_sample_data, preds)

# test using kaggle dataset 
preds = predict(svm_model_poly, newdata = test_data)
get_data_for_kaggle_score(preds = preds)


#################
# svm_model_rbf #
#################

svm_model_rbf = svm(Class ~ ., data = train_sample_data, kernel = 'radial', scale = TRUE) # rbf
preds = predict(svm_model_rbf, test_sample_data, type="Class", scale = TRUE)

preds = as.numeric(levels(preds))[preds]

get_test_metrics(test_sample_data, preds)

# test using kaggle dataset 
preds = predict(svm_model_rbf, newdata = test_data)
get_data_for_kaggle_score(preds = preds)

####################
# svm_model_linear #
####################

svm_model_linear = svm(Class ~ ., data = train_sample_data, kernel = "linear", scale = TRUE)

preds = predict(svm_model_linear, test_sample_data, type="Class")


preds = as.numeric(levels(preds))[preds]

get_test_metrics(test_sample_data, preds)

# test using kaggle dataset 
preds = predict(svm_model_linear, newdata = test_data)
get_data_for_kaggle_score(preds = preds)


#####################
# svm_model_sigmoid #
#####################

svm_model_sigmoid = svm(Class ~ ., data = train_sample_data, kernel = "sigmoid", scale = TRUE)
preds = predict(svm_model_sigmoid, test_sample_data, type="Class")



preds = as.numeric(levels(preds))[preds]

get_test_metrics(test_sample_data, preds)

# test using kaggle dataset
preds = predict(svm_model_sigmoid, newdata = test_data)

get_data_for_kaggle_score(preds = preds)
```


```{r - sparseSVM - may be excluded}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

#tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
#train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(test_sample_data)
tmp_x
# convert to matrix
x = as.matrix(tmp_x)
sparse_svm = sparseSVM(x, y)
sparse_svm

tmp_test_data = as.matrix(tmp_x)
tmp_test_data
class(tmp_test_data)
preds = predict(sparse_svm, tmp_test_data, lambda = c(0.2, 0.1))

sparse_svm_cross_val = cv.sparseSVM(x, y, nfolds = 5, ncores = 2, seed = 1234)

preds = predict(sparse_svm_cross_val, tmp_test_data, lambda = c(0.2, 0.1))

preds = predict(sparse_svm_cross_val, tmp_test_data)

preds = replace(preds, preds == -1, 0)
preds


test_sample_data$Class
cf = confusionMatrix(as.factor(preds), as.factor(test_sample_data$Class), mode="everything", positive = "1")


preds = predict(svm_model_poly, newdata = test_sample_data)
preds = as.numeric(levels(preds))[preds]


get_test_metrics(test_sample_data, preds)

# test using kaggle dataset 
preds = predict(svm_model_poly, newdata = test_data)
get_data_for_kaggle_score(preds = preds)

```


```{r - SVMMaj - Deprecated it seems, no support to work with external libraries for metrics}
# depends on running setup in sparseSVM

# TODO look up plotting functions for this alg
svm_maj = svmmaj(train_sample_data, train_sample_data$Class, hinge = 'quadratic', lambda = 1)


summary(svm_maj)

# weights.obs = list(positive = 2, negative = 1)
# I want to penalise negatives more, right?
weights.obs = list(positive = 1, negative = 2)
weights.obs
## using radial basis kernel
svm_maj_v2 = svmmaj(train_sample_data, train_sample_data$Class, hinge = 'quadratic', lambda = 1, 
                weights.obs = weights.obs, scale = 'interval', kernel = rbfdot, kernel.sigma = 1)

summary(svm_maj_v2)

## I-spline basis
svm_maj_v3 = svmmaj(x, y, weight.obs = weight.obs, spline.knots = 3, spline.degree = 2)
plotWeights(svm_maj_v3, plotdim = c(2, 4))

preds = predict(svm_maj, test_sample_data, type="Class")
pred

var_imp = varImp(preds)
tmp_df = data.frame(var_imp[1])
    
preds = as.numeric(levels(preds))[preds]

get_test_metrics(test_sample_data, preds)

pred = predict(svm_maj_v2, tmp_test_data)
pred = predict(svm_maj_v3, tmp_test_data)

```


```{r - WeightSVM - may be excluded}

weight_svm = wsvm(Class ~ ., weight = rep(1,69), data = train_sample_data)

pred = predict(weight_svm, tmp_test_data, lambda = c(0.2, 0.1))


```


```{r - SparseDWD - may be excluded}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(train_sample_data, select = -c(Class))
# convert to matrix
x = as.matrix(tmp_x)
sparse_dwd = sdwd(x, y, lambda2=1)

tmp_test_data = as.matrix(tmp_x)

pred = predict(sparse_dwd, tmp_test_data)
pred

```


```{r - kerndwd - cant collect metrics}

# Save references 
tmp_test_sample = test_sample_data
tmp_train_sample = train_sample_data

# restore references
test_sample_data = tmp_test_sample
train_sample_data = tmp_train_sample



# OLD FORMATING 
#x = as.matrix(data.frame(lapply(train_sample_data, as.numeric)))
#f = train_sample_data$Class
#y = as.numeric(levels(f))[f]

train_sample_data = data.frame(lapply(train_sample_data, as.numeric))
test_sample_data = data.frame(lapply(test_sample_data, as.numeric))

# REQUIRED format data for kern function, i guess?
# replace(test_sample_data$Class, test_sample_data$Class==0, -1)
# The order of conversion was wrong first 
test_sample_data$Class[test_sample_data$Class == 1] = 0
train_sample_data$Class[train_sample_data$Class == 1] = 0
test_sample_data$Class[test_sample_data$Class == 2] = 1
train_sample_data$Class[train_sample_data$Class == 2] = 1


test_sample_data$Class[test_sample_data$Class == 0] = -1
train_sample_data$Class[train_sample_data$Class == 0] = -1

# fit a linear DWD
kern = vanilladot()
dwd_linear = kerndwd(train_sample_data[1:410], train_sample_data$Class, kern, lambda = 1)

# why is the pred being squished down to [1:25 1:100] from 
train_sample_data = as.matrix(train_sample_data)
preds = predict(dwd_linear, kern, train_sample_data, test_sample_data)
preds
# revert value sanitazation for model so results can be used for gathering metrics 
store_for_svm = pred
pred = replace(preds, preds==-1, 0)
test_sample_data$Class[test_sample_data$Class == -1] = 0

# metrics tinkering 
auc = Metrics::auc(tmp_test_sample$Class, pred)

pred_v2 = as.factor(pred)
cf = confusionMatrix(pred_v2, as.factor(test_sample_data$Class), mode="everything", positive = "1")

get_test_metrics(tmp_test_sample, preds)

# test using kaggle dataset 
preds_v3 = predict(dwd_linear, kern, train_sample_data, test_data)
get_data_for_kaggle_score(preds = preds_v3)


##############
# STOP BLOCK #
##############


# fit a DWD using Gaussian kernel
kern = rbfdot(sigma=1)
dwd_gaussian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_gaussian, kern, x, test_data)

# fit a weighted kernel DWD
kern = rbfdot(sigma=1)
weights = c(1, 2)[factor(y)]
dwd_weighted_gaussian = kerndwd(x, y, kern, qval=1, wt = weights, eps=1e-5, maxit=1e5)
pred = predict(dwd_weighted_gaussian, kern, x, test_data)


# fit polynomial kernel
kern = polydot(degree = 1, scale = 1, offset = 1)
dwd_polynomial = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_polynomial, kern, x, test_data)



# TODO https://discuss.analyticsvidhya.com/t/how-to-resolve-error-na-nan-inf-in-foreign-function-call-arg-6-in-knn/7280/3
# fit laplacian kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = laplacedot(sigma = 1)
dwd_laplacian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_laplacian, kern, x, test_data)


# fit bessel kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = besseldot(sigma = 1, order = 1, degree = 1)
nobs = nrow(test_data)
dwd_bessel = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_bessel, kern, x, test_data)


# fit anova rbf kernel where rbf is a gaussian kernel 
kern = anovadot(sigma = 1, degree = 1)
dwd_anova = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_anova, kern, x, test_data)


# fit spline kernel - Error: cannot allocate vector of size 106.8 Gb
kern = splinedot()
dwd_spline = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_spline, kern, x, test_data)


# TODO add models to list and loop over them later - read about optimizing these kernels later

# sigest(x)

```


```{r - DWDLargeR}


# calculate the best penalty parameter
penalty_param = penaltyParameter(x, y,expon=1)
# solve the generalized DWD model
dwd_larger_r = genDWD(x, y, penalty_param, expon=1)

pred = predict(dwd_larger_r, test_data)

# TODO Error in genDWD(x, y, penalty_param, expon = 1) : trying to get slot "ra" from an object of a basic class ("matrix") with no slots
# Im tired do this later

```


