---
title: "R Notebook"
output: html_notebook
---



```{r - setup, reload data}
library(sparseSVM)
library(SVMMaj)
library(WeightSVM)
library(e1071) # SVM package 
library(kernlab) # SVM package/s - has multiple libs such as LiblineaR, kernlab, e1071, obliqueRF

# Notes for kernlab - seems to have e1071 svm impl too aswell as other libraries
# https://topepo.github.io/caret/available-models.html 
# https://stackoverflow.com/questions/18911338/vastly-different-results-for-svm-model-using-e1071-and-caret
# https://www.thekerneltrip.com/statistics/kernlab-vs-e1071/#:~:text=While%20kernlab%20implements%20kernel%2Dbased,bagged%20clustering%2C%20naive%20Bayes%20classifier. 

# library(rdwd) # DWD package
library(sdwd) # SparseDWD
library(kerndwd)
library(DWDLargeR)

library(caret) # data split - https://rdrr.io/rforge/caret/man/createDataPartition.html
library(mlbench) # https://cran.r-project.org/web/packages/mlbench/mlbench.pdf
library(ggplot2)

getwd()


y_labels_train = read.csv(file='mlsp-2014-mri/Train/train_labels.csv', head=TRUE, sep=",") # this is your y var
# Convert 'Class' into an unordered categorical variable, and assign labels
# to each level.
y_labels_train$Class = factor(y_labels_train$Class) 
# labels=c('Healthy_Control','Schizophrenic_Patient')
# 46 control # 40 patient 

############################
# loading and merging data #
############################

FNC_train = read.csv(file='mlsp-2014-mri/Train/train_FNC.csv',head=TRUE,sep=",")
SBM_train = read.csv(file='mlsp-2014-mri/Train/train_SBM.csv',head=TRUE,sep=",")
train_data = merge(FNC_train, SBM_train, by = "Id")
train_data = merge(train_data, y_labels_train, by="Id")


SBM_test = read.csv(file='mlsp-2014-mri/Test/test_SBM.csv',head=TRUE,sep=",")
FNC_test = read.csv(file='mlsp-2014-mri/Test/test_FNC.csv',head=TRUE,sep=",")
test_data = merge(FNC_test, SBM_test, by = "Id")

#######################################
# data 70% - 30% split for train-test #
#######################################
# https://rdrr.io/rforge/caret/man/createDataPartition.html
# get indices for 70% of the data set
tmp_data = createDataPartition(y = train_data$Class, p = 0.8, list=FALSE)

# seperate test and training sets
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]


```


```{temp}

"""
param_grid = expand.grid(C=c(1,2,3,4,5),
                         degree=c(1,2,3,4),
                         scale=seq(0,0.1, by=0.001))

svm_model_poly = train(Class ~ ., 
                       data = train_sample_data, 
                       method = "svmPoly",
                       trControl = train_control,
                       tuneLength = 10,
                       tuneGrid =param_grid,
                       verbose=FALSE)

svm_model_poly = svm_model_poly$finalModel


tmp = tune(svm, 
           Class~., 
           data = train_sample_data,
           kernel = "polynomial",
           ranges = list(cost = c(100, 75, 50, 25, 10), gamma = c(0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006), scale = TRUE, coef0=0.7),
           tunecontrol = tune.control(sampling = "cross")
         )

summary(tmp)

svm_model_poly = tmp$best.model
"""
```


```{r - SVM Baseline - Implemented}
sample_sub = read.csv(file='mlsp-2014-mri/submission_example.csv',head=TRUE,sep=",")

getModelInfo(model = "svmPoly")

# best params {'C': 100, 'cache_size': 200, 'coef0': 0.7, 'gamma': 0.0001, 'kernel': 'poly', 'random_state': 0} rs 104
set.seed(104)
train_control = trainControl(method = "cv", number = 10, search = "grid")

svm_model_poly = svm(Class ~ ., data = train_sample_data, kernel = "polynomial", cost=100, coef0= 0.7, cachesize=200, gamma=0.0001, scale = TRUE) 
# scale = True because ive 86 features
# tune poly look for better degree
pred = predict(svm_model_poly, test_data)

sample_sub$Probability = pred

write.csv(sample_sub,file='new_submission.csv',row.names=FALSE)

svm_model_rbf = svm(Class ~ ., data = train_sample_data, scale = TRUE) # rbf
pred = predict(svm_model_rbf, test_data, type="Class")

svm_model_linear = svm(Class ~ ., data = train_sample_data, kernel = "linear", scale = TRUE)
pred = predict(svm_model_linear, test_data, type="Class")

svm_model_sigmoid = svm(Class ~ ., data = train_sample_data, kernel = "sigmoid", scale = TRUE)
pred = predict(svm_model_sigmoid, test_data, type="Class")

```



```{r - DWD Baseline - Missing}

```




```{r - sparseSVM - Implemented}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(train_sample_data, select = -c(Class))
# convert to matrix
x = as.matrix(tmp_x)
sparse_svm = sparseSVM(x, y)

tmp_test_data = as.matrix(tmp_x)

pred = predict(sparse_svm, tmp_test_data, lambda = c(0.2, 0.1))



sparse_svm_cross_val = cv.sparseSVM(x, y, nfolds = 5, ncores = 2, seed = 1234)

pred = predict(sparse_svm_cross_val, tmp_test_data, lambda = c(0.2, 0.1))


```




```{r - SVMMaj - Implemented}
# depends on running setup in sparseSVM

# TODO look up plotting functions for this alg
svm_maj = svmmaj(x, y, hinge = 'quadratic', lambda = 1)


summary(svm_maj)

# weights.obs = list(positive = 2, negative = 1)
# I want to penalise negatives more, right?
weights.obs = list(positive = 1, negative = 2)
weights.obs
## using radial basis kernel
svm_maj_v2 = svmmaj(x, y, hinge = 'quadratic', lambda = 1, 
                weights.obs = weights.obs, scale = 'interval', kernel = rbfdot, kernel.sigma = 1)

summary(svm_maj_v2)

## I-spline basis
svm_maj_v3 = svmmaj(x, y, weight.obs = weight.obs, spline.knots = 3, spline.degree = 2)
plotWeights(svm_maj_v3, plotdim = c(2, 4))

pred = predict(svm_maj, tmp_test_data)
pred = predict(svm_maj_v2, tmp_test_data)
pred = predict(svm_maj_v3, tmp_test_data)

```




```{r - WeightSVM - Work in prog}
# depends on running setup in sparseSVM

# reset data - clean this up and write why you did it so you dont forget
# TODO read above
# https://www.rdocumentation.org/packages/WeightSVM/versions/1.7-9/topics/wsvm
# https://cran.r-project.org/web/packages/WeightSVM/WeightSVM.pdf

tmp_data = createDataPartition(y = train_data$Class, p = 0.8, list=FALSE)

# seperate test and training sets
train_sample_data = train_data[tmp_data,]
test_sample_data = train_data[-tmp_data,]

# this worked
weight_svm = wsvm(Class ~ ., weight = rep(1,69), data = train_sample_data)

pred = predict(weight_svm, tmp_test_data, lambda = c(0.2, 0.1))


```




```{r - SparseDWD - Work in prog}
# Convert factor back to num array without loosing information 
# TODO look up plotting functions for this alg

tmp_data = createDataPartition(y = train_data$Class, p = 1, list=FALSE)

# separate test and training sets
train_sample_data = train_data[tmp_data,]

# convert factor to numeric array without loosing data
f = train_sample_data$Class
y = as.numeric(levels(f))[f]
tmp_y = y
# this algo needs it to be in 1 and -1 format so im replacing it now before it crashes, 
# docs say it does it on its own but it doesnt seem to work
y = replace(tmp_y, tmp_y == 0, -1)


# remove target variable
tmp_x = subset(train_sample_data, select = -c(Class))
# convert to matrix
x = as.matrix(tmp_x)
sparse_dwd = sdwd(x, y, lambda2=1)

tmp_test_data = as.matrix(tmp_x)

pred = predict(sparse_dwd, tmp_test_data)


```




```{r - kerndwd}
# https://cran.r-project.org/web/packages/kerndwd/kerndwd.pdf

# fit a linear DWD
kern = vanilladot()
dwd_linear = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_linear, kern, x, test_data)



# fit a DWD using Gaussian kernel
kern = rbfdot(sigma=1)
dwd_gaussian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_gaussian, kern, x, test_data)


# fit a weighted kernel DWD
kern = rbfdot(sigma=1)
weights = c(1, 2)[factor(y)]
dwd_weighted_gaussian = kerndwd(x, y, kern, qval=1, wt = weights, eps=1e-5, maxit=1e5)
pred = predict(dwd_weighted_gaussian, kern, x, test_data)


# fit polynomial kernel
kern = polydot(degree = 1, scale = 1, offset = 1)
dwd_polynomial = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_polynomial, kern, x, test_data)



# TODO https://discuss.analyticsvidhya.com/t/how-to-resolve-error-na-nan-inf-in-foreign-function-call-arg-6-in-knn/7280/3
# fit laplacian kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = laplacedot(sigma = 1)
dwd_laplacian = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_laplacian, kern, x, test_data)


# fit bessel kernel -- Error in dwdpath(x, y, nobs, np, kern, qval, ulam, nlam, wt, eps, maxit, : NA/NaN/Inf in foreign function call (arg 2)
kern = besseldot(sigma = 1, order = 1, degree = 1)
nobs = nrow(test_data)
dwd_bessel = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_bessel, kern, x, test_data)


# fit anova rbf kernel where rbf is a gaussian kernel 
kern = anovadot(sigma = 1, degree = 1)
dwd_anova = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_anova, kern, x, test_data)


# fit spline kernel - Error: cannot allocate vector of size 106.8 Gb
kern = splinedot()
dwd_spline = kerndwd(x, y, kern, qval=1, eps=1e-5, maxit=1e5)
pred = predict(dwd_spline, kern, x, test_data)


# TODO add models to list and loop over them later - read about optimizing these kernels later

# sigest(x)






```




```{r - DWDLargeR}


# calculate the best penalty parameter
penalty_param = penaltyParameter(x, y,expon=1)
# solve the generalized DWD model
dwd_larger_r = genDWD(x, y, penalty_param, expon=1)

pred = predict(dwd_larger_r, test_data)

# TODO Error in genDWD(x, y, penalty_param, expon = 1) : trying to get slot "ra" from an object of a basic class ("matrix") with no slots
# Im tired do this later

```



```{r - TODO write functions here to gather statistics, put into df and write out, loop over models and append etc}
# TODO read title





```