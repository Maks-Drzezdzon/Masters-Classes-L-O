---
title: "PSI Independent Project C15311966"
output: html_notebook
---



```{r Setup}
# dataset used
# https://www.sciencedirect.com/science/article/pii/S2352340920304315#utbl0001
# download @ https://data.mendeley.com/datasets/83tcx8psxv/1

needed_packages <- c("tidyverse", "readxl", "feather", "car", "nortest", "skimr", "nortest", "psych", "ggplot2")                      
# Extract not installed packages
not_installed <- needed_packages[!(needed_packages %in% installed.packages()[ , "Package"])]    
# Install not installed packages
if(length(not_installed)) install.packages(not_installed)

library(tidyverse)
library(readxl) # read xlsx
library(feather) # file format lib
library(skimr) # summary statistics for larger data sets
library(car)
library(nortest)
library(psych)
library(ggplot2)
# df = read_excel('idepen_proj_stats.xlsx')
df = read_feather('idepen_proj_stats.feather')

```


```{r Initial Data Exploration}

# basic exploration
head(df)
summary(df)
glimpse(df)
colnames(df)
# how many missing values are in each col
map(df , ~sum(is.na(.)))

# looking at cols with potential problems 
unique(df$PHONE)
unique(df$...10) # found potential issues

unique(df$JOB) # found potential issues
unique(df$UNIVERSITY)
unique(df$SCHOOL_NAT)
unique(df$ACADEMIC_PROGRAM)

unique(df$EDU_FATHER) 
# found potential issues a field with a value of 0, 
# however this could indicate something else such as an absent parent, 
# it will be left as is
unique(df$OCC_FATHER) # found potential issues, ^
unique(df$EDU_MOTHER) # found potential issues, ^
unique(df$OCC_MOTHER) # found potential issues, ^


#########################
# variables of interest #
#########################

colnames(df)

# looking for odd values which can be found easily via unique, such as negative values on exams etc

unique(df$SISBEN) # found potential issues
unique(df$INTERNET)
unique(df$TV)
unique(df$COMPUTER)
unique(df$WASHING_MCH)
unique(df$CAR)
unique(df$MIC_OVEN)
unique(df$DVD)
unique(df$FRESH)
unique(df$PHONE)
unique(df$MOBILE)

# Saber 11 EXAMS - no issues found
unique(df$MAT_S11) # maths
unique(df$CR_S11) # critical reading
unique(df$CC_S11) # citizen competencies
unique(df$BIO_S11) # biology
unique(df$ENG_S11) # communication in English

# SABER PRO EXAMS - no issues found
unique(df$CR_PRO) # critical reading
unique(df$QR_PRO) # quantitative reasoning
unique(df$CC_PRO) # citizen competencies
unique(df$WC_PRO) # written communication
unique(df$ENG_PRO) # communication in English


```


```{r Data Cleaning}


# column is completely empty and serves no purpose 
df = df[,!names(df) %in% c("...10")]

# fixing inconsistency
df$SISBEN[df$SISBEN == "It is not classified by the SISBEN"] = "Level NA"
df$SISBEN[df$SISBEN == "Esta clasificada en otro Level del SISBEN"] = "Level NA"

df$SISBEN[df$SISBEN == "Level NA"] = 0
df$SISBEN[df$SISBEN == "0"] = 0
df$SISBEN[df$SISBEN == "Level 1"] = 1
df$SISBEN[df$SISBEN == "Level 2"] = 2
df$SISBEN[df$SISBEN == "Level 3"] = 3


df$JOB[df$JOB == "0"] = "No"

df$INTERNET[df$INTERNET == "Yes"] = 1
df$INTERNET[df$INTERNET == "No"] = 0

df$TV[df$TV == "Yes"] = 1
df$TV[df$TV == "No"] = 0

df$COMPUTER[df$COMPUTER == "Yes"] = 1
df$COMPUTER[df$COMPUTER == "No"] = 0

df$WASHING_MCH[df$WASHING_MCH == "Yes"] = 1
df$WASHING_MCH[df$WASHING_MCH == "No"] = 0

df$MIC_OVEN[df$MIC_OVEN == "Yes"] = 1
df$MIC_OVEN[df$MIC_OVEN == "No"] = 0

df$CAR[df$CAR == "Yes"] = 1
df$CAR[df$CAR == "No"] = 0

df$DVD[df$DVD == "Yes"] = 1
df$DVD[df$DVD == "No"] = 0

df$FRESH[df$FRESH == "Yes"] = 1
df$FRESH[df$FRESH == "No"] = 0

df$PHONE[df$PHONE == "Yes"] = 1
df$PHONE[df$PHONE == "No"] = 0

df$MOBILE[df$MOBILE == "Yes"] = 1
df$MOBILE[df$MOBILE == "No"] = 0

# saving file into feather while working on project
# it offers faster load times, however its not used
# to store data long term
write_feather(df, 'idepen_proj_stats.feather')
df = read_feather('idepen_proj_stats.feather')

```


```{r Testing for normality-pro exams}

# fragmenting data
df_pro_exams =  data.frame(df$CR_PRO, df$QR_PRO, df$CC_PRO, df$WC_PRO, df$ENG_PRO)
summary(df_pro_exams)
# there doesnt seem to be any issues with the summary statistics
# mean and median are very close, the 3rd Qu. is roughly ~20-25% above the mean more or less



# anderson-darling normality test is used over shapiro-wilks
# because it has a limitation of 5000 records 
# for comparison both give the same result
shapiro.test(df_pro_exams$df.CR_PRO[0:5000])
ad.test(df_pro_exams$df.CR_PRO)
ad.test(df_pro_exams$df.QR_PRO)
ad.test(df_pro_exams$df.CC_PRO)
ad.test(df_pro_exams$df.WC_PRO)
ad.test(df_pro_exams$df.ENG_PRO)
# p < 2.2e-16
qqPlot(df_pro_exams$df.CR_PRO, ylab = "critical reading exam scores")
qqPlot(df_pro_exams$df.QR_PRO, ylab = "quantitative reasoning exam scores")
qqPlot(df_pro_exams$df.CC_PRO, ylab = "citizen competencies exam scores")
qqPlot(df_pro_exams$df.WC_PRO, ylab = "written communication exam scores")
qqPlot(df_pro_exams$df.ENG_PRO, ylab = "communication in English exam scores")

hist(df_pro_exams$df.CR_PRO, xlab = "critical reading exam scores")
hist(df_pro_exams$df.QR_PRO, xlab = "quantitative reasoning exam scores")
hist(df_pro_exams$df.CC_PRO, xlab = "citizen competencies exam scores")
hist(df_pro_exams$df.WC_PRO, xlab = "written communication exam scores")
hist(df_pro_exams$df.ENG_PRO, xlab = "communication in English exam scores")

Boxplot(df_pro_exams$df.CR_PRO, ylab = "critical reading exam scores")
Boxplot(df_pro_exams$df.QR_PRO, ylab = "quantitative reasoning exam scores")
Boxplot(df_pro_exams$df.CC_PRO, ylab = "citizen competencies exam scores")
Boxplot(df_pro_exams$df.WC_PRO, ylab = "written communication exam scores")
Boxplot(df_pro_exams$df.ENG_PRO, ylab = "communication in English exam scores")

# from the visualizations, p-value and a-value the pro exams are NOT normally distributed

```


```{r Testing for normality-saber exams}

df_saber_exams =  data.frame(df$MAT_S11, df$CR_S11, df$CC_S11, df$BIO_S11, df$ENG_S11)
summary(df_saber_exams)
# there doesnt seem to be many issues with the summary statistics
# all summary statistics seem the be closely distributed with a 10% difference in scores
# from the 1st QU. to the mean/median and the 3rd Qu. this is reflected in the visualizations


ad.test(df_saber_exams$df.MAT_S11)
ad.test(df_saber_exams$df.CR_S11)
ad.test(df_saber_exams$df.CC_S11)
ad.test(df_saber_exams$df.BIO_S11)
ad.test(df_saber_exams$df.ENG_S11)
# p < 2.2e-16

hist(df_saber_exams$df.MAT_S11, xlab = "maths exam scores")
hist(df_saber_exams$df.CR_S11, xlab = "critical reading exam scores")
hist(df_saber_exams$df.CC_S11, xlab = "citizen competencies exam scores")
hist(df_saber_exams$df.BIO_S11, xlab = "biology exam scores")
hist(df_saber_exams$df.ENG_S11, xlab = "communication in English exam scores")

boxplot(df_saber_exams$df.MAT_S11, ylab = "Maths Exam scores exam scores")
boxplot(df_saber_exams$df.CR_S11, ylab = "critical reading exam scores")
boxplot(df_saber_exams$df.CC_S11, ylab = "citizen competencies exam scores")
boxplot(df_saber_exams$df.BIO_S11, ylab = "biology exam scores")
boxplot(df_saber_exams$df.ENG_S11, ylab = "communication in English exam scores")

# even though the p value in normality test doesnt confirm normality, the low A value and
# visualizations confirm a normal distribution 

```


```{r Dimension Reduction - heatmap, bartlett tests}
prepare_matrix_data = df[,!names(df) %in% c("ACADEMIC_PROGRAM", "UNIVERSITY", "Cod_SPro", 
                                            "SCHOOL_TYPE", "SCHOOL_NAT", "SCHOOL_NAME", 
                                            "JOB", "REVENUE", "OCC_MOTHER",
                                            "OCC_FATHER", "EDU_MOTHER", "EDU_FATHER", "COD_S11", 
                                            "GENDER", "PEOPLE_HOUSE", "STRATUM",
                                            "SEL", "SEL_IHE", "QUARTILE", "2ND_DECILE",
                                            "WC_PRO", "PERCENTILE", "G_SC")]


prepare_matrix_data_tests = df[,!names(df) %in% c("ACADEMIC_PROGRAM", "UNIVERSITY", "Cod_SPro", 
                                                  "SCHOOL_TYPE", "SCHOOL_NAT", "SCHOOL_NAME", 
                                                  "JOB", "REVENUE", "OCC_MOTHER",
                                                  "OCC_FATHER", "EDU_MOTHER", "EDU_FATHER", "COD_S11", 
                                                  "GENDER", "PEOPLE_HOUSE", "STRATUM", "INTERNET",
                                                  "TV", "COMPUTER", "WASHING_MCH", "MIC_OVEN",
                                                  "CAR", "DVD", "FRESH", "PHONE", "MOBILE", 
                                                  "SEL", "SEL_IHE", "QUARTILE", "2ND_DECILE",
                                                  "WC_PRO", "PERCENTILE", "G_SC")]


tests_matrix = sapply(prepare_matrix_data_tests, as.double)
matrix_data = sapply(prepare_matrix_data, as.double)

heatmap(cor(tests_matrix), Rowv = NA, Colv = NA)
heatmap(cor(matrix_data), Rowv = NA, Colv = NA)
# in the heatmaps even though there is little to no correlation between govenment aid sisben and grades
# there is a slight correlation between the things that are under the sisben umbreala such as
# internet, tv, computer, car

cortest.bartlett(cor(tests_matrix), n=nrow(prepare_matrix_data_tests))
cortest.bartlett(cor(matrix_data), n=nrow(prepare_matrix_data))
# since the pvalue is 0, it indicates that this data is suitable for PCA

# there seem to be two square clusters of correlations around exam scores and items on the sisben list
# which show that there is very little or no correlation between these variables

# Visualization of significance levels at 0.05
result = corrplot::cor.mtest(cor(matrix_data), conf.level = .95)
corrplot::corrplot(cor(matrix_data), method="square", p.mat = result$p, type="lower", sig.level = .05)

```


```{r Dimension Reduction - Kaiser-Meyer-Olkin}

KMOS(tests_matrix)


KMOS(matrix_data)

# these values indicate how well suited data is for factor analysis ranges from 0-1 the closer to 1 the better t he variable is
# most values are above in high .7-.9 which is very good
# WC_pro is in the .4 range and will be removed as its not good enough to be kept in

```


```{r Dimension Reduction - Determinant}
# this is done to check if there is co-linearity
dim(tests_matrix)
dim(matrix_data)
# det(matrix_data)

# downsized_tests_matrix = tests_matrix[0:13,]
# downsized_matrix_data = matrix_data[0:23,]

det(downsized_tests_matrix)
det(downsized_matrix_data)

# This set can not be performed as finding the determinant can only be done on
# square matrices, after making a small square matrix the method works, however 
# its too small to represent the whole data set

```


```{r Dimension Reduction - PCA}
PCA_data = sapply(prepare_matrix_data, as.double)
prepare_matrix_data
glimpse(PCA_data)
classes = factor(df$SISBEN)
# normalize data so larger values dont overweight smaller ones
# scaling divides by the standard of deviation 
# subtract the mean so the new mean is 0
# if variables are measured on different scales this will no longer be an issue
# exclude sisben from PCA
dfPCA = prcomp(scale(PCA_data[, -1]))

# view PCA statistics
summary(dfPCA)
# This is used to see how many of the variables can be dropped and at what cost
# The cumulative proportion increases with each PC component,
# for instance pc27 offers less than 1% up from pc26 making it not very useful
# you can then pick a cut off point to see how many components can be dropped

# the cut off for this analysis will be pc16-17 which allows the prediction of up to 90-92% the last 9-10 pc components only contribute 8-10%

# proportion of variance is similar to cumulative proportion except its 
# the proportion that that principal component caries
# these are sorted in order which indicates that past PC2 there is very little gain
# even less so as you reach PC6, PC10 and PC15 etc etc

# look for patterns
biplot(dfPCA, scale=0)
# as the value for pc2 exams floats around -2.5 and 0.5 it projects all the sisben 
# commodities to a value between -0.2 and 0.0


# to have a better look at the biplot im going to extract the top 6 principal components 
# attach them to the original prepared data and graph it to see if there are any 
# "clusters" that form naturally

# this was updated to 3 after a scree plot was produced that showed a slight cut off at 2+ but below 3-4
df2 = cbind(prepare_matrix_data, dfPCA$x[,1:4])

head(df2)

ggplot(df2, aes(PC1, PC2, PC4, col = SISBEN, fill = SISBEN)) +
  stat_ellipse(geom = "polygon", col = "black", alpha = 0.5) +
  geom_point(shape = 21, col = "black")

# after playing around with ggplot a much more readable graph was produced
# here you can see that as much as all individuals overlap
# there is much less overlap between people that receive no government aid
# sisben 0 and those that receive some sisben 1 some government aid, those two groups are most apart
# whereas all in between are closer grouped with those that dont receive any

# This is an indicator that 1 sisben is closing the gap between people
# and 2, that there is a difference between people with sisben 0 and sisben 1
# from previous findings it seems to be small

```


```{r Dimension Reduction - Extracting Factors}

# extract factors
unique(df$SISBEN)
facsol = psych::fa(matrix_data, nfactors=4, obs=NA, n.iter=1, rotate="varimax", fm="pa")

# scree plot
# indicates that 0.5 should be retained from the previous 2
plot(facsol$values, type = "b")

# Print the components with loadings 
# PA1-2 can be kept, PA3 arguably could be kept since some values do peak over
# 0.3 however PA1-2 are much higher and show commonalities
psych::print.psych(facsol,cut=0.3, sort=TRUE)

# Print sorted list of loadings 
# all loadings in PA1 seem to be significant 
# meanwhile most loadings in PA2 are above the 0.3 threshold with 4 being under 0.3
fa.sort(facsol$loading)
pre_rotation_loadings = fa.sort(facsol$loading)

# this further supports the analysis derived from the scree plot
# that 0.5-1 component can be extracted down from the previous 2
# with most having a factor of 0.7 which is on the margin of being considerable while being too low
fa.diagram(facsol)

# Variance accounted for by each factor/component
facsol$Vaccounted

# Eigenvalues
facsol$values 
# the two high eigenvalues are probably the percentile and global score for each student
# these two variables dont offer anything to the analysis so they were removed as they were outliers , and everything above was updated accordingly

```


```{r Apply rotation}

#Apply rotation to try to refine the component structure
facsolrot =  principal(matrix_data, nfactors = 4, rotate = "varimax")
#output the components
psych::print.psych(facsolrot, cut = 0.3, sort = TRUE)
#output the commonalities
facsolrot$communality

facsolrot$loadings
pre_rotation_loadings

```


```{r Reliability Analysis}
psych::alpha(matrix_data, check.keys=TRUE) # slide 78 for recap

# sisben for group 0 is at 0.62 which is in the questionable category
# for group 1, internet, tv,computer, phone, mobile and DVD seem to be very reliable
# group 2 and 3 are almost all 0

```









