{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Due data 30th April, Friday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "* [Project Setup](#Setup)\n",
    "  * [Loading Data](#Loading-Data)\n",
    "  * [Data Preprocessing](#Data-Preperation)\n",
    "  * [Quick Setup](#Start-Here---Personal-Setup)\n",
    "* [Model Training](#Models)\n",
    "  * [SVM](#SVM)\n",
    "  * [Random Forest](#Random-Forest)\n",
    "  * [Neural Network](#Neural-Network)\n",
    "* [Model Evaluation Visualisations](#Model-Evaluation-Visualisations)\n",
    "    * [Roc](#Roc)\n",
    "    * [Learn Rate](#Learn-Rate)\n",
    "    * [idk](#idk)\n",
    "    * [idk2](#idk2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer, cohen_kappa_score\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "- [Back To Top](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load complete\n"
     ]
    }
   ],
   "source": [
    "# load data for train split instead\n",
    "train_FNC = pd.read_csv('../../../../mlsp-2014-mri/Train/train_FNC.csv', delimiter=',')\n",
    "train_SBM = pd.read_csv('../../../../mlsp-2014-mri/Train/train_SBM.csv', delimiter=',')\n",
    "train_labels = pd.read_csv(  '../../../../mlsp-2014-mri/Train/train_labels.csv', delimiter=',')\n",
    "\n",
    "test_FNC = pd.read_csv('../../../../mlsp-2014-mri/Test/test_FNC.csv', delimiter=',')\n",
    "test_SBM = pd.read_csv('../../../../mlsp-2014-mri/Test/test_SBM.csv', delimiter=',')\n",
    "print('Data load complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation\n",
    "- [Back To Top](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Merge data\n",
    "# Train data\n",
    "data = pd.merge(train_FNC, train_SBM)\n",
    "data_2 = pd.merge(data, train_labels)\n",
    "data_2.to_csv('amalgamated_train_data.csv', index=False)\n",
    "train_data = pd.read_csv('../../../../mlsp-2014-mri/assign_data/amalgamated_train_data.csv', delimiter=',')\n",
    "\n",
    "# Test data\n",
    "test_data = pd.merge(test_FNC, test_SBM)\n",
    "test_data.to_csv('amalgamated_test_data.csv', index=False)\n",
    "print('done')\n",
    "# had to remove index in R because python apparently cant do that \n",
    "# R code below \n",
    "# data = read.csv(\"mlsp-2014-mri/amalgamated_train_data.csv\", as.is=T, header=T, sep=\",\")\n",
    "# write.csv(data[,-1], 'mlsp-2014-mri/assign_data/amalgamated_train_data.csv', row.names = F)\n",
    "\n",
    "# data = read.csv(\"mlsp-2014-mri/amalgamated_test_data.csv\", as.is=T, header=T, sep=\",\")\n",
    "# write.csv(data[,-1], 'mlsp-2014-mri/assign_data/amalgamated_test_data.csv', row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here - Personal Setup \n",
    "- [Back To Top](#Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load complete\n"
     ]
    }
   ],
   "source": [
    "# This cell just loads data and prepares it so i can run models using it #\n",
    "\n",
    "# load data\n",
    "train_data = pd.read_csv('../../../../mlsp-2014-mri/assign_data/amalgamated_train_data.csv', delimiter=',', index_col=False)\n",
    "test_data = pd.read_csv('../../../../mlsp-2014-mri/assign_data/amalgamated_test_data.csv', delimiter=',', index_col=False)\n",
    "# convert test data to df from numpy array\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "\n",
    "# Prepare values for training data\n",
    "labels = train_data.pop('Class').values\n",
    "data = train_data.values\n",
    "\n",
    "# load data for submition\n",
    "submition_csv = pd.read_csv(  '../../../../mlsp-2014-mri/submission_example.csv', delimiter=',')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print('Data load & setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Back To Top](#Index)\n",
    "    * [SVM - 79.5%](#SVM)\n",
    "    * [Random Forest - 70.2%](#Random-Forest)\n",
    "    * [Neural Network - 0.0%](#Neural-Network)\n",
    "    * [LinearDiscriminantAnalysis Model - 0.0%](#LinearDiscriminantAnalysis-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "- SVM Model - Kaggle best 79.5 % \n",
    "- [Back To Top](#Index)\n",
    "- [Models](#Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "Model Accuracy: 0.769\n",
      "Model Precision: 0.714\n",
      "Model Recall: 0.833\n",
      "F1 Score:  0.769\n",
      "Cohens Kappa : 0.541\n",
      "Classification Report :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.71      0.77        14\n",
      "           1       0.71      0.83      0.77        12\n",
      "\n",
      "    accuracy                           0.77        26\n",
      "   macro avg       0.77      0.77      0.77        26\n",
      "weighted avg       0.78      0.77      0.77        26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=109) \n",
    "# 70% training and 30% test\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(kernel='sigmoid') # Best Kernel was Sigmoid \n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "m_svm = clf.predict(X_test)\n",
    "\n",
    "####################\n",
    "# Model Evaluation #\n",
    "####################\n",
    "# read again, refresh memory https://www.jeremyjordan.me/evaluating-a-machine-learning-model/\n",
    "# read up on random_state=109 why, without it model is bad \n",
    "# https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "print ('Model Evaluation')\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct\n",
    "print(\"Model Accuracy:\", round(metrics.accuracy_score(y_test, m_svm), 3))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such\n",
    "print(\"Model Precision:\", round(metrics.precision_score(y_test, m_svm), 3))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such\n",
    "print(\"Model Recall:\", round(metrics.recall_score(y_test, m_svm), 3))\n",
    "# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "print('F1 Score: ', round(f1_score(y_test, m_svm, average=\"macro\"), 3))\n",
    "# \n",
    "print('Cohens Kappa :', round(cohen_kappa_score(y_test , m_svm), 3))\n",
    "# Combination of Accuracy, Precision, Recall \n",
    "print(\"Classification Report :\\n\", classification_report(y_test,m_svm))\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "\n",
    "X_test, y_test = train_test_split(test_data_df, test_size=1, random_state=109) # 70% training and 30% testprint(\"after split check\", len(X_test),  len(y_test))\n",
    "m_svm = clf.predict(test_data_df)\n",
    "# probability preformed really bad, so i think 0 or 1 is a better \n",
    "# y_predict_proba = clf.predict_proba(test_data_df)\n",
    "submition_csv['Probability'] = m_svm\n",
    "submition_csv.to_csv('m_svm_submition.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Feature Selection\n",
    "- finish this\n",
    "- [Back To Top](#Index)\n",
    "- [Models](#Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=109) \n",
    "# 70% training and 30% test\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "\n",
    "# Fitting logistic regression to the training set\n",
    "Classifier = LogisticRegression(random_state=0)\n",
    "Classifier.fit(x_train, y_train)\n",
    "# get values, returns a series object with index and keys fields, more can be found \n",
    "# here https://pandas.pydata.org/docs/reference/api/pandas.Series.keys.html\n",
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_,index=train_data.columns).sort_values(ascending=False)\n",
    "\n",
    "# keep top 10 values\n",
    "top_ten_features = feature_imp.nlargest(10, keep='all')\n",
    "top_ten_features\n",
    "# print(top_ten_features.keys())\n",
    "\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=top_ten_features, y=top_ten_features.index)\n",
    "# labels\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "# the legend works but takes up space and isnt needed\n",
    "# plt.legend(top_ten_features.keys())\n",
    "plt.show()\n",
    "\n",
    "# Im not sure what these features are specifically \n",
    "\n",
    "###########################################\n",
    "# Random Forest Model - FIX LATER DONE FOR NOW #\n",
    "###########################################\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(top_ten_data, top_ten_labels, test_size=0.3, random_state=100) # 70% training and 30% test\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "####################\n",
    "# Model Evaluation #\n",
    "####################\n",
    "print ('Model Evaluation')\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct\n",
    "print(\"Model Accuracy:\", round(metrics.accuracy_score(y_test, y_pred), 3))\n",
    "\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "\n",
    "X_test, y_test = train_test_split(test_data_df[top_ten_labels].transpose(), test_size=0.3, random_state=114) # 70% training and 30% testprint(\"after split check\", len(X_test),  len(y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "# probability preformed really bad, so i think 0 or 1 is a better \n",
    "# y_predict_proba = clf.predict_proba(test_data_df)\n",
    "submition_csv['Probability'] = y_pred\n",
    "submition_csv\n",
    "submition_csv.to_csv('test_submition.csv', index=False)\n",
    "\n",
    "\n",
    "top_ten_labels = train_data[top_ten_features.keys()].keys()\n",
    "top_ten_data = train_data[top_ten_features.keys()].values.transpose()\n",
    "print(top_ten_data.shape)\n",
    "print(top_ten_labels.shape)\n",
    "print(test_data_df.shape)\n",
    "print(test_data_df[top_ten_labels].transpose().shape)\n",
    "test_data_df[top_ten_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "- Kaggle best 70.2%\n",
    "- [Back To Top](#Index)\n",
    "- [Models](#Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "Model Accuracy: 0.692\n",
      "Model Precision: 0.714\n",
      "Model Recall: 0.455\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=100) # 70% training and 30% test\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "m_random_forest=clf.predict(X_test)\n",
    "\n",
    "####################\n",
    "# Model Evaluation #\n",
    "####################\n",
    "# read again, refresh memory https://www.jeremyjordan.me/evaluating-a-machine-learning-model/\n",
    "# read up on random_state=109 why, without it model is bad \n",
    "# https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "\n",
    "print ('Model Evaluation')\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct\n",
    "print(\"Model Accuracy:\", round(metrics.accuracy_score(y_test, m_random_forest), 3))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such\n",
    "print(\"Model Precision:\", round(metrics.precision_score(y_test, m_random_forest), 3))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such\n",
    "print(\"Model Recall:\", round(metrics.recall_score(y_test, m_random_forest), 3))\n",
    "\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "\n",
    "X_test, y_test = train_test_split(test_data_df, test_size=1, random_state=114) # 70% training and 30% testprint(\"after split check\", len(X_test),  len(y_test))\n",
    "m_random_forest = clf.predict(test_data_df)\n",
    "# probability preformed really bad, so i think 0 or 1 is a better \n",
    "# y_predict_proba = clf.predict_proba(test_data_df)\n",
    "submition_csv['Probability'] = m_random_forest\n",
    "submition_csv\n",
    "submition_csv.to_csv('m_random_forest_submition.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "- Kaggle best 0.0%\n",
    "- [Back To Top](#Index)\n",
    "- [Models](#Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# Neural Network Model -  #\n",
    "###########################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size= 0.3, random_state=2)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(random_state=0, max_iter=100)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "m_nn = mlp.predict(scaler.transform(test_data_df))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test))) \n",
    "m_nn\n",
    "\n",
    "submition_csv['Probability'] = m_nn\n",
    "submition_csv\n",
    "submition_csv.to_csv('m_nn_submition.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearDiscriminantAnalysis Model\n",
    "- Kaggle best 68.4%\n",
    "- [Back To Top](#Index)\n",
    "- [Models](#Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation\n",
      "Model Accuracy: 0.692\n",
      "Model Precision: 0.625\n",
      "Model Recall: 0.833\n"
     ]
    }
   ],
   "source": [
    "# look up https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=109) # 70% training and 30% test\n",
    "#Create a LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "linear_disc_analysis_m = clf.predict(X_test)\n",
    "\n",
    "####################\n",
    "# Model Evaluation #\n",
    "####################\n",
    "# read again, refresh memory https://www.jeremyjordan.me/evaluating-a-machine-learning-model/\n",
    "# read up on random_state=109 why, without it model is bad \n",
    "# https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python\n",
    "\n",
    "print ('Model Evaluation')\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct\n",
    "print(\"Model Accuracy:\", round(metrics.accuracy_score(y_test, linear_disc_analysis_m), 3))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such\n",
    "print(\"Model Precision:\", round(metrics.precision_score(y_test, linear_disc_analysis_m), 3))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such\n",
    "print(\"Model Recall:\", round(metrics.recall_score(y_test, linear_disc_analysis_m), 3))\n",
    "\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "\n",
    "X_test, y_test = train_test_split(test_data_df, test_size=1, random_state=109) # 70% training and 30% testprint(\"after split check\", len(X_test),  len(y_test))\n",
    "linear_disc_analysis_m = clf.predict(test_data_df)\n",
    "# probability preformed really bad, so i think 0 or 1 is a better \n",
    "# y_predict_proba = clf.predict_proba(test_data_df)\n",
    "submition_csv['Probability'] = linear_disc_analysis_m\n",
    "submition_csv\n",
    "submition_csv.to_csv('test_submition.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Back To Top](#Index)\n",
    "    * [Roc](#Roc)\n",
    "    * [Learn Rate](#Learn-Rate)\n",
    "    * [idk](#idk)\n",
    "    * [idk2](#idk2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roc\n",
    "- [Back To Top](#Index)\n",
    "- [Model Evaluation Visualisations](#Model-Evaluation-Visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1d309337bed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfprS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtprS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mroc_aucS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfprS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtprS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_curve' is not defined"
     ]
    }
   ],
   "source": [
    "fprS, tprS, t = roc_curve(y_test, y_score)\n",
    "roc_aucS = auc(fprS, tprS)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fprG, tprG, color='red',\n",
    "         lw=lw, label='ROC NB (area = %0.2f)' % roc_aucG)\n",
    "plt.plot(fprD, tprD, color='green',\n",
    "         lw=lw, label='ROC Dtree (area = %0.2f)' % roc_aucD)\n",
    "plt.plot(fprS, tprS, color='grey',\n",
    "         lw=lw, label='ROC SVM (area = %0.2f)' % roc_aucS)\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Analysis for Hotel Review data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Rate\n",
    "- [Back To Top](#Index)\n",
    "- [Model Evaluation Visualisations](#Model-Evaluation-Visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps=300\n",
    "num = 30\n",
    "ho_s = []\n",
    "s_s = []\n",
    "for i in range(1,num):\n",
    "    s = i/num\n",
    "    for j in range(reps):\n",
    "        ss =[]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = s)\n",
    "        y_pred = dtree.fit(X_train, y_train).predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        ss.append(acc)\n",
    "    ho_s.append(mean(ss))\n",
    "    s_s.append(s)\n",
    "    \n",
    "ho_s = pd.DataFrame(ho_s, index = s_s, columns = ['Hold Out'])\n",
    "ax = ho_s.plot()\n",
    "ax.set_xlabel(\"Train Set Proportion\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy: Train Set Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idk\n",
    "- [Back To Top](#Index)\n",
    "- [Model Evaluation Visualisations](#Model-Evaluation-Visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv = []\n",
    "for i in range(reps):\n",
    "    kf = KFold(n_splits=10, shuffle = True) # needed to ensure shuffling\n",
    "    scores = cross_val_score(dtree, X, y, cv=kf)\n",
    "    xv.append(scores.mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "res = pd.DataFrame(ho, columns = ['Hold Out'])\n",
    "res['X Val']=xv\n",
    "%matplotlib inline\n",
    "ax = res.plot()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy: Hold Out and X Val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idk2\n",
    "- [Back To Top](#Index)\n",
    "- [Model Evaluation Visualisations](#Model-Evaluation-Visualisations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data vis block\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_dash, classes=['Not Helpful','Helpful'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_dash, classes=['Not Helpful','Helpful'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
