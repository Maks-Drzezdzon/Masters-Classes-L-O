{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Neural Network Essentials.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NqvYaHGGlYzN"},"source":["# Neural Network Representation\n","\n","In the first two sections of the course we focused on the important Machine Learning foundations of Linear Regression and Logistic Regression. Both are very important data analysis techniques that have been used extensively in both statistics and machine learning for decades. They are powerful, they are (relatively) easy to understand, and they can be used with a surprisingly large number of problem types. But they aren't the end of the story for us, they are just the beginning. This week we will look at how the idea of linear and logistic functions form the building blocks of Deep Neural Networks. \n","\n","## Preliminaries \n","We begin by reintroducing some functions from our Logistic Regression and Linear Regression notes. "]},{"cell_type":"code","metadata":{"id":"Vz74K_DelYzW","executionInfo":{"status":"ok","timestamp":1613376767718,"user_tz":0,"elapsed":684,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJpIFOrrjxWZop0YQn_ZEscjIkShJyJr2Xxqa_yZWdjHwXd2pMnoYIA_RRCSI0sSkQXj3pJzNjFghSUre_OjA1-Tj3b1CzHWCTlgCLWNSHYXYWNFlXkG6eaP_py0tPDngWXJ56Bc7Bc7jYRcoL48I5LOjoR7Aro8ZHVoOyaFvByc3gCk65_rNzcYV8hNYCJcerkdQ1mZtUtRfkYOhq2sSlKQZv2nvG4pWvXcL4U4wKPiLT_oTCjauHMsBHLE1wlYfuoSkXwGRoUK5bCT7E7PQipu9EuOMGzEfRA1iRodJDB1WGxFVXrNVkYfvGF_GRSKBD4qVhIopjOW1GW7hNLFCIaC8Bs3sPiPQ5jR_GCB285_QOPn9rh_4ug2HNacjGcSJvd-QvnWF588p0t2eTCtfsOvZZilfQWff_sLOZeP7LreXjpSVqzxOK1o9WwKkgNpeCn7Kt7NQbHhk_oody0xyrPPvuRTDTXDjfym2CE826qkbfEe-kYxlx_4zckTGAwiYJU5z66y8grp07uMSftNeR61mJuIDMZPUXbCW7gTrDsEiJNg4gE0lu54ZgVqTWcIMA5Zp5fnYUdkvooekoSXjOKYxhkftn6Z0NA9NcoePldAnPN53yyeodvBTve0191lLZT3op7itPk6bwC5RUtIT2MObBorD_slOMh-S-WUJSq8fJaVvEKbFjxCL2-3xQYGPgN6CCxz0HcERHCFxhcl0x5STfB6_TNwdJRVD8zdazzEXTBSK9vA7f6dgfoRkM0M_0dA=s64","userId":"03086069160226397014"}}},"source":["import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","def standardize(x):\n","    col_means = np.mean(x,axis=0)\n","    col_std = np.std(x,axis=0)\n","    return (x - col_means) / col_std , col_means, col_std\n","\n","def h_linear(theta,X):\n","    z = np.matmul(theta,X)\n","    return z\n","\n","def h_logistic(theta,X):\n","    z = np.matmul(theta,X)\n","    return 1 / (1 + np.exp(-z))\n","\n","def cost_logistic(theta,xi,yi):\n","    # note - this function calculate the cost for all variations of theta. A vector is returned. \n","    z = np.matmul(theta,xi)\n","    h = 1 / (1 + np.exp(-z))\n","    if yi == 1:\n","        return -np.log(h)\n","    elif yi == 0:\n","        return -np.log(1-h)\n","    else: raise ValueError('yi was neither 0 nor 1: ' + str(yi))\n","        \n","def cost_linear(theta,xi,yi):\n","    # note - this function calculate the cost for all variations of theta. A vector is returned. \n","    z = np.matmul(theta,xi)\n","    h = 1 / (1 + np.exp(-z))\n","    rv = ((h - yi)**2) / 2\n","    return rv  \n","\n","def J(theta,x,y,f): \n","    s = 0\n","    m = len(x)\n","    for i in range(0,m):\n","        v = f(theta,x[i],y[i])\n","        s+= v\n","    return s / m\n","\n","def gd(theta_in,min_delta_in,a_in,X_in,Y_in,h,maxiterations=-1,cost=cost_logistic):\n","    m = len(Y_in) # number of training examples\n","    _ , n = np.shape(X_in) # number of features\n","    \n","    # define a list used to store the costs - these do not have to be calculated but are useful for debugging\n","    c = []\n","    \n","    # Create a theta variable which we will use to keep track of the current value of theta\n","    theta = theta_in\n","    \n","    # Create a list which we will use for storing theta results during optimization\n","    thetas = np.array(theta_in,ndmin=2)\n","\n","    # Create a counter which is used for limiting the number of iterations used if maxiterations has been defined\n","    k = 0\n","    \n","    # Begin main search loop\n","    while maxiterations is -1 or k < maxiterations:\n","        k+=1\n","        c.append(J(theta_in,X_in,Y_in,cost))\n","        \n","        temp = np.copy(theta)\n","        for j in range(0,n):\n","            # making calculations for jth parameter \n","            s = 0 \n","            for i in range(0,m):\n","                s+=  (h(temp.T,np.ravel(X_in[i,:])) - Y_in[i])  * X_in[i,j]\n","            theta[j] = temp[j] - a_in * (1/m) * s \n","        thetas = np.vstack((thetas,theta))\n","        \n","        progressing = False\n","        for j in range(0,n):\n","            if(abs(temp[j] - theta[j]) > min_delta_in):\n","                progressing = progressing | True\n","        if not progressing: break \n","    return thetas, c"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1CyoLyalYzY"},"source":["## Motivation: From Linear to Non-Linear Model Learning\n","Both Logistic Regression and Linear Regression are essentially linear models. That is, the hypotheses that they create are based on linear combinations of inputs. When we know that the structure we are looking for in data is more complicated than a linear model we can cheat in Linear and Logistic Regression by introducing extra non-linear features which can be fed into the linear classifier. This is exactly what we did when we introduced higher order features in our datasets in examples for Linear Regression and Logistic Regression. \n","\n","The problem with this approach is that it does not scale. In order to achieve complex non-linear decision boundaries in a classification problem we might have to add many additional features. If we are introducing quadratic or other higher order polynomial variants this can lead to an exponential number of features. \n","\n","Neural Networks allow us to build non-linear models without having to manual encode all of these additional higher order features from our base features. The network itself looks at building these higher-order features as part of its own training process. The advantage of this method is that we don't have to guess what higher order features are useful - the neural network will only create higher order features that are shown to have a beneficial effect. \n","\n","The disadvantage to this neural network approach is computational cost. Neural Networks are orders of magnitude more expensive to train than a logistic regression classifier - even if that classifier has had many extra manually created features added through data expansion. From the mid 1990s to the early 2010s this high computational cost meant that neural networks were not practical problem solvers. However, faster CPUs and speed-ups made possible by GPUs have allowed Neural Networks to finally realize their promise. \n","\n","## Logistic Units\n","Neural Networks are architectures of simple processing units called neurons which are inspired by neurons in the brain but which are really just simple computational units which calculate a function of their input. Taken individually these neurons cannot achieve much, but when integrated into a complex architecture and trained with a powerful learning algorithm, they can build complex non-linear models over our data. \n","\n","In practice many different types of neurons can be used to implement a neural network. Here we will begin by focusing on one of the most important of the these, the logistic unit or logistic neuron. \n","\n","A logistic unit is based on the logistic classifier as reviewed last week. In simple terms, the logistic unit makes a binary classification decision based on the application of the logistic function to the negative scalar product of an input vector times some parameters. This is formalized in our familiar function below: \n","\n","\\begin{equation}\n","h_{\\theta}(x) = \\frac{1}{1 + x^{-\\theta x}}\n","\\end{equation}\n","\n","or\n","\n","\\begin{equation}\n","h_{\\theta}(x) = \\frac{1}{1 + e^{-z}}\n","\\end{equation}\n","\n","where z, the logit, is defined as:\n","\n","\\begin{equation}\n","z = \\theta x \n","\\end{equation}\n","\n","In the above, $\\theta$ are our model parameters, $x$ is our input data vector, and $h$ is the output of our function which is commonly referred to as the hypothesis. $z$ is the linear product of our parameters and input values, which as we have mentioned is often referred to as the **logit**. \n","\n","We interpret the logistic function as follows: \n","\n","\\begin{equation}\n","h_{\\theta}(x) = P(Y=1 \\;|\\; x_{i},\\theta)\n","\\end{equation}\n","\n","meaning that we must apply a threshold to $h$ to get our classification decision. \n","\n","As mentioned we can have many different types of neurons. The most significant difference between neuron types is the output or **activation function**. In the above the activation function is the logistic function, but a linear activation function can for example be used to produce a continuous real valued output from a neuron. \n","\n","For example we can define and apply a simple linear classifier over two input variables. In the example below our target $Y$ indicates whether a part for a truck was found to be faulty following a vehicle inspection. Feature $x1$ indicates the number of years since the part was replaced and $x2$ indicates the distance the truck has traveled since the last inspection in kilometers.  "]},{"cell_type":"code","metadata":{"id":"L-2HCg2OlYzb","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1613376768340,"user_tz":0,"elapsed":1286,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJpIFOrrjxWZop0YQn_ZEscjIkShJyJr2Xxqa_yZWdjHwXd2pMnoYIA_RRCSI0sSkQXj3pJzNjFghSUre_OjA1-Tj3b1CzHWCTlgCLWNSHYXYWNFlXkG6eaP_py0tPDngWXJ56Bc7Bc7jYRcoL48I5LOjoR7Aro8ZHVoOyaFvByc3gCk65_rNzcYV8hNYCJcerkdQ1mZtUtRfkYOhq2sSlKQZv2nvG4pWvXcL4U4wKPiLT_oTCjauHMsBHLE1wlYfuoSkXwGRoUK5bCT7E7PQipu9EuOMGzEfRA1iRodJDB1WGxFVXrNVkYfvGF_GRSKBD4qVhIopjOW1GW7hNLFCIaC8Bs3sPiPQ5jR_GCB285_QOPn9rh_4ug2HNacjGcSJvd-QvnWF588p0t2eTCtfsOvZZilfQWff_sLOZeP7LreXjpSVqzxOK1o9WwKkgNpeCn7Kt7NQbHhk_oody0xyrPPvuRTDTXDjfym2CE826qkbfEe-kYxlx_4zckTGAwiYJU5z66y8grp07uMSftNeR61mJuIDMZPUXbCW7gTrDsEiJNg4gE0lu54ZgVqTWcIMA5Zp5fnYUdkvooekoSXjOKYxhkftn6Z0NA9NcoePldAnPN53yyeodvBTve0191lLZT3op7itPk6bwC5RUtIT2MObBorD_slOMh-S-WUJSq8fJaVvEKbFjxCL2-3xQYGPgN6CCxz0HcERHCFxhcl0x5STfB6_TNwdJRVD8zdazzEXTBSK9vA7f6dgfoRkM0M_0dA=s64","userId":"03086069160226397014"}},"outputId":"31b66e9e-f197-4a8a-93cf-e0fb8180a4b2"},"source":["# Define the target values\n","Y =  [0,0,0,0,0,1,0,1,1,1,1,1,1]\n","\n","# Define the 2 input features in a instance wise fashion\n","X = np.array([[0.3,50000],\n","              [3.2,100000],\n","              [0.4,170000],\n","              [0.8,120000],\n","              [2,150000],\n","              [1.2,250000],\n","              [1.0,160000],\n","              [2,210000],\n","              [2,300000],\n","              [2,400000],\n","              [3,300000],\n","              [3,340000],\n","              [1,350000]\n","             ])\n","\n","# standardize our input data\n","X, means, stds = standardize(X)\n","\n","# Define the function parameters - these were figured out using gradient descent (not shown here)\n","theta = [3.6,2.6,11]\n","\n","# initialize values for features x1 and x2\n","x1_range = np.arange(0,4,0.1) \n","x2_range = np.arange(0.0,500000,1000.0) \n","\n","x1s, x2s = np.meshgrid(x1_range, x2_range) # remember - plt.surface likes this meshgrid output\n","mesh_rows, mesh_cols = x1s.shape\n","\n","# construct 1D arrays for x1, x2 and x0\n","x1 = np.ravel(x1s)\n","x2 = np.ravel(x2s)\n","x = np.vstack((x1,x2)).T\n","\n","# rescale our data based on our normalization parameters\n","x =  (x-means)/stds\n","\n","# define the new matrix X based around our standardized vector for X previously obtained \n","x_0 = np.array([1]*len(x2))\n","x = np.hstack((x_0[:, np.newaxis],x))\n","\n","# perform calculations\n","z = h_logistic(theta,x.T)\n","\n","# reshape result to have the same organization as result of meshgrid\n","zs = np.reshape(z,(mesh_rows,mesh_cols))\n","x1s = np.reshape(x[:,1],(mesh_rows,mesh_cols))\n","x2s = np.reshape(x[:,2],(mesh_rows,mesh_cols))\n","\n","for l in range(len(Y)):\n","    if Y[l] == 0:\n","        plt.plot(X[l,0],X[l,1],'o',c='r')\n","    elif Y[l] == 1:\n","        plt.plot(X[l,0],X[l,1],'o',c='b')\n","    else: print(Y[l])\n","\n","plt.contour(x1s, x2s, zs, 1)\n","plt.show()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"SVEx4QeLlYzf"},"source":["A logistic neuron is simply a different architectural perspective on the logistic function. Structurally an artificial neuron has the following components:\n"," * A set of inputs x - these are the values that are fed into the neuron \n"," * A set of weights w - each weight $w_{i}$ can be interpreted as the relative strength (positive or negative) that we assign to an individual input value $x_{i}$\n"," * A cell body which computes the output for this neuron - the cell body can compute different types of outputs. For our purposes here we assume the output is a logistic function over the inputs x and the weights w. \n"," * An output which holds the computed logistic function\n"," \n","We can illustrate this structure with respect to our faulty truck part classifier with the figure below. \n","\n","<!-- neurons.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1YEoJWzGJdZ8FJUaQHI5pjlzNOtBP_Rmr\"/>\n","\n","As previously described, in logistic regression we appended a vector of 1s to our input data array X in order to facilitate a vectorized implementation of linear functions. The same is accomplished in the architecture of the neuron through the addition of the **bias unit** which:\n"," 1. is always assumed to have a value = 1\n"," 2. Is usually referenced as the $0^{th}$ input unit to the neuron.  \n","\n","In the example above we draw in the bias unit and weights explicitly, but it is important to note that in drawings we usually omit the bias unit and often the values for weights. Thus the example above would more normally be drawn as shown below. \n","\n","<!-- neurons2.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1YL1Z1RY8zApFvL30Gi2pE0KrCuKiBa8-\"/>\n","\n","\n","## Combining Logistic Units\n","Logistic units can be combined to implement more complex functions. We can examine this by looking at the design of individual binary operations, and then the combination of these units to achieve more sophisticated operations. \n","\n","### AND Units\n","\n","Consider first the construction of the simple AND operator defined over two variables x1 and x2. The desired logic for an AND operation is defined as follows:\n","\n","x1 | x2 | x1 AND x2\n","--- | --- | :---:\n","0 |\t0 |\t0\n","0 |\t1 |\t0\n","1 |\t0 |\t0\n","1 | 1 | 1\n","\n","where x1, x2 and are result are all binary values. \n","\n","It is relatively easy to find the parameters for a logistic unit which will result in computing an AND function. Such a unit (illustrated fully with bias units and weights) is illustrated below. \n","\n","<!-- and.png -->\n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1XpDFm3ANBi0G7pC-u2vlf3L9dTrLmtix\"/>\n","\n","\n","Noting that the value for $x0 = 1$ and $x1,x2 \\in \\{0,1\\}$, we see that the parameter on the bias unit pushes the scalar product of inputs and parameters to a minimum value of -30. The only way this scalar sum of inputs and weights can go above 1 is if both the inputs x1 and x2 are 1. This is summarized in the table below:\n","\n","x1 | x2 | $\\theta x$ | g($\\theta$ x) \n","--- | --- | :---: | :---: \n","0 |\t0 |\t-30 | 0\n","0 |\t1 |\t-10 | 0 \n","1 |\t0 |\t-10 | 0 \n","1 | 1 | 10  | 1 \n","\n","Note that the values of g are actually rounded a little bit. You can see the actual values below and tweak the parameters below. "]},{"cell_type":"code","metadata":{"id":"TcVKg_6DlYzg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613376768341,"user_tz":0,"elapsed":1272,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJpIFOrrjxWZop0YQn_ZEscjIkShJyJr2Xxqa_yZWdjHwXd2pMnoYIA_RRCSI0sSkQXj3pJzNjFghSUre_OjA1-Tj3b1CzHWCTlgCLWNSHYXYWNFlXkG6eaP_py0tPDngWXJ56Bc7Bc7jYRcoL48I5LOjoR7Aro8ZHVoOyaFvByc3gCk65_rNzcYV8hNYCJcerkdQ1mZtUtRfkYOhq2sSlKQZv2nvG4pWvXcL4U4wKPiLT_oTCjauHMsBHLE1wlYfuoSkXwGRoUK5bCT7E7PQipu9EuOMGzEfRA1iRodJDB1WGxFVXrNVkYfvGF_GRSKBD4qVhIopjOW1GW7hNLFCIaC8Bs3sPiPQ5jR_GCB285_QOPn9rh_4ug2HNacjGcSJvd-QvnWF588p0t2eTCtfsOvZZilfQWff_sLOZeP7LreXjpSVqzxOK1o9WwKkgNpeCn7Kt7NQbHhk_oody0xyrPPvuRTDTXDjfym2CE826qkbfEe-kYxlx_4zckTGAwiYJU5z66y8grp07uMSftNeR61mJuIDMZPUXbCW7gTrDsEiJNg4gE0lu54ZgVqTWcIMA5Zp5fnYUdkvooekoSXjOKYxhkftn6Z0NA9NcoePldAnPN53yyeodvBTve0191lLZT3op7itPk6bwC5RUtIT2MObBorD_slOMh-S-WUJSq8fJaVvEKbFjxCL2-3xQYGPgN6CCxz0HcERHCFxhcl0x5STfB6_TNwdJRVD8zdazzEXTBSK9vA7f6dgfoRkM0M_0dA=s64","userId":"03086069160226397014"}},"outputId":"49e03ac5-b1f6-41a5-a7bf-2b8e1c4abec5"},"source":["def h_log(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def calc(x1,x2,theta0,theta1,theta2):\n","    x0 = 1\n","    z = x0*theta0 + x1*theta1 + x2*theta2\n","    g = h_log(z)\n","    return z, g\n","\n","# parameters for AND approximation \n","theta0 = -30\n","theta1 = 20\n","theta2 = 20\n","\n","z, g = calc(0,0,theta0,theta1,theta2)\n","print(str(z) + \" \" + str(g))\n","\n","z, g = calc(0,1,theta0,theta1,theta2)\n","print(str(z) + \" \" + str(g))\n","\n","z, g = calc(1,0,theta0,theta1,theta2)\n","print(str(z) + \" \" + str(g))\n","\n","z, g = calc(1,1,theta0,theta1,theta2)\n","print(str(z) + \" \" + str(g))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["-30 9.357622968839299e-14\n","-10 4.5397868702434395e-05\n","-10 4.5397868702434395e-05\n","10 0.9999546021312976\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AT7GFBnGlYzg"},"source":["### OR Units\n","\n","Similarly the OR operator can be defined over two binary variables $x1$ and $x2$. The logic for the OR operator is summarized below:\n","\n","x1 | x2 | x1 OR x2\n","--- | --- | :---:\n","0 |\t0 |\t0\n","0 |\t1 |\t1\n","1 |\t0 |\t1\n","1 | 1 | 1\n","\n","By choosing appropriate weights for a logistic unit we can once again achieve the desired logic as illustrated below. \n","\n","<!-- or.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1YM9MEpa-OVAcNXJrWcW04h87C3FmKKzM\"/>\n","\n","Here the bias unit only partly pushes the scalar product of inputs and parameters below 1. In the case of either or both x1 and x2 being set to 1, the total scalar product will be greater than 0 with the result being that the logistic function has a total output 1. This can be illustrated in the table below. \n","\n","x1 | x2 | $\\theta x$ | g($\\theta$ x)\n","--- | --- | :---: | :---:\n","0 |\t0 |\t-1 | 0\n","0 |\t1 |\t1  | 1 \n","1 |\t0 |\t1  | 1 \n","1 | 1 | 3  | 1 \n","\n","### NOT Units\n","\n","The final atomic unit we will consider is the NOT unit. The NOT unit takes only one explicit input $x1$ and returns the negation of the value on that input as summarized below:\n","\n","x1 | NOT x1\n","--- | :---:\n","0 |\t1 \n","1 |\t0 \n","\n","A logistic unit which simulates the behaviour of a NOT operation when applied to binary values is illustrated below. \n","\n","<!-- not.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1YLhgaCDoZaQMOCq6iBbys3N8bJ5W-Lai\"/>\n","\n","\n","Here when x1 is set the 0, the effect of the bias unit with its weight will be to force the scalar product $\\theta x$ to be greater than 0. However if $x1=1$ the strong negative value of the weight on $x2$ will lead the scalar product $\\theta x$ to be less than 0. This is summarized in the table below. \n","\n","x1  | $\\theta x$ | g($\\theta$ x)\n","--- | :---:      | :---:\n","0   |     1      |        1\n","1   |    -1      |        0 \n","\n","### XNOR Units\n","\n","Given the AND, OR, and NOT units defined above we can implement more sophisticated functtions based on those units. To illustrate let us consider the XNOR operation which is defined with the following truth table:\n","\n","x1 | x2 | x1 XNOR x2\n","--- | --- | :---:\n","0 |\t0 |\t1\n","0 |\t1 |\t0\n","1 |\t0 |\t0\n","1 | 1 | 1\n","\n","XNOR requires both values $x1$ and $x2$ to have the same inputs. The XNOR operation may be defined in terms of our 3 fundamental operations as follows: \n","\n","\\begin{equation}\n","x1\\; XNOR\\; x2 = (x1\\; AND\\; x2)\\; OR\\; ((NOT\\; x1)\\; AND\\; (NOT\\; x2))\n","\\end{equation}\n","\n","By allowing the outputs of some units to act as inputs to another units we can implement the above equation with logistic units as follows: \n","\n","<!-- xnor.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1YWyh4AT8jD2c60mWopZxmMC0PQLqd6xE\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"qfICCOP6lYzh"},"source":["The point of all of this is simply this. We can use logistic units to build all types of simple binary functions, and by layering them on top of each other we can build more complex functions. Constructing a XNOR unit is just a basic example. By having more and more layers of units we can build up even more complex functions if we want to -- this can be complex enough to understand words, images and the like. The example above though is handcrafted, so lets move beyond this handcrafted approach. \n","\n","## The Architecture of a Neural Network\n","\n","The network of logistic units used above to define the XNOR operation captures many of the principles of standard neural networks. In this section we discuss the standard basic form of a Neural Network, which is the Feedforward Network. We discuss the Feedforward Network specifically where the network is built exclusively out of logistic units. Networks built out of many other unit types are possible and we will address them later. \n","\n","### Layers of the Network \n","\n","The Feedforward Neural Network consists of at least three layers of artificial neurons as illustrated below. \n","\n","<!-- ffnn.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1Y4-m_Cg3zvW777Qw4xFgW0D8gQwf8NHM\"/>\n","\n","In the Feedforward architecture we usually talk about three distinct layer types, i.e., the input layer, the hidden layers, and the output layer. During processing an input layer is clamped to an input data vector, i.e., $x$. The output layer meanwhile captures the result of our complete model, i.e., $Y$. One or more hidden layers can be found between the input and output layers. The hidden layers encode higher order features which operate over the input data which transform the data prior to the final classification at the output layer. \n","\n","Each node in layer $j$ is connected to each node in layer $j-1$ through connections which we refer to as weights. The exception to this is layer 0 which is directly clamped to input data as noted above. In practice weights are simply the parameters we assumed in linear and logistic regression. \n","\n","### Multi-class Classification\n","\n","In the example above we have only one neuron in the output layer but there can be many more. For example in the larger network below thee are three output units. \n","\n","<!-- ffnn2 --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1Y4ulL3VQ67GUAFK-6t5D5gFbntQRqVUa\"/>\n","\n","\n","This means that we can build 3 binary classifiers in parallel. This gives us our normal way of handling multi-class classification problems. For a target, e.g., animal type, with 3 classes, e.g., dog, cat and hamster, we create a binary output unit for each of those 3 types. In training the network on a multi-class classification problem we expect only one output unit to be active at a time. During classification we ideally want only one output unit to be active, but in practice most will have some activity and we select the output node that has the highest activity as our hypothesised class. This is referred to as the **One versus All** method. \n","\n","### Bias Units\n","\n","Each layer (with the exception of the output layer) also has a bias unit associated with the layer. We include the bias unit in the network below for illustration, and note that bias unit in layer $j$ is not connected to units in unit $j-1$. The is because the value of the bias unit is fixed to 1. \n","\n","<!-- ffnn3 --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1YDqvM3v_uzEmqate7FAZu1TOGyWzwsCD\"/>\n","\n","### Deep Networks \n","\n","Deep Feedforward Neural Networks are simply defined as Feedforward Netowrks that have more than one hidden layer. For example even this simple model below would be technically referred to as a Deep Network: \n","\n","<!-- dnn.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1XqmILbylkypZCLuyk46DeVQ97plQZQ_W\"/>\n","\n","\n","Although more typically our networks will in practice have many more nodes at each layer and a number of hidden layers. \n","\n","\n","<!-- dnn2.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1XtkT1tIkuSrcCaQGrW3yAEpzu0ZiFPQp\"/>\n","\n","\n","## Notational Conventions\n","\n","When working through descriptions of neural network one of the most challenging problems is keeping track of the notation - particularly with respect to indexing through multiple layers of the network and multiple training and testing examples. \n","\n","### Notation for Data\n","As with any other supervised machine learning method, our model is trained with a set of m input-output tuples:\n","\n","\\begin{equation}\n","Data_{Training} = \\{ (x^{0},y^{0}),(x^{1},y^{1}),(x^{2},y^{2})\\dots (x^{m-1},y^{m-1}) \\}\n","\\end{equation}\n","\n","Each tuple is an individual training case which we index with a superscript. \n","\n","For each individual training case we assume vectorized inputs and outputs. The length of an input vector must be equal to the number of units on the input layer (not counting the bias unit). Similarly the length of an output vector must be equal to the number of units on the output layer of the network. Thus for a network with 3 input units and 2 output units:\n","\n"," * $x^{i} \\in \\{0,1\\}^{3\\times1}$\n"," * $y^{i} \\in \\{0,1\\}^{2\\times1}$\n"," \n","### Notation for Layers\n","A neural network is built out of a number of layers. A layer typically includes both the nodes and a set of associated weights. In indexing and describing the layers of an individual network we use the following notation:\n"," * $L$ - the total number of layers in the network\n"," * $s_{l}$ - the number of nodes the the $l^{th}$ layer of the network - excluding bias units. \n","\n","In many sources our first layer is indexed as layer 1 rather than layer 0. However this only makes the programming of these networks more cumbersome. Here we will stick with standard computer science convention of indexing from 0. Hence the input layer is layer 0 and our output layer is layer $L-1$. \n","\n","#### Notation for Nodes\n","\n","Each layer includes a set of activation units. We rarely index the units themselves. Instead we index the values produced at those units. We refer to the output of a node as its activation value. For the first layer the activation value for a given unit is simply the corresponding input value from the data. For all other layers the activation value for a unit is its computed output. For logistic units, this is the logistic function applied to the scalar product of the values of the weights and activations of the $j-1$th layer. \n","\n","We denote activation values with the character $a$. A superscript on $a$ is used to index layers. A subscript is used to index the activation values for an individual unit within that layer. To illustrate:\n"," * $a^{0}$ refers to the vector of activation values for our input layer. This will correspond to our input vector $x$\n"," * $a^{L-1}$ refers to the vector of activation values for our output layer. This will be our output vector $y$. \n"," * $a^{1}$ refers to the vector of activation values on our (first) hidden layer. \n"," * $a^{1}_{0}$ refers to the activation value of the bias unit on our (first) hidden layer. This will always be 1. \n"," * $a^{1}_{3}$ refers to the activation value of the 3rd (non-bias) unit in the (first) hidden layer as illustrated below. \n","\n","<!-- activation.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1XmYwPWuLyzNZCyfjbs9ETpdyvTQnj68t\"/>\n","\n","\n","As well as indexing the activation values of the layer, it is often useful to index the partial result $\\theta X$. We refer to this value as $z$ and use the same indexing conventions as with the activation values. It is worth noting though that no values for $z$ exist for the input layer as our activation values for that layer come directly from the input vector $x$ rather than through an application of the logistic function. \n"," \n","#### Notation for Weights\n","\n","For our current purposes the weights associated with layer $j$ are the weights which follow the outputs from the nodes rather than the weights which feed into nodes. Therefore all layers in the network apart from the output layer have a set of weights. (It is worth noting as an aside here that in implementations we often reverse this notation and have weights associated with their target rather than their source layer.)\n","\n","Weights are denoted with the character $\\theta$. A superscript on $\\theta$ is used to index layers. A subscript is used to index individual weights within a layer of weights. That subscript is based on a tuple where the first index refers to the source of the weight and the second index refers to the target. \n","\n"," * $\\theta^{0}$ refers to the weights emerging from the input layer\n"," * $\\theta^{L-2}$ refers to the layer of weights feeding in to the output layer\n"," * $\\theta^{1}$ refers to the weights emerging from the (first) hidden layer\n"," * $\\theta^{1}_{0,1}$ refers to the weight from the bias unit in the (first) hidden layer to the first unit in the following layer\n"," * $\\theta^{1}_{5,2}$ refers to the weight highlighted in the figure below. \n"," \n","\n","<!-- weights.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1YWkEFZ1gQe3Qfwzgpvr6BkQxnQ50iCvU\"/>\n","\n","\n","\n","Note that the target index should never be equal to 0 since bias units are never updated. Note also that the output layer does not have a set of weights defined, therefore $\\theta^{L-1}$ and $\\theta^{L}$ are both invalid indices.  \n","\n","## The Feedforward Algorithm\n","\n","The feedfoward algorithm is a parallelized (within a layer) and sequentialized (down through the layers) extension of applying individual activation functions. In our case below the activation functions are all logistic functions but it is important to remember that other possibilities exist. \n","\n","To illustrate the use of the feedforward algorithm let us assume a network with the topology illustrated below. \n","\n","<!-- example.png --> \n","<img width=\"400\" src=\"https://drive.google.com/uc?id=1XvUyRTcYu_IErTvfRY8MZr6venBhaHVi\"/>\n","\n","Here:\n"," * L = 4\n"," * $S_{0} = 3$, $S_{1} = 3$, $S_{2} = 3$, $S_{3} = 1$\n","\n","We also assume a single input X. Since $S_{0} = 3$ then the length of the individual input vector is also 3. We also assume a randomly initialized set of weights. \n","\n","The feedforward algorithm itself can be implemented simply as a sequence of logistic function executions as illustrated in the example below: "]},{"cell_type":"code","metadata":{"id":"WMFwFqqKlYzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613376768342,"user_tz":0,"elapsed":1261,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJpIFOrrjxWZop0YQn_ZEscjIkShJyJr2Xxqa_yZWdjHwXd2pMnoYIA_RRCSI0sSkQXj3pJzNjFghSUre_OjA1-Tj3b1CzHWCTlgCLWNSHYXYWNFlXkG6eaP_py0tPDngWXJ56Bc7Bc7jYRcoL48I5LOjoR7Aro8ZHVoOyaFvByc3gCk65_rNzcYV8hNYCJcerkdQ1mZtUtRfkYOhq2sSlKQZv2nvG4pWvXcL4U4wKPiLT_oTCjauHMsBHLE1wlYfuoSkXwGRoUK5bCT7E7PQipu9EuOMGzEfRA1iRodJDB1WGxFVXrNVkYfvGF_GRSKBD4qVhIopjOW1GW7hNLFCIaC8Bs3sPiPQ5jR_GCB285_QOPn9rh_4ug2HNacjGcSJvd-QvnWF588p0t2eTCtfsOvZZilfQWff_sLOZeP7LreXjpSVqzxOK1o9WwKkgNpeCn7Kt7NQbHhk_oody0xyrPPvuRTDTXDjfym2CE826qkbfEe-kYxlx_4zckTGAwiYJU5z66y8grp07uMSftNeR61mJuIDMZPUXbCW7gTrDsEiJNg4gE0lu54ZgVqTWcIMA5Zp5fnYUdkvooekoSXjOKYxhkftn6Z0NA9NcoePldAnPN53yyeodvBTve0191lLZT3op7itPk6bwC5RUtIT2MObBorD_slOMh-S-WUJSq8fJaVvEKbFjxCL2-3xQYGPgN6CCxz0HcERHCFxhcl0x5STfB6_TNwdJRVD8zdazzEXTBSK9vA7f6dgfoRkM0M_0dA=s64","userId":"03086069160226397014"}},"outputId":"58016140-d708-4ed3-f4c2-642eb41f37c7"},"source":["# Define the input vector to our network\n","X = np.array([[0.5,1.1,1.6]])\n","X = X.T\n","\n","# Define the weights for all layers\n","Theta1 = np.random.rand(4,3)\n","Theta2 = np.random.rand(4,3)\n","Theta3 = np.random.rand(4,1)\n","\n","# construct the activation values for layer 1 from X and a bias value\n","a1 = np.vstack(([1.0],X))\n","\n","# calculate z for the first hidden layer\n","z2 = np.matmul(Theta1.T,a1)\n","# calculate a for the first hidden layer\n","a2 = np.vstack(([1.0],1 / (1 + np.exp(-z2))))\n","\n","# calculate z for the second hidden layer\n","z3 = np.matmul(Theta2.T,a2)\n","# calculate a for the second hidden layer\n","a3 = np.vstack(([1.0],1 / (1 + np.exp(-z3))))\n","\n","# calculate z for the output layer\n","z4 = np.matmul(Theta3.T,a3)\n","# calculate a for the output layer\n","a4 = 1 / (1 + np.exp(-z4))\n","\n","print(a4)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[0.91887512]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xt8mKRohlYzj"},"source":["Note the following about the implementation above:\n"," * When we initialize weights we must remember that weights must also be initialized for weights coming from (but not to) bias units. \n"," * When we construct the activation values for a given layer we add the bias unit value to the layer at that point \n"," * No bias unit is added to the output layer when we compute the final set of activation functions\n","\n","Given a set of appropriate parameters we can rework the code above with some arrays for storage and a loop over layers which gives us a more extensible implementation. "]},{"cell_type":"code","metadata":{"id":"-sCjhqphlYzk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613376768344,"user_tz":0,"elapsed":1251,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjJpIFOrrjxWZop0YQn_ZEscjIkShJyJr2Xxqa_yZWdjHwXd2pMnoYIA_RRCSI0sSkQXj3pJzNjFghSUre_OjA1-Tj3b1CzHWCTlgCLWNSHYXYWNFlXkG6eaP_py0tPDngWXJ56Bc7Bc7jYRcoL48I5LOjoR7Aro8ZHVoOyaFvByc3gCk65_rNzcYV8hNYCJcerkdQ1mZtUtRfkYOhq2sSlKQZv2nvG4pWvXcL4U4wKPiLT_oTCjauHMsBHLE1wlYfuoSkXwGRoUK5bCT7E7PQipu9EuOMGzEfRA1iRodJDB1WGxFVXrNVkYfvGF_GRSKBD4qVhIopjOW1GW7hNLFCIaC8Bs3sPiPQ5jR_GCB285_QOPn9rh_4ug2HNacjGcSJvd-QvnWF588p0t2eTCtfsOvZZilfQWff_sLOZeP7LreXjpSVqzxOK1o9WwKkgNpeCn7Kt7NQbHhk_oody0xyrPPvuRTDTXDjfym2CE826qkbfEe-kYxlx_4zckTGAwiYJU5z66y8grp07uMSftNeR61mJuIDMZPUXbCW7gTrDsEiJNg4gE0lu54ZgVqTWcIMA5Zp5fnYUdkvooekoSXjOKYxhkftn6Z0NA9NcoePldAnPN53yyeodvBTve0191lLZT3op7itPk6bwC5RUtIT2MObBorD_slOMh-S-WUJSq8fJaVvEKbFjxCL2-3xQYGPgN6CCxz0HcERHCFxhcl0x5STfB6_TNwdJRVD8zdazzEXTBSK9vA7f6dgfoRkM0M_0dA=s64","userId":"03086069160226397014"}},"outputId":"6ca22a8d-1cf4-4488-c1c7-c06be4661084"},"source":["# Define the input vector to our network\n","X = np.array([[0.5,1.1,1.6]])\n","X = X.T\n","\n","# Define the number of (visible) units in each layer \n","S = [3,3,3,1]\n","\n","# Define empty arrays for theta, activation and z values\n","theta = [None] * len(S)\n","a =  [None] * len(S)\n","z =  [None] * len(S)\n","\n","# Initialise the weights for all layers\n","for idx, val in enumerate(S):\n","    # we don't have weights for the final layer\n","    if idx == (len(S)-1): \n","        break\n","    theta[idx] = np.random.rand( S[idx]+1,S[idx+1])\n","\n","# Implement the Feedforward Algorithm \n","for idx, val in enumerate(S):\n","    if idx == 0:\n","        # construct the activation values for layer 1 from X and a bias value\n","        a[idx] = np.vstack(([1.0],X))\n","    elif idx == (len(S)-1):\n","        # calculate z for the output layer\n","        z[idx] = np.matmul(theta[idx-1].T,a[idx-1])\n","        # calculate a for the output layer\n","        a[idx] = 1 / (1 + np.exp(-z[idx]))\n","    else:\n","        # calculate z for a hidden layer\n","        z[idx] = np.matmul(theta[idx-1].T,a[idx-1])\n","        # calculate a for a hidden layer\n","        a[idx] = np.vstack(([1.0],1 / (1 + np.exp(-z[idx]))))\n","print(a4)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[[0.91887512]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j2dZghmy0Nam"},"source":["## Tutorial Assignments\n","\n","1. Create a Jupyter notebook called “week 04 work”. \n","For each of the items below complete the work it its own cell. We will refer to the contents of the cell as a “programme” just because it is a natural way of talking.  The contents of a single cell are not a programme in the normal sense of the word of course. \n","2. Write a programme to print out “Hello network”\n","Write a programme that creates a python array with the integers 0 to 9 in it, and then prints them\n","3. Write a programme that creates a numpy vector with the integers 0 to 9 in it and then try to print the vector\n","4. Write a programme to print out the cube of each element in the array created in item 3. Try doing this with a loop.\n","5. Write a programme to print out the cube of each element in the vector created in item 4. Try doing this with vector notation. \n","6. Write a function name my_fun that takes two arguments as input. The function should assume that the first item passed into the function is a vector and the second item is a scalar. The function should return a new vector which is the vector raised to the power of the scalar. Test the function with your own selection of vectors and scalars. \n","7. Write a function named my_fun2 that takes as input a single numpy vector and returns the sum of the squares of that vector. Test with input [1,1,1,1] and [2,2,2,2]. \n","8. Write a function that takes as input a vector and a scalar. The function should return the sum of the square of the difference between each individual item and the scalar. Test with your own data. \n","9. Write a function that takes a vector as input and returns the log of each item in the vector as another vector. Test with your own data. Look in particular at the results you get for inputs such as 0, 0.5, 1, 1.5. \n","10. Test the function created in point 10 with a sequence of numbers 0.1, 0.2, 0.3, …. 2.0. Use matplotlib to plot the data with a line plot where the input numbers are the X axis and the returned values give the Y axis. \n","11. Using your own intuitions and trial and error, give parameters Theta0 and Theta1 for the line below. Theta0 is c and Theta1 is m in the traditional highschool equation y=mx+c\n","12. Use matplotlib to draw two distinct lines in 3D that intersect. \n","\n","\n","<!-- graph.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1SwLJNMIflhmri0OKCV-MzF4R2QFWz1W6\"/>\n","\n","\n"]}]}