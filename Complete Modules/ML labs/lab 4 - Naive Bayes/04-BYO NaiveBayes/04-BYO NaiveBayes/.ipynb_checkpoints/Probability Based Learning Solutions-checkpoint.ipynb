{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability-Based Learning\n",
    "\n",
    "In this lab we will implement functions for calculating probability, joint probability and working out the **maximum a posteriori (MAP)** prediction for a given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Probability\n",
    "\n",
    "In order to build a Naive Bayes model, we need to be able to calculate the probability of an event given evidence. This is quite a straightforward calculation. Say we want to calculate the probability that a patient has a headache, our *evidence* is the dataset of patient records contained in `meningitis.csv`. The probability of that a patient has a headache is then\n",
    "\n",
    "\n",
    "$\\frac{NROW(Headache)}{NROW(All)}$\n",
    "\n",
    "In the example below, use the pandas `.loc` function to calculate the probability that a patient has a headache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headache</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Vomiting</th>\n",
       "      <th>Meningitis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Headache  Fever  Vomiting  Meningitis\n",
       "ID                                       \n",
       "1       True   True     False       False\n",
       "2      False   True     False       False\n",
       "3       True  False      True       False\n",
       "4       True  False      True       False\n",
       "5      False   True     False        True\n",
       "6       True  False      True       False\n",
       "7       True  False      True       False\n",
       "8       True  False      True        True\n",
       "9      False   True     False       False\n",
       "10      True  False      True        True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('meningitis.csv')\n",
    "df = df.set_index('ID')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "nrow_headache = len(df.loc[df['Headache']==True])\n",
    "nrow_all = len(df)\n",
    "prob_headache = nrow_headache / nrow_all\n",
    "print(prob_headache) # 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've calculated the probability for a single column, we can write a function which will do this for any column we choose. For the moment, we'll assume that any given column is a boolean column, and we'll return the probability of that column having the value `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Headache): 0.7\n",
      "P(Fever): 0.4\n",
      "P(Vomiting: 0.6\n"
     ]
    }
   ],
   "source": [
    "def calculate_probability(df: pd.DataFrame, column: str) -> float:\n",
    "    return len(df.loc[df[column]==True]) / len(df)\n",
    "\n",
    "print(f\"P(Headache): {calculate_probability(df, 'Headache')}\") # 0.7\n",
    "print(f\"P(Fever): {calculate_probability(df, 'Fever')}\") # 0.4\n",
    "print(f\"P(Vomiting: {calculate_probability(df, 'Vomiting')}\") # 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Factor Table\n",
    "\n",
    "If we wanted to build up a full contingency table of all the possible probabilities we'd need at least one row representing every possible combination of features, but due to the curse of dimensionality, we'd still be very susceptible to outliers. The Naive Bayes works around this by assuming **conditional independence**. That is, we ignore any interaction between the feature columns, and we're only interested in the probability of each feature **given the target variable**.\n",
    "\n",
    "For the meningitis dataset above, this means that we only need to calculate the probability of a patient having a headache *when they have meningitis* and the probability of a patient having a headache *when they don't have meningitis*. In order to do this we can use our `calculate_probability` function, above. To calculate the factors for *headache* we first split our dataset into two, rows of patients who have meningitis and rows of patients who don't have meningitis. We then work out the probability of headache for each of these subsets\n",
    "\n",
    "Each feature column has 2 entries in the factor table, one for `meningitis=True` and one for `meningitis=False`. It makes sense, then, for our function to return two values rather than one. Python allows you to return multiple values from a function using the comma operator. For example, the following function returns the most common word in a string of text along with the number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_mode_and_count(text: str) -> Tuple[str, int]:\n",
    "    word_dict = dict()\n",
    "    for word in text.split(' '):\n",
    "        word_dict[word] = word_dict.get(word, 0) + 1\n",
    "    \n",
    "    most_common_word = max(word_dict, key=word_dict.get)\n",
    "    word_count = word_dict[most_common_word]\n",
    "    return most_common_word, word_count\n",
    "\n",
    "mode, count = get_mode_and_count('the quick brown fox jumped over the lazy dog')\n",
    "mode, count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we want to use a TypeHint for a function with multiple return values we use the *Tuple* type. A tuple is like a fixed size list with definite types (it's essentially the same thing as a database row). We can use the same approach to return a tuple of probabilities for each feature column; one for each possible target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factors(Headache): (0.6666666666666666, 0.7142857142857143)\n",
      "Factors(Fever): (0.3333333333333333, 0.42857142857142855)\n",
      "Factors(Vomiting: (0.6666666666666666, 0.5714285714285714)\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def calculate_factors(df: pd.DataFrame, column: str, target_column: str) -> Tuple[float, float]:\n",
    "    return calculate_probability(df.loc[df[target_column]==True], column), \\\n",
    "        calculate_probability(df.loc[df[target_column]==False], column)\n",
    "\n",
    "print(f\"Factors(Headache): {calculate_factors(df, 'Headache', 'Meningitis')}\") # 0.667, 0.714\n",
    "print(f\"Factors(Fever): {calculate_factors(df, 'Fever', 'Meningitis')}\") # 0.333, 0.429\n",
    "print(f\"Factors(Vomiting: {calculate_factors(df, 'Vomiting', 'Meningitis')}\") # 0.667, #0.571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating *A Posteriori* Probabilities from Factors\n",
    "\n",
    "We've looked at how to calculate factors from a dataset. The next thing we need to build our Naive Bayes is some way to calculate *a posteriori* probabilities given a factor table. Let's take another look at a basic factors table to see how this works\n",
    "\n",
    "![Factors](factors.png)\n",
    "\n",
    "We can see our table contains one factor for each feature, and another for the overall probability of the target. We can see from the table above that the probability of a headache given that a patient has meningitis is 0.666. We didn't make it explicit, but it follows that the probability of *not* having a headache given that a patient has meningitis is 0.333 (from the theorem of total probability).\n",
    "\n",
    "How do we work out the probability that a patient who has headache, no fever and is vomiting has meningitis?\n",
    "\n",
    "1. First, we calculate the *a posteriori* probability for meningitis=True\n",
    "    * This is the prior $P(meningitis=True)$ multiplied by\n",
    "    * $P(headache)$ multiplied by\n",
    "    * $1 - P(Fever)$ multiplied by\n",
    "    * $P(Vomiting)$\n",
    "2. Then, we do the same for meningitis=False (using the second column of our factor table)\n",
    "3. Finally, return the prediction with the maximum *a posteriori* prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that becomes clear is that we can easily separate our table into two. In step 1 we're only interested in the probabilities for `meningitis=True`, in step 2 we're only interested in the probabilities for `meningitis=False`\n",
    "\n",
    "The next thing we need to be able to do is find the factor value for any given column. We're associating each factor in this table with a column in our dataframe. The easiest way to tie two values together in Python like this is to use a dictionary. In the cell below, create a dictionary containing the values factors for `headache`, `fever` and `vomiting`.\n",
    "\n",
    "To get back the probability of a patient having a headache given that they have meningitis you would use \n",
    "\n",
    "```python\n",
    "factor_dict['headache']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meningitis': (0.3, 0.7), 'headache': (0.6667, 0.7143), 'fever': (0.3333, 0.4286), 'vomiting': (0.6667, 0.5714)}\n"
     ]
    }
   ],
   "source": [
    "factor_dict = dict()\n",
    "factor_dict['meningitis'] = (0.3, 0.7)\n",
    "factor_dict['headache'] = (0.6667, 0.7143)\n",
    "factor_dict['fever'] = (0.3333, 0.4286)\n",
    "factor_dict['vomiting'] = (0.6667, 0.5714)\n",
    "\n",
    "print(factor_dict) # {'headache': (0.6667, 0.7143), 'fever': (0.3333, 0.4286), 'vomiting': (0.6667, 0.5714)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're storing our factor table as a dictionary we need a function which will calculate conditional probability using that dictionary. Remember that the factor table stores the probability that the descriptive feature is true, to find the probability that the descriptive feature is false we subtract it from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(h|m): 0.6667\n",
      "P(¬h|m): 0.33330000000000004\n",
      "P(f|m): 0.3333\n",
      "P(¬f|m): 0.6667000000000001\n",
      "P(v|m): 0.6667\n",
      "P(¬v|m): 0.33330000000000004\n",
      "P(h|¬m): 0.7143\n",
      "P(¬h|¬m): 0.28569999999999995\n",
      "P(f|¬m): 0.4286\n",
      "P(¬f|¬m): 0.5714\n",
      "P(v|¬m): 0.5714\n",
      "P(¬v|¬m): 0.4286\n"
     ]
    }
   ],
   "source": [
    "def calculate_column_conditional_probability(factor_dict: dict, target_value: bool, column: str, value: bool) -> float:\n",
    "    # our factor comes back as a tuple, the first element for target=True, second for target=False\n",
    "    factor = factor_dict[column][0] if target_value == True else factor_dict[column][1]\n",
    "    # we only store the probability of our descriptive feature being True, subtract from 1 for False\n",
    "    return factor if value == True else 1 - factor\n",
    "\n",
    "print(f\"P(h|m): {calculate_column_conditional_probability(factor_dict, True, 'headache', True)}\") # 0.6667\n",
    "print(f\"P(¬h|m): {calculate_column_conditional_probability(factor_dict, True, 'headache', False)}\") # 0.3333\n",
    "print(f\"P(f|m): {calculate_column_conditional_probability(factor_dict, True, 'fever', True)}\") # 0.3333\n",
    "print(f\"P(¬f|m): {calculate_column_conditional_probability(factor_dict, True, 'fever', False)}\") # 0.6667\n",
    "print(f\"P(v|m): {calculate_column_conditional_probability(factor_dict, True, 'vomiting', True)}\") # 0.6667\n",
    "print(f\"P(¬v|m): {calculate_column_conditional_probability(factor_dict,True, 'vomiting', False)}\") # 0.3333\n",
    "print(f\"P(h|¬m): {calculate_column_conditional_probability(factor_dict, False, 'headache', True)}\") # 0.7143\n",
    "print(f\"P(¬h|¬m): {calculate_column_conditional_probability(factor_dict, False, 'headache', False)}\") # 0.2857\n",
    "print(f\"P(f|¬m): {calculate_column_conditional_probability(factor_dict, False, 'fever', True)}\") # 0.4286\n",
    "print(f\"P(¬f|¬m): {calculate_column_conditional_probability(factor_dict, False, 'fever', False)}\") # 0.5714\n",
    "print(f\"P(v|¬m): {calculate_column_conditional_probability(factor_dict, False, 'vomiting', True)}\") # 0.5714\n",
    "print(f\"P(¬v|¬m): {calculate_column_conditional_probability(factor_dict,False, 'vomiting', False)}\") # 0.4286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating A Posteriori Values of a Row\n",
    "\n",
    "The *a posteriori* values are the conditional probabilities based on the value of our query (red) multiplied by the prior probability for the target value (blue). The divisor is a normalisation term which allows us to convert our *a posteriori* values into actual probabilities. You'll notice that the a posteriori values for True and False don't necessarily sum to 1; if we include the divisor they will. We can often ignore the divisor as usually we're interested only in what prediction the model will make rather than the actual probability the model assigns to its prediction. The larger of the two a posteriori values will always have a probability > 50% \n",
    "\n",
    "![Bayes Theorem](bayes_theorem_1.png)\n",
    "\n",
    "We've now got a function which will take a column name and a value and give us back the conditional probability. In order to make a prediction, we need to take a pandas row, and calculate the *a posteriori* probability for each column value. The datatype of a Pandas row is a *Series*. A series lets us extract values using a column name, but unlike a DataFrame it can only every contain one value per column. Notice that when we want to get the column names from a pandas Series we use the **.index** property unlike a DataFrame where we would use **.columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6667, 0.3333, 0.33330000000000004]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_conditional_probs(factor_dict: dict, row: pd.Series, target_value: bool) -> float:\n",
    "    conditional_probs = [calculate_column_conditional_probability(factor_dict, target_value, column, row[column]) for column in row.index]\n",
    "    return conditional_probs\n",
    "\n",
    "query = pd.Series({'headache': True, 'fever': True, 'vomiting': False})\n",
    "calculate_conditional_probs(factor_dict, query, True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducer Functions\n",
    "\n",
    "In the example above, we've used a list comprehension to calculate the conditional probability for each value in the query. The last thing we need to do is multiply all of these probabilities together. If we were adding all of the values together we could use the buit-in Python function `sum()`. Summing is a very common operation which is why it has been provided to us out of the box. The sum() function takes a list of numbers, applies an operation to them in turn and returns a single number as output. It takes multiple numbers and reduces them down to a single number, this is where the term **reducer** comes from.\n",
    "\n",
    "If we want to create our own custom reducer in Python we can do using the **reduce()** function. Let's take a look at how to to rewrite the sum() function using reduce()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accumulator: 0, next: 1, sum: 1\n",
      "accumulator: 1, next: 2, sum: 3\n",
      "accumulator: 3, next: 3, sum: 6\n",
      "accumulator: 6, next: 4, sum: 10\n",
      "accumulator: 10, next: 5, sum: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "def custom_sum(accumulator, nxt):\n",
    "    print(f\"accumulator: {accumulator}, next: {nxt}, sum: {accumulator + nxt}\")\n",
    "    return accumulator + nxt\n",
    "\n",
    "reduce(custom_sum, numbers, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a function here, custom_sum which takes two numbers, adds them together and returns the result. When we call the reduce() function we're telling python to take the list of numbers, and pass each of them to the custom_sum function. Notice that our function takes two parameters, though. The reduce function uses an *accumulator*. We call our function on the first item in the list, and whatever comes back is passed in as the first parameter when we move onto the second item. The return value of this call is passed in with the third item *etc.* The final parameter to the reduce() sets the initial value for the accumulator.\n",
    "\n",
    "Try it yourself. Use the reduce function to take a list of words and output a single string with each word separated by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def paste(words):\n",
    "    return reduce(lambda acc, nxt: acc + ' ' + nxt, words, '')[1:]\n",
    "\n",
    "# you can use str[1:] to remove the first character of a string\n",
    "paste(['The', 'quick', 'brown', 'fox']) #'The quick brown fox'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now put all of this together to calculate the a posteriori values. Implement the calculate_a_posteriori function below. The answers you are expecting are included in the comments.\n",
    "\n",
    "1. Find the prior probability (in the factor_dict using the target_column name as key)\n",
    "2. Find the conditional probabilities for each column in the row (using the calculate_column_conditional_probability function\n",
    "3. Reduce the conditional probabilities by multiplying each of them together\n",
    "4. Multiply the result by the prior\n",
    "\n",
    "To make a prediction we calculate the a posteriori for each possible target value and predict the value with the highest a posteriori value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_a_posteriori(factor_dict: dict, row: pd.Series, target_column: str, target_value: bool) -> float:\n",
    "    prior = factor_dict[target_column][0] if target_value == True else factor_dict[target_column][1]\n",
    "    conditional_probs = [calculate_column_conditional_probability(factor_dict, target_value, column, row[column]) for column in row.index]\n",
    "    return reduce(lambda acc, nxt: acc * nxt, conditional_probs, 1) * prior\n",
    "\n",
    "query = pd.Series({'headache': True, 'fever': True, 'vomiting': False})\n",
    "print(f\"A Posteriori True: {calculate_a_posteriori(factor_dict, query, 'meningitis', True)}\") # 0.0222188888889\n",
    "print(f\"A Posteriori False: {calculate_a_posteriori(factor_dict, query, 'meningitis', False)}\") # 0.0918508169796\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "1. We've looked at how to calculate the *a posteriori* values for a given query using a factor table. This is essentially the code that we would run for `model.predict()`. What would we need to do when `model.train()` was called? Can you implement it?\n",
    "2. So far we've only looked at how to determine which value a model should predict. ScikitLearn usually provides a `model.predict_proba()` function returning the expected probability of a given value. How would you implement predict_proba for a Naive Bayes? Try it.\n",
    "3. Laplacian smoothing allows us to smooth probabilities using a parameter *k*. How would you update the calculate_probability() function from the beginning of this notebook to allow for Laplacian smoothing. You may assume all columns are boolean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
