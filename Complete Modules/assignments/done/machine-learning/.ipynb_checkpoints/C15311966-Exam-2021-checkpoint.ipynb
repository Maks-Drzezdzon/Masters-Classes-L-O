{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maksymilian Drzezdzon C15311966 Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, make_scorer, cohen_kappa_score\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('../data.csv', delimiter=',')\n",
    "\n",
    "# Prepare values for training data\n",
    "labels = train_data.pop('Class').values\n",
    "data = train_data.values\n",
    "\n",
    "print('Data load complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=xxx)\n",
    "\n",
    "#sc = StandardScaler()\n",
    "\n",
    "#X_train = sc.fit_transform(X_train)\n",
    "#X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': [1, 10, 100], 'kernel': ['poly'], 'gamma': [0.001, 0.0001, 'scale', 'auto'], \n",
    "     'random_state': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 21, 22, 23, 24, 25], \n",
    "     'cache_size': [200], \n",
    "     'coef0': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    " ]\n",
    "\n",
    "clf = svm.SVC()\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring = 'accuracy')\n",
    "svm_gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_acc = grid_search.best_score_\n",
    "best_params = grid_search.best_params_\n",
    "print(\"best acc \", round(best_acc*100, 3))\n",
    "print(\"best params \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Model Evaluation #\n",
    "####################\n",
    "print ('Model Evaluation')\n",
    "# Model Accuracy: how often is the classifier correct\n",
    "print(\"Model Accuracy:\", round(metrics.accuracy_score(y_test, m_svm), 3))\n",
    "# Model Precision: what percentage of positive tuples are labeled as such\n",
    "print(\"Model Precision:\", round(metrics.precision_score(y_test, m_svm), 3))\n",
    "# Model Recall: what percentage of positive tuples are labelled as such\n",
    "print(\"Model Recall:\", round(metrics.recall_score(y_test, m_svm), 3))\n",
    "# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "print('F1 Score: ', round(f1_score(y_test, m_svm, average=\"macro\"), 3))\n",
    "\n",
    "print('Cohens Kappa :', round(cohen_kappa_score(y_test , m_svm), 3))\n",
    "# Combination of Accuracy, Precision, Recall \n",
    "print(\"Classification Report :\\n\", classification_report(y_test,m_svm))\n",
    "\n",
    "cvs = cross_val_score(svm_gs, data, labels, cv=10)\n",
    "\n",
    "print('K-fold Cross Validation scores:')\n",
    "print('Max Score: ', round(max(cvs), 3))\n",
    "print('Min Score: ', round(min(cvs), 3))\n",
    "print('Mean Score :', round(mean(cvs), 3))\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "\n",
    "X_test, y_test = train_test_split(test_data_df, test_size=1, random_state=104) # 70% training and 30% testprint(\"after split check\", len(X_test),  len(y_test))\n",
    "#scaled_test = sc.fit_transform(X_test)\n",
    "#scal = StandardScaler().fit(test_data_df)\n",
    "#test = scal.transform(test_data_df)\n",
    "\n",
    "\n",
    "m_svm = svm_gs.predict(test_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=104) \n",
    "#X_test = sc.transform(X_test)\n",
    "\n",
    "m_svm = svm_gs.predict(X_test)\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, m_svm)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn rate Curve\n",
    "# uses k-cross validation = 5 by default\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator = svm_gs, \n",
    "                                                        X = data, \n",
    "                                                        y = labels,\n",
    "                                                        cv=5,\n",
    "                                                        scoring='accuracy',\n",
    "                                                        n_jobs=-1)\n",
    "\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1))\n",
    "plt.title(\"Learn Curve for SVM Model\")\n",
    "plt.xlabel(\"Experiance Gained\")\n",
    "plt.ylabel(\"Accuracy Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=100) \n",
    "\n",
    "# Predict the response for test dataset\n",
    "# sc = StandardScaler()\n",
    "# x_train = sc.fit_transform(X_train)\n",
    "# x_test = sc.transform(X_test)\n",
    "\n",
    "clf=RandomForestClassifier(criterion='entropy')\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "m_random_forest = clf.predict(X_test)\n",
    "\n",
    "feature_imp = pd.Series(clf.feature_importances_, index=train_data.columns).sort_values(ascending=False)\n",
    "\n",
    "# keep top 15 values\n",
    "top_ten_features = feature_imp.nlargest(15, keep='all')\n",
    "\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=top_ten_features, y=top_ten_features.index)\n",
    "# labels\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "# the legend works but takes up space and isnt needed\n",
    "# plt.legend(top_ten_features.keys())\n",
    "plt.show()\n",
    "\n",
    "##############################\n",
    "# Testing model on test data #\n",
    "##############################\n",
    "top_ten_labels = list(train_data[top_ten_features.keys()].keys())\n",
    "top_ten_data = train_data[top_ten_features.keys()].values\n",
    "# extracted 15 features \n",
    "train_data_2 = train_data[top_ten_labels]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
