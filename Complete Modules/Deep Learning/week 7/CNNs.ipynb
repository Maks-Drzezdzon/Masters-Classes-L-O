{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"CNNs.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"WXwidOqnCzCu"},"source":["# Convolutional Neural Networks\n","\n","## Introduction\n","A neural network's hidden layers are used to learn feature detectors from input data. For simple input data types we can use fully connected hidden layers to allow us to learn features across any combination of input feature values. Thus one feature might be learned which is activated only when the first and last input feature values are both active. Another feature might be learned which is active when the average intensity across all input feature values is above a certain threshold. \n","\n","The figure below illustrates this situations. Here neurons $\\delta^{2}_{1}$ and $\\delta^{2}_{2}$ for example might be defined to depend on the first and last input neurons, and mean values of input neurons respectively.\n","\n","<!-- conv-1.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1fMCHTOlnbr6_DegGJ4f11i6O4hzWuQQx\"/>\n","\n","While fully connected layers like this are very useful at learning features that might depend on any particular combination of input data values, they are computationally costly. The number of weights to be learned in a fully connected layer between input units $L1$ and hidden units $L2$ is: \n","\n","\\begin{equation}\n","(size(L1)+1) \\times size(L2)\n","\\end{equation}\n","\n","where the addition of 1 to the left hand side is due to the fact we need to learn the weight to a bias unit on the input layer. \n","\n","Lets consider for a moment the case of working with image data. Here if we assume a low resolution square image of edge 256 lines, and assume we want a hidden layer that can say pick out 64 features, then we require $((256 \\times 256) + 1) \\times 64$ weights, i.e., 4,194,368 weights. This is a very significant number of weights in comparison to the examples we have considered to date. \n","\n","For complex input data types such as images we deal with this exponential growth in the number of weights by taking advantage of specific properties of the input data. In the case of images we can take advantage of two facts:\n","1. That in images the input is often translation invariant; and\n","2. In early layers we can focus on learning local features rather than global features. \n"," \n","To illustrate consider the case of analyising an image to find edges. An edge can be found by looking for a steep gradient in image pixel intensities in a local block of pixels. For example simple features for horizontal and vertical edge detection in a 3x3 block of pixels can be illustrated by the following weight matrices for linear feature detectors. \n","\n","<!-- edge-detector-unattributed.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1fUwg2t7-z65YLj8hDUd6hMZQ3_zzKnW3\"/>\n","\n","\n","Such a local feature is translation invariant -- a horizontal edge at the bottom left of the picture generally looks like a horizontal edge in any other part of the image when viewed as a steep gradient in pixel intensities. Such a property has been taken advantage of in the image processing domain for many years. For example consider the image below where vertical and horizontal edges are extracted from an image. \n","\n","<!-- edges-unattributed.jpg --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1fYHuRLAdsMd1FjIw5Qbl1HJUGBqxf5b6\"/>\n","\n","We can illustrae the use of a simple edge detector below by scanning over an image from the MNIST dataset based on the Gx filter shown above. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"vtJ_phQBCzC5","executionInfo":{"status":"ok","timestamp":1614448878642,"user_tz":0,"elapsed":3364,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhggEcknlD2trvJvC8LZps9GOv7qM-jIpnS1Ngh58C-sEOMV7_YJwEDduH5NTBWCFiQ0D7qpBVQIXbXOPzRa7els5xdPBsfKo2tukQwmN49R3bN3Z7KlxeVNjstNlp82_q3yawUHW2qyTzI2Dq5l3G-Mwe8SfX4Uho0avIG-k2k8ZWWD5q6p_K2JLNqdMy10ZZ-nXvTG_yK5Q2dcFZjLbVXc5YSrG4j4RABLj1f-IwFsvdKMNM5fF-uCitxQfSy_C5Yreq4Aw8q9GLOZpqQdlPv6c6ixxHrFzvLeUc-07ggoAUE8CZEyfzIYS1qhqSjWnuj-JtHF3n-JyhHj2cMa53nFbuPRLxVBE0EQW4lLo622sx1qLIf2iXSf8zXBHvjuuI32tk8h_3TeDzhJQMgKvN8YWXHRGruQCcAUGXnHRMvKJKkZU2kS_w9hxx2xBua93Sd61PoWCAPzHC-EWORnAACvlaXq0UT1rGzbcWO5uOrCAAQn8ydqNIKg2ojY9EgMwNasZ2PeP8AXTPB33rJyDwx-PUolBgIxfjp_3RzQ1vFMlJQofrongVxF7yj2wtKuswfZBD85L50MQTpmR_7Gsq7jUsFqW0hR2xuGGcghC6JOvd0pqdoUBoxBbdCHGFwVDE2tCkt84i03S47WpNx23KbIRRTTKYxUrKm5Ca2Y7QlLCu5nn9QqWCA7O3S__jMny_yNkfF25HtfckA3LBQJ4N7l1rDQ7-vshTCdUDsyew0ZUMvnNqw8Q-nhbREWIUwTbmZYQ=s64","userId":"03086069160226397014"}},"outputId":"3f64f879-1375-4c47-c9ff-d1e7759ede61"},"source":["# %matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib as mp\n","import numpy as np\n","import tensorflow as tf \n","\n","# Get a copy of the mnist dataset container \n","mnist = tf.keras.datasets.mnist \n","\n","# Pull out the training and test data \n","(x_train, y_train),(x_test, y_test) = mnist.load_data() \n","image = x_train[5]\n","\n","# Reshape the image into 2D and normalize pixel intensities to between 0 and 1\n","flattened_image = np.reshape(image, (-1, 28)) \n","flattened_image = flattened_image / 255.\n","\n","# For illustration purposes display the flattened image of a digit \n","fig = plt.figure()\n","ax = fig.add_subplot(1, 1, 1)\n","ax.matshow(flattened_image, cmap = mp.cm.binary)\n","plt.xticks(np.array([]))\n","plt.yticks(np.array([]))\n","plt.show()\n","\n","# design a filter which picks out verticle edges\n","# in this case it is a 3x3 filter \n","#weights = [-1,0,1,-1,0,1,-1,0,1]\n","weights = [-1,-1,-1,0,0,0,1,1,1]\n","\n","# Construct a conainer for the result of filter application. \n","# Note that our output image will be slightly smaller than our input image\n","num_rows, num_cols = np.ma.shape(flattened_image)\n","edge_output = np.zeros((num_rows-2,num_cols-2))\n","\n","# iterate over the rows in the image and apply the filter\n","for i,row in enumerate(flattened_image):\n","    if i < (num_rows - 2):\n","        # iterate over each cell in the given row\n","        for j,val in enumerate(row):\n","            if j < (num_cols - 2):\n","                # manually isolate the pixels that will have the filter applied to\n","                sample = [flattened_image[i][j],\n","                          flattened_image[i][j+1],\n","                          flattened_image[i][j+2],                          \n","                          flattened_image[i+1][j],\n","                          flattened_image[i+1][j+1],\n","                          flattened_image[i+1][j+2],                            \n","                          flattened_image[i+2][j],\n","                          flattened_image[i+2][j+1],\n","                          flattened_image[i+2][j+2],                                                    \n","                         ]\n","                # calculate and store a logistic function based on the sample and weights\n","                logit = np.matmul(sample,weights)\n","                edge_output[i,j] = 1 / (1 + np.exp(-logit))\n","\n","# for illustration purposes display the results of the feature detector \n","fig = plt.figure()\n","ax = fig.add_subplot(1, 1, 1)\n","ax.matshow(edge_output, cmap = mp.cm.binary)\n","plt.xticks(np.array([]))\n","plt.yticks(np.array([]))\n","plt.show()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_oL_3LLjCzC8"},"source":["In the example above we say that our filter is being *convolved* over the input image. In other words the filter is being applied independently to each input. \n","\n","Note that in the example code above we design our filter to only look at complete 3x3 blocks of our input image. This is a simplification to allow a quick and transparent implementation of the process. This results in our output image being slightly smaller than the input image. In this case our output image is 26x26 pixels rather than the 28x28 of our input image. \n","\n","It is also worth noting that the filters above have no bias value. This is acceptable in this simple case since we are not training the filter. However later when we switch to a Neural Network implementation we need to introduce a bias unit. "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"8tA2_o3jCzC-"},"source":["## Convolutional Layer Design\n","\n","In convolutional neural networks we apply the principle above to the design of hidden layers in the network. Specifically, instead of relying on neurons in a hidden layer that are fully connected to each neuron in the preceding layer, we instead design our hidden layer neurons in such a way that they are only exposed to a small sample of the input units. However we introduce clones of these neurons so that the entire set of inputs is covered by a given hidden layer neuron type. \n","\n","We can achieve this goal in a Deep Neural Network architecture by simultaneously learning copies of simple feature detectors that are applied across a complete input vector (2D in the case of images). Each of these detectors share the same weights and can thus be thought of as being copies or clones of each other. The features detected are local and assumed to be translational invariant. \n","\n","Rather than having a hidden layer that is fully connected to the input layer we end up with a hidden layer that consists of a set of neurons that each look at a slightly different subsample of the input image. The sub-sample might for example be a 3x3 grid of the input image. Each neuron in the convolutional neuron set is looking at a different part of the input, but crucially since all these neurons are forced to share the same weights, they all are looking for the exact same thing. \n","\n","This principle is illustrated in the figure below where the same feature detector is applied to different samples of the input image to generate a feature map. Note that the weight marked is fixed to the same value across each application. \n","\n","<!-- cnn-animated.gif --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1ezhdHl23eVafOshPStEO9QaCjG5ekwcf\"/>\n","\n","In a convolutional layer we typically train a number of these small local features. Each of these features in turn results in individual feature maps. Considering our example earlier we might for example construct a very simple convolutional layer with two features: one feature for vertical line detection and another feature for horizontal line detection. Such a simple convolutional layer is illustrated below. \n","\n","<!-- cnn_example.png --> \n","<img width=\"300\" src=\"https://drive.google.com/uc?id=1ezhdHl23eVafOshPStEO9QaCjG5ekwcf\"/>\n","\n","We see in this example that there are in fact two neuron types in our convolutional layer. Each of these is characterissed by having its own set of weights, but is applied iteratively over the input image to produce a given feature map. The feature itself is a simple 3x3 feature, i.e. it contains only 9 weights that are applied to a local block of the input. The feature results in a Feature Map over our input. The Feature Map is our resultant image above. \n","\n","### Convolutional Layer Parameters \n","\n","Looking again at the example we can see that a convolutional layer has 3 key parameters that describe the dimensionality of the layer's output. The first two of these are **width** and **height** and are used to describe the dimensionality of a given feature map in the convolutional layer. The third dimensionality feature is **depth**. Depth is the number of filter or neuron types in the convolution layer. Each index in the depth of the convolution layer allows another filter type / another feature type to be learned. All neurons at the same depth index share the same weights. \n","\n","The figure below shows a traditional illustration of one layer of a convolutional neural network. From this illustration we can see where the term depth has come to mean the number of features / feature maps in a convolutional layer. \n","\n","<!-- cnn_depth.png.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1f6Z_4-mEdP9WQz2kD5p3D7ao0hRXyKm9\"/>\n","\n","It should be noted however that CNNs are sometimes visualised as being orthogonal to our viewing plane. In this case the different feature types move from left to right across the screen as illustrated in this wikimedia image below. In that image the depth of the convolutional layer is 5 units and we see that all 5 units with the same height and width values are projections from the same scanned section of the input image. \n","\n","<!-- cnn_alt.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1f1CZyNB0-kGg12EPGcyNGkM_CbNG8lzo\"/>\n","\n","In our examples above our filter was applied with a sliding window of 1 over our input image. Thus the overlap between our application of the filters was maximized. It is possible to reduce this overlap and thus result in smaller convolutional feature maps. The amount of overlap is controlled by a parameter called **stride**. In our example above we used a stride of 1. A stride of 3 in the example above would result in no actual overlap of our filter over the input images. In our examples below we will stick with a stride of 1 for notational simplicity. \n","\n","### From One to Multiple Channels\n","\n","In our mnist example, our input is a black and white image. We talk about this image type as just having one channel -- just black and white intensity. Colour images on the other hand are usually built around three individual maps of red, green, and blue intensities. Our feature based approach is easily expanded to deal with multiple channels. In such case the information from all 3 channels is accounted for in a single kernel computation. For example, in the example above we use a 3x3 filter applied to our 1 channel information. This results in 9 parameters to be trained (plus a single bias value). We can apply the same kernel type instead to 3 channel information. In this case the number of parameters that our kernel has to learn is (3 x 3 x 3) + 1, i.e., 28. A single feature detector will still produce a single image map. The advantage of this approach is that our filters not only get the opportunity to learn spatial features, but they can learn features dependent on particular combinations of colour. \n","\n","### Multiple Convolutional Layers\n","\n","Rather than having a single convolutional layer in a network, we usually have a number of layers configured in a feedforward arrangement. Here a feature map at layer k with depth $d_{1}$ feeds can be used as input to any number of feature detectors in the next convolution layer. \n","\n","In a typical design, we treat all the image maps is a layer as individual channels being passed into the next layer. This gives our network the opportunity to combine information from multiple trained features. \n","\n","\n","### Linking to Fully Connected Layers\n","\n","Layers of convolutional neurons create more and more sophisticated feature detectors that operate locally. At a certain point we want to be able to investigate global connections in an image. For example we might want one feature that activates if a straight vertical line is detected in one area of an image and a horizontal line is detected to its upper right. For these global connections to be detected we need to once again introduce standard non-convolutional layers. \n","\n","<!-- cnn_full.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1fF3Uv8xwIelZXf9h45jHSx5VmhWY_SMy\"/>\n","\n","\n","In the context of CNNs we refer to standard layers as fully connected layers. The reason for this is simply that all neurons in a standard layer are fully connected back to all units in the prior layer. Where the prior layer is a convolutional layer this means that all units in the convolution are connected. This can lead to an explosion of connections / weights. Consider the case where the convolutional layer has depth 20, height 20, and width 20. If our fully connected layer has say 30 neurons then we are talking about 240000 connections in one layer alone. \n","\n","## Pooling Layers\n","\n","With a stride of 1 our convolutional feature maps are almost as big as our input images. With a potentially large number of features in our convolutional layer this can result in a very large number of output values which would then have to be fed into layers further down the network. To limit this growth in connection number we can subsample our convolutional feature maps to reduce their size. In neural network terminology this subsampling is referred to as pooling. \n","\n","We can illustrate this pooling method by applying a pooling layer to our image detection feature map as follows. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"tF0XdqUFCzDE","executionInfo":{"status":"ok","timestamp":1614448878644,"user_tz":0,"elapsed":3328,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhggEcknlD2trvJvC8LZps9GOv7qM-jIpnS1Ngh58C-sEOMV7_YJwEDduH5NTBWCFiQ0D7qpBVQIXbXOPzRa7els5xdPBsfKo2tukQwmN49R3bN3Z7KlxeVNjstNlp82_q3yawUHW2qyTzI2Dq5l3G-Mwe8SfX4Uho0avIG-k2k8ZWWD5q6p_K2JLNqdMy10ZZ-nXvTG_yK5Q2dcFZjLbVXc5YSrG4j4RABLj1f-IwFsvdKMNM5fF-uCitxQfSy_C5Yreq4Aw8q9GLOZpqQdlPv6c6ixxHrFzvLeUc-07ggoAUE8CZEyfzIYS1qhqSjWnuj-JtHF3n-JyhHj2cMa53nFbuPRLxVBE0EQW4lLo622sx1qLIf2iXSf8zXBHvjuuI32tk8h_3TeDzhJQMgKvN8YWXHRGruQCcAUGXnHRMvKJKkZU2kS_w9hxx2xBua93Sd61PoWCAPzHC-EWORnAACvlaXq0UT1rGzbcWO5uOrCAAQn8ydqNIKg2ojY9EgMwNasZ2PeP8AXTPB33rJyDwx-PUolBgIxfjp_3RzQ1vFMlJQofrongVxF7yj2wtKuswfZBD85L50MQTpmR_7Gsq7jUsFqW0hR2xuGGcghC6JOvd0pqdoUBoxBbdCHGFwVDE2tCkt84i03S47WpNx23KbIRRTTKYxUrKm5Ca2Y7QlLCu5nn9QqWCA7O3S__jMny_yNkfF25HtfckA3LBQJ4N7l1rDQ7-vshTCdUDsyew0ZUMvnNqw8Q-nhbREWIUwTbmZYQ=s64","userId":"03086069160226397014"}},"outputId":"f61a3197-6e2c-4513-f1f2-88f3127618cf"},"source":["# Construct a conainer for the result of pooling application. \n","num_rows, num_cols = np.ma.shape(edge_output)\n","mean_pool = np.zeros((num_rows//2,num_cols//2))\n","max_pool = np.zeros((num_rows//2,num_cols//2))\n","\n","# iterate over the rows in the image and apply the filter\n","for i,row in enumerate(edge_output):\n","    if i % 2 == 0: \n","        # iterate over each cell in the given row\n","        for j,val in enumerate(row):\n","            if j % 2 == 0: \n","                # manually isolate the pixels that we will pool\n","                sample = [edge_output[i][j],\n","                          edge_output[i][j+1],                     \n","                          edge_output[i+1][j],\n","                          edge_output[i+1][j+1],                                                     \n","                         ]\n","                # store the pooled values\n","                mean_pool[i//2,j//2] = np.mean(sample)\n","                max_pool[i//2,j//2] = np.amax(sample)\n","\n","# for illustration purposes display the results of the feature detector \n","fig = plt.figure()\n","ax = fig.add_subplot(1, 2, 1)\n","ax.matshow(max_pool, cmap = mp.cm.binary)\n","plt.xticks(np.array([]))\n","plt.yticks(np.array([]))\n","ax = fig.add_subplot(1, 2, 2)\n","ax.matshow(mean_pool, cmap = mp.cm.binary)\n","plt.xticks(np.array([]))\n","plt.yticks(np.array([]))\n","plt.show()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nR-mDrejCzDI"},"source":["We pool by taking a clump of units and producing one aggregate value for those units. There are a number of ways in which we can aggregate or pool these values. One straightforward way is to simply calculate the mean activation over the clump of units. This is referred to as mean-pooling. Another popular method is to take the maximum value across the clump of units as the activation for the new aggregate unit. This is referred to as max-pooling. Max pooling rather than mean pooling has become the dominant method for pooling. One 'hand-waving' interpretation of why max-pooling might be most useful is that it preserves the identification of strong features rather than smoothing out the feature map. \n","\n","It is important to note that whereas a convolutional layer has an actual activation function which can be anything from a logistic function to RELU, a pooling layer does not apply any function to the input data apart from the pooling function itself (usually max or mean). \n","\n","The primary motivation for pooling is thus simply to reduce the size of feature maps and therefore reduce the number of connections in the network. Not only is this important in terms of reducing computational cost, but it also helps to reduce the potential for overfitting. \n","\n","The figure below illustrates the application of pooling in a complete workflow. \n","\n","<!-- cnn_pooling.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=1fMBuHWsUWZYHxpre61DvPvQPLrALk5gs\"/>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wz-6sbeECzDI"},"source":["## Convolution and Pooling in Tensorflow \n","\n","TensorFlow provides a very neat implementation of Convolutional Neural Network and Pooling functionality where once again we just need to add in some extra layer definitions. \n","\n","### MNIST in TensorFlow without Convolution\n","\n","To illustrate the TensorFlow approach we will swap over to the use of the MNIST digits corpus. In the code below we first provide an implementation without the use of convoutional or pooling layers. This implementation uses 2 hidden layers of 256 RELU units each. The Adam Optimzier is also used. This is basically the same examples we have seen a couple of times over the last few weeks. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ZLIhm00CzDJ","executionInfo":{"status":"ok","timestamp":1614448901633,"user_tz":0,"elapsed":26267,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhggEcknlD2trvJvC8LZps9GOv7qM-jIpnS1Ngh58C-sEOMV7_YJwEDduH5NTBWCFiQ0D7qpBVQIXbXOPzRa7els5xdPBsfKo2tukQwmN49R3bN3Z7KlxeVNjstNlp82_q3yawUHW2qyTzI2Dq5l3G-Mwe8SfX4Uho0avIG-k2k8ZWWD5q6p_K2JLNqdMy10ZZ-nXvTG_yK5Q2dcFZjLbVXc5YSrG4j4RABLj1f-IwFsvdKMNM5fF-uCitxQfSy_C5Yreq4Aw8q9GLOZpqQdlPv6c6ixxHrFzvLeUc-07ggoAUE8CZEyfzIYS1qhqSjWnuj-JtHF3n-JyhHj2cMa53nFbuPRLxVBE0EQW4lLo622sx1qLIf2iXSf8zXBHvjuuI32tk8h_3TeDzhJQMgKvN8YWXHRGruQCcAUGXnHRMvKJKkZU2kS_w9hxx2xBua93Sd61PoWCAPzHC-EWORnAACvlaXq0UT1rGzbcWO5uOrCAAQn8ydqNIKg2ojY9EgMwNasZ2PeP8AXTPB33rJyDwx-PUolBgIxfjp_3RzQ1vFMlJQofrongVxF7yj2wtKuswfZBD85L50MQTpmR_7Gsq7jUsFqW0hR2xuGGcghC6JOvd0pqdoUBoxBbdCHGFwVDE2tCkt84i03S47WpNx23KbIRRTTKYxUrKm5Ca2Y7QlLCu5nn9QqWCA7O3S__jMny_yNkfF25HtfckA3LBQJ4N7l1rDQ7-vshTCdUDsyew0ZUMvnNqw8Q-nhbREWIUwTbmZYQ=s64","userId":"03086069160226397014"}},"outputId":"d118f64a-f52e-40df-c8e9-5e36ebb7941c"},"source":["# import tensorflow library \n","import tensorflow as tf \n","\n","# Get a copy of the mnist dataset container \n","mnist = tf.keras.datasets.mnist \n","\n","# Pull out the training and test data \n","(x_train, y_train),(x_test, y_test) = mnist.load_data() \n","\n","# Normalize the training and test datasets\n","x_train = tf.keras.utils.normalize(x_train, axis=1)\n","x_test = tf.keras.utils.normalize(x_test, axis=1)\n","\n","# Create a simple sequential network object\n","model = tf.keras.models.Sequential()\n","\n","# Add layers to the network for processing the input data \n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n","model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n","\n","# Compile the model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","# Start the training process\n","model.fit(x=x_train, y=y_train, epochs=5) \n","\n","model.summary()\n","\n","# Evaluate the model performance with test data\n","test_loss, test_acc = model.evaluate(x=x_test, y=y_test,verbose=0)\n","\n","# Print out the model accuracy \n","print('\\nTest accuracy: ' + str(test_acc*100) + \"%\" )"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1875/1875 [==============================] - 5s 2ms/step - loss: 0.4701 - accuracy: 0.8648\n","Epoch 2/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.1156 - accuracy: 0.9641\n","Epoch 3/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0730 - accuracy: 0.9778\n","Epoch 4/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0524 - accuracy: 0.9836\n","Epoch 5/5\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0389 - accuracy: 0.9873\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (32, 784)                 0         \n","_________________________________________________________________\n","dense (Dense)                (32, 128)                 100480    \n","_________________________________________________________________\n","dense_1 (Dense)              (32, 128)                 16512     \n","_________________________________________________________________\n","dense_2 (Dense)              (32, 10)                  1290      \n","=================================================================\n","Total params: 118,282\n","Trainable params: 118,282\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","Test accuracy: 97.22999930381775%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F76w5-eUCzDJ"},"source":["The implementation above achieves very high accuracy in just 15 epochs. In fact we can see that the error even after completion of the 0th epoch is already down to below 20%. This isn't an error in the code, but is rather due to the use of mini-batch learning. In the code above we did not wait until collecting all of our error before making adjustments to our weights. Instead we split our training set into a number of mini-batches each of size 100. After collecting the errors for 100 examples we made adjustments to our weights. Given our training data size is very large, this means that we in fact make over 500 applications of the back-propagation of weights over every full epoch of training. Before proceeding, try adjusting the batch size and observer the error graph. \n","\n","With a batch size of 10000, how many epochs does it take for the error to reduce below 10%?\n","\n","### MNIST in TensorFlow with Convolution\n","\n","Implementing a convolutional layer in TensorFlow is very straightforward. We can take our MNIST code from above and very quickly update it to incorporate a CNN. We implement the major changes in our ff_network function. \n","\n","Note that we have to take our input images which had been flattened to 1D arrays and rework them into 2D for use with the convolutional layer. Similarly following the convolutional layer we also have to reshape the data back to 1D to feed it into a fully-connected layer. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jTADUqaCzDK","executionInfo":{"status":"ok","timestamp":1614448990147,"user_tz":0,"elapsed":114703,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhggEcknlD2trvJvC8LZps9GOv7qM-jIpnS1Ngh58C-sEOMV7_YJwEDduH5NTBWCFiQ0D7qpBVQIXbXOPzRa7els5xdPBsfKo2tukQwmN49R3bN3Z7KlxeVNjstNlp82_q3yawUHW2qyTzI2Dq5l3G-Mwe8SfX4Uho0avIG-k2k8ZWWD5q6p_K2JLNqdMy10ZZ-nXvTG_yK5Q2dcFZjLbVXc5YSrG4j4RABLj1f-IwFsvdKMNM5fF-uCitxQfSy_C5Yreq4Aw8q9GLOZpqQdlPv6c6ixxHrFzvLeUc-07ggoAUE8CZEyfzIYS1qhqSjWnuj-JtHF3n-JyhHj2cMa53nFbuPRLxVBE0EQW4lLo622sx1qLIf2iXSf8zXBHvjuuI32tk8h_3TeDzhJQMgKvN8YWXHRGruQCcAUGXnHRMvKJKkZU2kS_w9hxx2xBua93Sd61PoWCAPzHC-EWORnAACvlaXq0UT1rGzbcWO5uOrCAAQn8ydqNIKg2ojY9EgMwNasZ2PeP8AXTPB33rJyDwx-PUolBgIxfjp_3RzQ1vFMlJQofrongVxF7yj2wtKuswfZBD85L50MQTpmR_7Gsq7jUsFqW0hR2xuGGcghC6JOvd0pqdoUBoxBbdCHGFwVDE2tCkt84i03S47WpNx23KbIRRTTKYxUrKm5Ca2Y7QlLCu5nn9QqWCA7O3S__jMny_yNkfF25HtfckA3LBQJ4N7l1rDQ7-vshTCdUDsyew0ZUMvnNqw8Q-nhbREWIUwTbmZYQ=s64","userId":"03086069160226397014"}},"outputId":"a08d33da-87fb-4a21-8603-6e296d0beb0a"},"source":["x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","\n","input_shape = (28, 28, 1)\n","\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Conv2D(12, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(16, activation='relu'))\n","model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","model.summary()\n","\n","# Start the training process\n","model.fit(x=x_train, y=y_train, epochs=5) \n","\n","# Evaluate the model performance with test data\n","test_loss, test_acc = model.evaluate(x=x_test, y=y_test,verbose=0)\n","\n","# Print out the model accuracy \n","print('\\nTest accuracy: ' + str(test_acc*100) + \"%\" )"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 26, 26, 12)        120       \n","_________________________________________________________________\n","max_pooling2d (MaxPooling2D) (None, 13, 13, 12)        0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 2028)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 16)                32464     \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 10)                170       \n","=================================================================\n","Total params: 32,754\n","Trainable params: 32,754\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/5\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.6760 - accuracy: 0.7947\n","Epoch 2/5\n","1875/1875 [==============================] - 17s 9ms/step - loss: 0.1739 - accuracy: 0.9513\n","Epoch 3/5\n","1875/1875 [==============================] - 17s 9ms/step - loss: 0.1069 - accuracy: 0.9687\n","Epoch 4/5\n","1875/1875 [==============================] - 17s 9ms/step - loss: 0.0900 - accuracy: 0.9730\n","Epoch 5/5\n","1875/1875 [==============================] - 17s 9ms/step - loss: 0.0713 - accuracy: 0.9789\n","\n","Test accuracy: 97.49000072479248%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tOmd-Y0QC79v"},"source":["##Â Skip Connections & Residual Networks \n","\n","CNNs allow us to build networks that can build general purpose feature detectors that can be applied over an entire image but yet only require a relatively small number of weights. This versatility lead to many different network architectures being developed around the basic CNN architecture over the last 10 years. However it is important to note that deeper and deeper networks with 50+ layers were often found to be very useful as they could detect more and more complex features from fed in image data. However it was equally found that well known challenges in building deeper and deeper networks were seen. These problems such as the 'vanishing gradient' problem limited the depth of Deep Learning networks in general and deep CNN networks in particular. \n","\n","The so-called Skip Connection provided an essential next step in the development of very deep CNN based networks. The basic idea of the skip connection is that instead of having simply a single collection of neurons in a given hidden layer, that we instead build a special layer type that includes a typical layer architecture but also a specialist direct link between inputs and outputs that allowed signal to fully flow from inputs to outputs without the alteration caused by the activation function. These so-called skip connections have become an integral part of so-called Residual Networks and underpin the standard Deep CNN architectures at this point. \n","\n","The figure below illustrates the general layout of a skip connection as used in a residual network. \n","\n","<!-- cnn_pooling.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=18ShyNnJtXKoG32xmOeGT28ZvjFlkWxT2\"/>\n","\n","The top of the image shows an extract from two standard layers of a network. Here we have an input x. For the moment we can assume this is either directly from an actual input, or perhaps is an output from another hidden layer that is not shown. In a normal way, that value x passes through two hidden layers in sequence. In each layer we first of all compute the value z from x and the weights, before passing this value, the logit, on to the unit's activiation function. Here we assume the activation function is the Rectified Linear Unit. The output of this unit is then passed forward as input to the next unit which in turn calculates the logit with its own weights before again calculating the activation function. This second activtion function can be thought of as the output of this extract, or block. \n","\n","In the bottom of the image we show the altered form as used in a residual network. Again we have an input x, but in this case as well as being passed into the first unit, it is also copied around the first hidden unit and fed into the second unit directly. When it is fed into the second unit, it is important to note that the input is not fed into the unit's standard inputs before calculation of the logit. Instead it is added to the output of the logit calculation just before the calculation of the activation function. \n","\n","While this explains the mechanism of what a skip connection is, it says little to us about the intuition of why this would work. In short, the intuition is that in backpropogation the skip connection is allowing more of the error signal to continue to backpropogate through the network than would be possible otherwise. The two RELU functions can still learn complex functions, but importantly we can now build longer deeper networks. \n","\n","It should be noted that this is just one example of what a residual block can look like. There are many more variations of this general theme. "]},{"cell_type":"markdown","metadata":{"id":"_S_hPhNJ1BE4"},"source":["## The Inception Networks\n","\n","While residual blocks solve many of the challenges in deep CNNs, they are not the end of the story. Making image classifiers more generic required a numbe of other engineering tricks that increased performance. The Incpetion Networks lead the way in many of these changes. \n","\n","One of the most important changes introduced by the Inception Network concerned the issue of scale. \n","\n","The core of the CNN is the feature kernel. In the examples above we suggested that feature kernels of size 3, 4, or 5 might be useful for image processing. In practice however different kernel sizes are better for different object analysis types. For example small kernels are good at identifying small patterns in data, where large kernels are better at identifying more global information. The most important contribution of the inception networks was the idea of having kernels of different sizes within the same layer of the network. This allowed the network to be far more scale invariant in training than was possible previously. \n","\n","<!-- cnn_pooling.png --> \n","<img width=\"500\" src=\"https://drive.google.com/uc?id=19lb9lUilT0dzLJ4VXvO9P0BcNwo_umub\"/>\n","\n","The figure above from Inception Network v1 illustrates how multiple different convolution filter sizes were used in a typical layer alongside max pooling layers to provide a more complex or rather wider range of analysis than was possible previously. \n","\n","Inception introduced many other useful improvements that are beyond of what we can cover here. For anyone interested, a good blog review of the inception network versions and their individual improvements can be found here: \n","\n","https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202#:~:text=The%20Inception%20network%20was%20an,Designing%20CNNs%20in%20a%20nutshell.\n"]},{"cell_type":"markdown","metadata":{"id":"7ZRlV4heCzDK"},"source":["## CIFAR 10 Tutorial\n","\n","The TensorFlow Website has a number of excellent tutorials which demonstrate the power of TensorFlow. CIFAR 10 is a basic dataset for image classification which is often used as a next step after MNIST classification. \n","\n","Read, study and put to work the TensorFlow CIFAR 10 Tutorial as described here:\n","https://www.tensorflow.org/tutorials/images/cnn\n","\n","In order to get your code to run on non GPU hardware it may be necessary for you to reduce the complexity of the network, or only training for a shorter period of time than would be possible. \n"]},{"cell_type":"markdown","metadata":{"id":"TgNxQ98VCzDK"},"source":["## Suggested Tasks\n","\n","1. Add a second CNN layer to the MNIST example above. How does performance and training compare to the 1 CNN variant?\n","2. Add dropout to the MNIST example. Once again, how does training speed and accuracy compare to the original variant? \n","3. Integrate the CIFAR-10 dataset into the MNIST example above. "]}]}