{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Representations & Transfer Learning.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9IuArMDfLOWR"},"source":["# Representations & Transfer Learning\n","\n","Over the last couple of weeks we introduced Convolutional Neural Networks and Recurrent Neural Networks. We saw that both these network types have advantages for particular data types. Generally RNNs are thought of as being used for text and time series data processing, while CNNs are generally seen as being used for image processing. Before we go forward, we should take a moment to note that this is not a hard and fast difference. CNNs can and are used for text processing. RNNs can and are used for processing certain image based tasks, e.g, for video rather than still image processing. \n","\n","This week we aren't going to look at a different architecture type, but instead we are going to step back and consider as a whole the issue of how are data is captured or represented by these networks, and ask what impacts that has for issues like efficient data processing and re-use.\n","\n","Some notes in this section - including some images - are based on the excellent tutorials on convolutional network use for Natural Language Processing that you can find online here:\n","http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ as well as the follow-up with some implementation notes: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n","\n","The notes here are more compact than the original tutorial. Please refer to the original source for more detail if you need it. \n","\n","Let's start by importing some bits and pieces to get that out of the way. "]},{"cell_type":"code","metadata":{"id":"DcGkl6x3LOWY","executionInfo":{"status":"ok","timestamp":1616396568702,"user_tz":0,"elapsed":2550,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}}},"source":["import numpy as np\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n","from tensorflow.keras.datasets import imdb\n","import tensorflow as tf"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJRfK2aBLOWa"},"source":["## Representing the Meaning of Words\n","\n","Since we were looking at text last week, let's begin with the representation of text. This question of 'text representation' is not only important for Neural Network approaches to text processing tasks, but is equally important to text processing in Machine Learning in general. \n","\n","The challenge of representing text for Machine Learning tasks is a very broad research topic. Here we unfortunately don't have time to look at the particular motivations and problems with many approaches, but we will cover the key approaches that are used in practice. \n","\n","### Bag of Word Count Matrices \n","\n","We need a way of representing text, but text presents us some challenges. Words have variable length, i.e., one word can have length 1, e.g., 'a', while another word can be just a little bit longer, e.g., 'pneumonoultramicroscopicsilicovolcanoconiosis'. Worse still, sentences themselves have variable length. Obviously the string type in a computer can capture a sentence in a way that is great for word processing tasks and similar operations, but it isn't a good representation for Machine Learning. Instead we need a representation that is a bit more predictable in terms of the length of our inputs. \n","\n","To achieve this, he most basic form of Text Representation is the so-called Bag of Words style approach. Bag of Words (BOW) representations were very popular for many years and are typical of what is used in Information Retrieval and Text Classification Tasks. The basic features of a BOW approach are: \n","\n"," 1. Each input text document is represented by a vector of values\n"," 2. The vector length corresponds to the number of entries in our vocabulary\n"," 3. Each entry in the vector represents the presence of a given word occurred in the input text \n"," \n","The image below summarises this approach. \n","\n","<!-- bow.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1geHOCUF3eT5_Nwjt6tbso1a18GQJo_I6\"/>\n"," \n","Each row in our table corresponds to an individual document. Depending on the task at hand, the document could be just a sentence, or a paragraph, or indeed, a document, such as a webpage or book. For a given row, each cell represents whether a given word was found in the document. In basic approaches, this might just be a binary flag that says whether the word was in the document or not. Alternatively it can be a count of the number of times that the word is found in the document. \n","\n","The number of columns is equal to the length of our vocabulary. This is something that we need to know in advance for our task. In practice it is often impossible to know this perfectly -- there will be new words that crop up in our test data or at run time that we have not seen before -- so for new unseen words we might represent them as a symbol such an UNK which stands for 'unknown'. \n","\n","Even in English words can have many different forms, e.g,, eat, eats, eaten, ate. This presents a design challenge for BOW style representations. Do we represent such similar but different words with multiple columns or do we create a single column that captures what we call the stem or basic form of a word? There is no perfect answer to this question and this will often again be a task defined decision. By the way, if we think this problem is difficult in English, it is far more difficult in a morphologically rich language such as Turkish. \n","\n","This BOW style approach leads to sparse vectors per document where most entries in a given vector are 0s. In other words, if our vocabulary is say 171,000 (approximate length of the Oxford dictionary), and the sentence we want to encode has say 17 words, then our vector will have 17 non-0 entries and an awful lot of 0s. \n","\n","This Bag of Words approach works surprisingly well for a range of text classification tasks. By limiting the size of our vocabulary and essentially ignoring rarely seen words, we can get very reasonable results for many text classification tasks by simply feeding our sparse vector representations into basic classifiers like Logistic Regression and Naive Bayes classifiers. This is exactly how early spam classifiers were designed.  \n","\n","### Word Vectors \n","\n","While BOW representations are useful, they are now very far from the state of the art. There are many problems with them \n","\n"," 1. We loose information on the ordering of words. The representation is a list of words, typically in alphabetical order, and an indication of whether the word was present. The sentences 'John gave Mary 500 Euro' and 'Mary gave John 500 Euro' have the exact same BOW representation. \n"," 2. The vectors are long. For real word tasks the vectors are very long -- and mostly empty -- which is in practice not a great representation for feeding into classifiers. \n"," 3. There is no relationship between different columns. From a semantics (or meaning) perspective, the document represented by the short BOW representation 001000 is as close to 010000 as it is to 000001.  \n","\n","What we need are representations of words that overcome these limitations. \n","\n","To do this, the first thing we need to do is stop storing a sentence in a single vector. Instead we switch to representing every word as a single vector. Thus we represent an input document as a full 2D array rather than just a vector as in the BOW case above. \n","\n","In these matrices each row captures the meaning of a single word, and the columns are then the features that are used to represent the meaning of the individual words. We can visualize this approach with the simple example below. \n","\n","<!-- word_vectors.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gvmNOsIqc2h3Z5tYjcV-6OhzykYFqzeg\"/>\n","\n","The major advantage of this approach over our BOW representation is that it retains ordering information on our input text. The key disadvantage of this approach is that we end up needing a two dimensional document to represent each individual input document (sentence or full webpage etc.). Also, the number of rows in our representation of the document is now variable and depends on the length of the sentence. In practice though we can say assume that the maximum length of our sentence is maybe 256 words, and we can pad out sentences with empty vectors up until the end of the array. \n","\n","We have said nothing yet about the contents of our word vectors. In fact how we capture meaning in these word vectors can vary considerably from one approach to another. In a basic approach we take a similar approach to our Bag of Words representation and once again use a one-hot-encoding against vocab features. In this approach each word maps to a single active feature with all other features being inactive. Thus for a 10 word sentence represented with features taken from a 10,000 word vocabulary, we would end up with a 10x10000 matrix. In the visualization below I leave 0s unmarked for clarity. \n","\n","<!-- word_vectors_ohe.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1h6-OKjv8aucMIXRoA7ciQvucqZsYYkh6\"/>\n","\n","\n","### Distributed Word Vectors\n","\n","While the approach above can help since the order of words in sentences is now left intact, it isn't enough yet. The representation is still highly sparse and hence difficult to process for large vocabulary sizes. We can get around this problem a little bit by using specialist sparse matrix representations to encode the sparse matrix in a more compact way, but the sparse encoding in itself can make it more difficult to train. The second huge limitation is that our representation fails to account for any similarities between words. As indicated above with my 001000 example, the words 'dog' and 'labrador' are as close to each other in the data space as the words 'dog' and 'deprive'. \n","\n","To overcome these limitations, an alternative approach is to instead use a distributed representation where words are represented in a lower-dimensional feature space (e.g., 128 dimensions) where individual features are real valued (unlike the binary features used in a word identity matrix). This compacted representation is widely known as a distributed representation or word embedding and is a key tool in neural text processing. Keep in mind that each feature in a distributed word embeddings does not necessarily have to 'mean' anything. It can be an aspect of meaning that is well defined, or perhaps only makes sense when we combine it with other features. \n","\n","We can illustrate this approach as follows: \n","\n","<!-- word_vectors_dist.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1h0vrvcNw_0HBec3yifGCLeKA48asVWpf\"/>\n","\n","\n","Within the embedding space related words are located nearby each other. For example we would expect the words 'dog' and 'laborador' to be located much closer to each other in our 128 dimensional space than we would expect the words 'dog' and 'deprive'. Generally speaking, related words are clustered together. \n","\n","We can visualise these clusterings at a course grain by taking a high-level view of clusterings of words in our embedding space by projecting them into 2D:\n","\n","\n","<!-- word_vectors_colab.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1grGp1yvd0uDvT9ceDIPFJFyDoLFgBVzv\"/>\n","<!-- Sourced from http://sebastianruder.com/content/images/2016/04/word_embeddings_colah.png\" -->\n","\n","If we zoom in to a particular section of our embedding space we will find similar words co-located together:\n","\n","<!-- word_vectors_WordTSNE.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gj_8v1PWi5q2Kc6fhKmjeZMzJYySUi2-\"/>\n","\n","Keep in mind that any 2D visualization of our embedding space is a projection out from our n-dimensional embedding space. \n","\n","Projecting from our n-dimensional embedding space into 2D for visualization has been a big research topic over the years. tSNE is one well regarded tool for making these projections, but there are many other options. \n","\n","## Learning Distributed Representations\n","\n","There are many ways in which we could learn a compacted distributed word embedding for text representation. For those familiar with auto-encoders for example, you might consider one approach for learning embeddings as follows:\n","\n","<!-- auto-embed.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gQnTGyuJNZPQuhO7NcsZgHuK3HD6BdPG\"/>\n","\n","where X is the one-hot-encoding of a word in our vocabulary and Z is our embedding that we can then used whenever we need a distributed representation. \n","\n","In practice this simple auto-encoder method does not work well for the problem at hand - not least of all because the auto-encoder method does not use any information about the co-occurrences of words in the training process. \n","\n","Fortunately a lot of work has gone into devising other methods to learn the compacted word embeddings. The most popular method is the Word2Vec approach from Mikolov et al which learns word embeddings in a supervised learning process between target words and their contexts. Actually there are two variant models in Word2Vec. The first variant, continuous bag of words, learns the embeddings in a task where the target word is to be predicted from a context (set of surrounding words). The second variant, skip-gram, learns the word embeddings in a task where context words are to be predicted from a single target word. \n","\n","Whether we train using the continuous bag of words or skip-gram variants, we still achieve as a side effect a hidden layer which can be used subsequently as a word embedding. These two variants are illustrated below:\n","\n","<!-- word2vec.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1gmiOr-gTnCMMhixxmWXebtKZksPzUeFX\"/>\n","\n","For anyone interested in the specifics of learning Word2Vec style embeddings, the TensorFlow website has a nice tutorial that will guide you through the theory and the practical code for learning Word2Vec style embeddings. \n","\n","## Training Word Embeddings\n","\n","Embeddings can be used in one of 3 ways in practice:\n","\n"," 1. We learn embeddings for the sake of learning embeddings with a Word2Vec model or similar.\n"," 2. We learn word embeddings on the fly for a given task.\n"," 3. We use pre-computed word embeddings for our task.\n"," \n","Option 2 is one of the most frequently encountered examples, so let's put it to use here. \n","\n","Keras / Tensorflow proves an awful lot of the machinery for us to do this. There is a very short and elegant tutorial on the subject on the Tensorflow website here:\n","https://www.tensorflow.org/tutorials/text/word_embeddings\n","\n","The code below roughly follows the same layout in that tutorial, but we use a different example to give an alternative perspective on the design. The example we work from is based on this code:\n","https://keras.io/examples/imdb_cnn/\n","\n","Tensorflow provides an Embeddings layer for your networks. This Embeddings layer is a very specialist layer that that has all the machinery to both train and apply embeddings all built into it. The Embeddings that will be learned are not state-of-the-art but they are generally 'good enough'. Specifically the Embeddings layer learns a mapping between the input text that we will provide in a format similar to our 'word vectors' above, and maps them to a real valued space. \n","\n","Let's set up by setting up our example and importing some text that we will use to illustrate. "]},{"cell_type":"code","metadata":{"id":"WnIxARqTLOWh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396574609,"user_tz":0,"elapsed":8452,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"a23cf776-99e3-4cb5-e1eb-4b863ff80840"},"source":["max_features = 5000\n","maxlen = 400\n","embedding_dims = 50\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"},{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Sc9oI1GYsI8-"},"source":["Here we have loaded up loading up Keras's inbuilt wrapper for the IMDB dataset and used it to generate training and validation / testing data. The data is split between inputs (_x) and labels (_y). \n","\n","Next we need to trake the inputs and pad them out to all be the same length. This is essential for processing them. "]},{"cell_type":"code","metadata":{"id":"kST-Jfn2sJSO","executionInfo":{"status":"ok","timestamp":1616396575814,"user_tz":0,"elapsed":9655,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}}},"source":["x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPguk1JKLOWp"},"source":["Let's start building up our model / network. We can use the Embeddings layer in a simple Sequential model. "]},{"cell_type":"code","metadata":{"id":"cdaPJAsxLOWp","executionInfo":{"status":"ok","timestamp":1616396575816,"user_tz":0,"elapsed":9653,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}}},"source":["model = Sequential()"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3sx2f0sLOWp"},"source":["The first layer of our network will be our Embeddings layer. When we define the Embeddings layer, we need to specify the expected size of the raw non-distributed vectors, and then the target embedding dimensionality. We will go with an embedding size of just 8 to begin with. This means that each of our words will be encoded simple as a vector of 8 numbers after the first layer. "]},{"cell_type":"code","metadata":{"id":"nUkR_AwvLOWq","executionInfo":{"status":"ok","timestamp":1616396576020,"user_tz":0,"elapsed":9855,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}}},"source":["embedding_size=8 # try 50 instead\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s34AWiYLOWq"},"source":["The Embeddings layer will make the mapping from the non-distributed input text to our numerical embeddings. After the embedding layer we have an output of dimension Batch_Size x Max_Document_Length x Embedding size. The Batch_Size here is just a batch size in the normal sense. In other words we can train in parallel for multiple instances of our training data in parallel. The Max_Document_Length is our expected maximum length of words in each review. Remember that the input encoder will pad the length of shorter documents out so that the tensor size is always as expected --even for much shorter sentences.\n","\n","At this point there are many different ways in which we could deal with our text. In the simple case we can feed this whole document representation into a Dense Layer. Every unit in this dense layer has access to the embedding values for each word in our document. This unfortunately will also include access to all those 0 padded entries between the actual document length and the max document length. "]},{"cell_type":"code","metadata":{"id":"UX_jHkw0LOWq","executionInfo":{"status":"ok","timestamp":1616396576020,"user_tz":0,"elapsed":9853,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}}},"source":["model.add(Flatten())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V5Vv12cPLOWq"},"source":["Once that is done we can think of our representation just as we would any other layer in a network. We can now follow it with a number of different layers as required before we eventually finish up with an output layer. "]},{"cell_type":"markdown","metadata":{"id":"9OwCkfJULOWr"},"source":["If we look at the model summary, we can see that the Embeddings layer is just treated as any other layer in the network. "]},{"cell_type":"code","metadata":{"id":"DTC4E8dQLOWr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396576021,"user_tz":0,"elapsed":9843,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"18102de3-0968-471d-bf35-1c69c0217320"},"source":["model.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 400, 8)            40000     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 3200)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 200)               640200    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 201       \n","=================================================================\n","Total params: 680,401\n","Trainable params: 680,401\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Qp-RzJsLOWs"},"source":["We can now compile and train the model in the usual way. "]},{"cell_type":"code","metadata":{"id":"rw2PLmk-LOWs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396596136,"user_tz":0,"elapsed":29949,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"7a1a9137-76f1-4e3d-b2cc-95342c3b17ec"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=2,\n","          validation_data=(x_test, y_test))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","782/782 [==============================] - 10s 12ms/step - loss: 0.5528 - accuracy: 0.6472 - val_loss: 0.3058 - val_accuracy: 0.8733\n","Epoch 2/2\n","782/782 [==============================] - 9s 12ms/step - loss: 0.1889 - accuracy: 0.9251 - val_loss: 0.3229 - val_accuracy: 0.8678\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bzvJeQ1OLOWt"},"source":["We can see that the validation accuracy is about 85%. "]},{"cell_type":"markdown","metadata":{"id":"o6cwFCVcLOWu"},"source":["As an alternative to just flatting all embedded document and passing it straight to a dense layer, we do have other options. Another option that is widely applied is that we instead simplify our representation a little by taking an average of all our word embeddings. In other words we end up with an averaged embedding across the whole document. What this means in practice is that we are assuming that the meaning of a document is equal to the averaged meaning of all the words in the document. We can implement this with a GlobalAveragePooling1D layer. It should be noted that this will throw away the information we wanted to keep on the ordering of words in our representations, but we will come back to better ways of handling this later. "]},{"cell_type":"code","metadata":{"id":"fAdcFLdPLOWu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396603691,"user_tz":0,"elapsed":37494,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"c4e75843-860d-432c-aceb-23ca145b5100"},"source":["model = Sequential()\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=2,\n","          validation_data=(x_test, y_test))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 400, 8)            40000     \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 8)                 0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 200)               1800      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 201       \n","=================================================================\n","Total params: 42,001\n","Trainable params: 42,001\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/2\n","782/782 [==============================] - 4s 5ms/step - loss: 0.6135 - accuracy: 0.5831 - val_loss: 0.3298 - val_accuracy: 0.8715\n","Epoch 2/2\n","782/782 [==============================] - 3s 4ms/step - loss: 0.2810 - accuracy: 0.8803 - val_loss: 0.2890 - val_accuracy: 0.8646\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vmqX3CpOLOWu"},"source":["Note that (a) the number of parameters that we have to use in our network are considerable less because we averaged the embeddings within a document; (b) as a result, training was much faster; and (b) we actually achieved a better result! "]},{"cell_type":"markdown","metadata":{"id":"6v7SSLuhLOWv"},"source":["## Convolving Through Word Embeddings \n","\n","In the work above we threw away any processing that was dependent on the order of words in our documents. In practice we want to keep this information. One way in which we can make use of it in a sensible way is through the application of CNNs. \n","\n","What we will be doing is allowing local feature detectors that maybe look at 4 words at a time to convolve through a complete document. This way our feature detectors are able to look for local features in the text, i.e., combinations of 4 words together that have a specific benefit and meaning in our text processing task. \n","\n","In our image processing tasks our feature detectors were convolved across the length and breadth of our input image. A key difference with convolution when applied to text is that we do not convolve across our columns (i.e., the input features). Instead we convolve only down the vertical, i.e., through the words. This makes a lot of sense as the purpose of convolution is to pick up on local features in the data where there is global invariance. For our input representation features we do not have global invariance. However where we are scanning down through the words in sequence it makes a lot of sense to look for features across adjacent words or words which are close by each other. \n","\n","This approach to applying a convolution to an input document is illustrated below. Note that this image comes from the blog past here:\n"," http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ \n","\n","<!-- conv_text.png --> \n","<img width=\"600\" src=\"https://drive.google.com/uc?id=1ghFyeqjfTt9eAVskp1todD1agxLFT3M5\"/>\n","\n","In our image processing examples we stuck with a consistent sliding window size for each convolution in a given layer. In text processing we however would often want to use different length convolutional windows that operate over say 2, 3, 4 or even more words. For each convolution length we will typically have a number of instances of that convolution length, i.e., we allow training of multiple features at each permitted window length. These convolutions in turn produce our standard convolved features which can then be pooled and combined in the usual way. \n","\n","In the illustration above our word embeddings are of dimension 5 and we have a maximum document length of 7 with an input document of length 7. 6 convolution features (kernels or filters) are being used in this case. Two of these have a convolution width of two, two have a convolution width of three, and two have a convolution width of 4. Convolutional application of these filters to our input results in 6 separate feature maps. Since we do not convolve horizontally across our input, note that our feature maps are 1 dimensional. \n","\n","Max pooling can then optionally be used to reduce the size of our feature maps. In the illustration above an extreme form of max pooling is used where all feature maps are reduced to a single value for each feature map. "]},{"cell_type":"markdown","metadata":{"id":"UgNKvEKTLOWv"},"source":["###  Implementing Convolution to Embeddings \n","\n","In the code below we apply the idea of convolutions to our training case. We assume a fixed filter width of 3. This is in contrast to the image above where filters of length 2, 3 and 4 were investigated. We use 50 different kernel instances - this is much larger than the image above, and we use a fully connected hidden layer of 50 units after the pooling layer. \n"," "]},{"cell_type":"code","metadata":{"id":"Bdv9_la8LOWv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396694580,"user_tz":0,"elapsed":128374,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"fb81de36-1170-4264-a782-521a125f2854"},"source":["model = Sequential()\n","model.add(Embedding(max_features,\n","                    embedding_size,\n","                    input_length=maxlen))\n","model.add(Conv1D(250,3,padding='valid',activation='relu',strides=1))\n","model.add(GlobalMaxPooling1D())\n","model.add(Dense(200, activation='relu'))\n","model.add(Dense(1))\n","\n","model.summary()\n","\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","hist = model.fit(x_train, y_train,\n","          batch_size=32,\n","          epochs=2,\n","          validation_data=(x_test, y_test))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 400, 8)            40000     \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 398, 250)          6250      \n","_________________________________________________________________\n","global_max_pooling1d (Global (None, 250)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 200)               50200     \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 201       \n","=================================================================\n","Total params: 96,651\n","Trainable params: 96,651\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/2\n","782/782 [==============================] - 46s 58ms/step - loss: 0.5380 - accuracy: 0.6584 - val_loss: 0.2995 - val_accuracy: 0.8694\n","Epoch 2/2\n","782/782 [==============================] - 45s 58ms/step - loss: 0.2421 - accuracy: 0.8981 - val_loss: 0.2887 - val_accuracy: 0.8811\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wTaPWq2SLOWw"},"source":["From the code above we can see that the CNN based method in this case does not necessarily do much better than in the averaged embeddings case -- but it does generally outperform the base case where just a dense layer is used straight after the embeddings. Generally the results are expected to improve though if we use a couple of different types of covolutions, i.e., convolutions of length 2, 3, 4 and 5. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"TWYBxqCsLOWw"},"source":["## Using Pre-Trained Word Embeddings\n","\n","In many cases we do not need to learn word embeddings from scratch and can instead use pre-trained word embeddings that have been learned from large text corpora using the skip-gram method or similar. Generally the workflow for using these pre-trained embeddings is to download pre-computed vectors for all of the words in a large vocabulary. We then lookup the word vector for a particular word in a lookup table as needed in our task. \n","\n","There are many trained embedding sets available which vary in terms of: (a) the size and nature of the raw text data they were trained over and (b) the specific training method which was used to learn the embeddings. You can for example download a dataset of word vectors for 3 million words and phrases trained from a 100 Billion words from the Google News dataset, and understandably enough the dataset is big. Incidentally the embeddings in that model are 300 dimensions wide. \n","\n","### Tensorflow and Word Embeddings\n","\n","TensorFlow Hub gives us a lot of the plumbing which we require to quickly start working with pre-trained embeddings (and other types of pre-trained models). Basically we can simply download a set of representations that have already been built for us by others and use them directly as an embedding layer just as we did above. The difference is that the training of these Embeddings has already been done for us, and we often can leave the embeddings as is. Sometimes it makes sense though to fine tune these embeddings during our own training process. \n","\n","The Tensorflow Hub makes it incredibly easy to download pre-trained embeddings information and put them directly to use. For example, on the page below you can learn how to directly download an embeddings definition and use it to build a distributed embedding one sentence at a time. \n","\n","https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\n"]},{"cell_type":"markdown","metadata":{"id":"1WwESbCxuqTS"},"source":["To illustrate, let's import Tensorflow Hub and ask it to load the SWIVEL word embeddings. "]},{"cell_type":"code","metadata":{"id":"ru2LJZjtLnH9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616396695187,"user_tz":0,"elapsed":128979,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"e338c5ae-1483-4a05-99b3-b356753b302f"},"source":["import tensorflow_hub as hub\n","\n","print(\"loading embedding\")\n","embed = hub.load(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\")\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["loading embedding\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hDHDDyMGuzN3"},"source":["We can illustrate the use of the embeddings by just feeding some text in and seeing what we get out. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSQt9ul6u1Sf","executionInfo":{"status":"ok","timestamp":1616396695188,"user_tz":0,"elapsed":128977,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"ebcb0445-fedb-47da-f849-eae71bee3e85"},"source":["embeddings = embed([\"cat is on the mat\", \"dog is in the fog\", \"dog\"])\n","print(embeddings)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[ 0.8666395   0.35917717  0.00579667  0.681002   -0.54226625  0.22343189\n","  -0.38796625  0.62195706  0.22117122 -0.48538068 -1.2674141   0.886369\n","  -0.32849073 -0.13924702 -0.53327686  0.5739708  -0.05905761  0.13629246\n","  -1.1718255  -0.31494334]\n"," [ 0.9602181   0.62520486  0.06261905  0.37425604  0.24782333 -0.39351934\n","  -0.7418429   0.56599647 -0.26197797 -0.69016844 -0.76565284  0.71412426\n","  -0.4537978  -0.50701594 -0.8499377   0.8917156  -0.30278975  0.2149126\n","  -1.1098894  -0.46719775]\n"," [ 1.1263883  -0.46177042 -0.8531583   0.5697219  -0.04634653  0.00869457\n","  -0.41134015  1.0862297   0.9390011   0.53587663 -0.964659    0.9846872\n","  -0.5436216  -0.459042   -1.0998259   0.37084442 -0.05279565  0.2736311\n","  -0.54693335 -0.20116976]], shape=(3, 20), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"86LFM4ZZvEzd"},"source":["Note that the particular embeddings model provides us back one embedding per sentence rather than one embedding per word. \n","\n","To use the embedding layer within a network, we simply have to parameterise it appropriately and then directly add it to a model as illustrated below. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pymdjUXnvFA-","executionInfo":{"status":"ok","timestamp":1616396695990,"user_tz":0,"elapsed":129777,"user":{"displayName":"Robert John Ross","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQp5pxJ6ivZFTS0cgX2dBj2TJWg7Bv1z0iTJHVkxsTX9pdes82L1eQb8oAq3ZUp0CL0zRmn4WN838Q68MRH7_0yHzN8aNUKBK-5S32QaiI44tiytGD9udqe5z8nU1Xbvhlntxk2sGAsvL3tkr_0zittdDwhJB7hCsZP1HP778yVRzu2XMPvnbPLtTwURYItUHPeOy6CXR9jsczyJLxAt1WM--VVMKXX0rWT1w2XrJKBsZlhNc63nW9b0DHKfhBxhmss5DPZGHXHbxSdag0ZyyWgJ2hG5tf6x-2NTWxJng3IpmXN1d7ei3CrZu01-6m-iAS1neoTazUvt43oHPFiDUPQqfe4935uJSM7ZK4_Xsp8IKNvozMHTx6VFY2MpLpBCtvkQOYHzeSBigpQ-ec9LuPx0xg7iSCHB7kixxYHT3YNG5jFsSbguJWm_HS0Y_R5ObpXgj3rRgPrI_en67MfzDejhAWucYfpssfe_kZJVVlTkoccKd4BODepqfcjy9eZyHFJLnsy18pkeCFLyraLgTraKyY_XVeQRk1hc5_LeVkNglytSDeoZsY5g7ZX8CjGvPFeTf73dtRj2LLMoFbyiaCZToDJhdK7k8DvREBvNjHyzG04LxNh0-k33FvErcGuDyN7JA5sXXHN5oX0cpt1BuY20Im3-CwpyEn5xufCfdJNz-Now4-v0J25ylw9BdKaSzV1P_oxThOU3ex38lzw-gfb2TBsNIdhH6xKOHizI3SwXvK03TB0EQqt2RqMAe6AkIskA=s64","userId":"03086069160226397014"}},"outputId":"8bcc7be9-ff75-44d1-e46b-2d9f7521659b"},"source":["hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\", output_shape=[20],\n","                           input_shape=[], dtype=tf.string)\n","\n","m2 = tf.keras.Sequential()\n","m2.add(hub_layer)\n","m2.add(tf.keras.layers.Dense(16, activation='relu'))\n","m2.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","m2.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbed3cbee60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7fbed3cbee60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","keras_layer (KerasLayer)     (None, 20)                400020    \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 16)                336       \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1)                 17        \n","=================================================================\n","Total params: 400,373\n","Trainable params: 353\n","Non-trainable params: 400,020\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JcP4LZ24wUgf"},"source":["As an assignment, make use of a pre-trained embeddings layer to train the IMDB data. How does performance compare against on-the-fly embeddings? "]},{"cell_type":"markdown","metadata":{"id":"_piMonq1d8_A"},"source":["## Pre-Trained Models & Transfer Learning\n","\n","We have just seen that with embeddings it is possible to use pre-trained word embeddings rather than have to learn the embeddigns on the fly for our current application. This is an important point in Deep Learning -- we have some modularity in our designs and try to take advantage of pre-trained models when possible. \n","\n","To understand this, consider that a neural network is just an architecture (design) and a collection of parameters that can instantiate that design. Networks can in general be saved to disk and loaded back up whenever we like. This is essential in allowing us to save the model as we are going along (just in case anything causes it to crash) but is also essentail in allowing a degree of re-use across models. \n","\n","While re-use is used considerably in text embeddings models, it is arguably used even more in image processing. \n","\n","All the major computer vision architectures are available to be used on a pre-trained basis. In most of these cases the networks have been pre-trained using the ImageNet dataset. This is a very large collection of labelled images that have been used for many years to train neural networks. Keras for example provides very convinient wrappers that allow us to load up a pre-trained instance of one of these networks and use it within our own model type. \n","\n","Note that this idea of re-use (from a software or design perspective) is very close to the idea of Transfer Learning from a strct machine learning perspective. Transfer Learning generall refers to the idea of learning a model within one domain, and then applying that model to a new domain. What we are trying to achieve is a transfer of knowledge from one application to another -- this is different to a transfer of data. \n","\n","In Transfer Learning we typically run one background training process once on a VERY large dataset to build a good re-usable model that can then be applied to new domains that typically have less labelled data available. We usually however do not copy over the whole network. Instead we often leave the final few layers of the original network behind. We refer to these as the head of the network, and typically these are very much related to the original application that was used to train the network. For example if the original network was trained for image classification of 100 different classes, these networks will be very focused on providing the final softmax layer that maps directly to 100 classes. \n","\n","The assumption here is that while the head of the network is very focused on the specific task, the earlier layers in the network will be more general purpose and will in fact to re-usable in other domains. \n","\n","In our new application network we typically have to create a new 'head' that is specific to our own application. The head of the network will be trained for our new target data but with lower layers in the network being those that came from our original source network. \n","\n","For the layers imported into our target network we have a choice on how to use those layers. In the extreme we can decide to leave the parameters in those layers exactly as they were (freeze), or allow the training process on the new application to modify these weights for the new domain (fine-tune). \n","\n","In practice there are many different strategies for fine tuning availabile, but these often involve designing your new architecture initially with imported weights that cannot change. Then after a period of training we allow the weights to be adjusted during our training process.\n","\n","What do you suppose is the advantage to this graduated fine tuning process versus allowing full weight editing from the beginning? \n","\n","As mentioned the idea of Transfer Learning and re-use is prevalent in image processing, and pre-trained networks based on all the popular image processing architectures are widely available.  "]},{"cell_type":"markdown","metadata":{"id":"cOBdrkwZLOWw"},"source":["## Suggested Tasks\n","\n"," 1. Extend the example above to use a combination of filters of length 2, 3 and 4 as suggested on the source article http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n"," 2. Use the pre-trained embeddings layers in place of an on-the-fly embeddings layer"]}]}